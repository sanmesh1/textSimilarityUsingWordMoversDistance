{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MainV2.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"xpu-w4fP_uKk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":88},"executionInfo":{"status":"ok","timestamp":1592568055063,"user_tz":240,"elapsed":629,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"outputId":"268ca8ef-0c76-4488-e3cb-187a257256d4"},"source":["###################################\n","# new_field_name_list"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","['eBay', 'item', 'number:371252240431', 'Seller', 'assumes', 'all', 'responsibility', 'for', 'this', 'listing', '.', 'Last', 'updated', 'on', 'May', '04', ',', '2015', '10:34:20', 'PDT', 'View', 'all', 'revisions', 'Item', 'specifics', 'Condition', ':', 'New', ':', 'A', 'brand-new', ',', 'unused', ',', 'unopened', ',', 'undamaged', 'item', 'in', 'its', 'original', 'packaging', '(', 'where', 'packaging', 'is', 'applicable', ')', '.', 'Packaging', 'should', 'be', 'the', 'same', 'as', 'what', 'is', 'found', 'in', 'a', 'retail', 'store', ',', 'unless', 'the', 'item', 'is', 'handmade', 'or', 'was', 'packaged', 'by', 'the', 'manufacturer', 'in', 'non-retail', 'packaging', ',', 'such', 'as', 'an', 'unprinted', 'box', 'or', 'plastic', 'bag', '.', 'See', 'the', 'seller', \"'s\", 'listing', 'for', 'full', 'details', '.', 'See', 'all', 'condition', 'definitions-', 'opens', 'in', 'a', 'new', 'window', 'or', 'tab', '...', 'Read', 'moreabout', 'the', 'condition', 'Brand', ':', 'Toshiba', 'MPN', ':', 'HDTB205XK3AA', 'UPC', ':', '022265494950', 'Adorama', 'Visit', 'my', 'eBay', 'store']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZUOxTZ2jzA2F","colab_type":"text"},"source":["##########################################################################"]},{"cell_type":"markdown","metadata":{"id":"qDDGytRNzAyg","colab_type":"text"},"source":["This is version 1 of trying to modularize code so that we can run many experiments and collaborate on this colab.\n","The functions that are common to all experiments that need to be run first are at the bottom of the document beginning with a line of #'s like below:\n","##########################################################################"]},{"cell_type":"markdown","metadata":{"id":"4P3U5iGvDaGX","colab_type":"text"},"source":["#SETUP TO RUN CODE"]},{"cell_type":"markdown","metadata":{"id":"Cq5nIvkiDe2R","colab_type":"text"},"source":["##IMPORTS AND PIP INSTALLS"]},{"cell_type":"code","metadata":{"id":"ZdGKiaG8XHQ5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592820215335,"user_tz":240,"elapsed":5544,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"outputId":"83a8bc36-797a-4f84-c3d9-7992521ca5e2"},"source":["#pip installs\n","! pip install ipdb -q\n","import ipdb \n","#place the line of code below from whichever line you want to start debugging\n","#type n for step over and s for step into\n","# ipdb.set_trace()\n","\n","from time import time\n","import gensim\n","import pandas as pd\n","from typing import List, Tuple, Callable\n","import numpy as np\n","from gensim.models.keyedvectors import KeyedVectors\n","from gensim.models.wrappers import FastText\n","from gensim.similarities import WmdSimilarity\n","from ast import literal_eval"],"execution_count":1,"outputs":[{"output_type":"stream","text":["  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Fbhu9IOfDu3_","colab_type":"text"},"source":["##MOUNT GOOGLE DRIVE SO THAT WE CAN ACCESS FILES FROM GOOGLE DRIVE"]},{"cell_type":"code","metadata":{"id":"8pCZgWJlXzbx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1592820235108,"user_tz":240,"elapsed":19599,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"outputId":"cfa15838-570a-4d1c-f97b-d0a55ad606e2"},"source":["#repoPath should start with the path \n","#'/content/gdrive/My Drive/' and go to the folder where the git repo is, and should end with the name of the repo:\n","repoPath = '/content/gdrive/My Drive/TestingGitCloneIntoGoogleDrive/textSimilarityUsingWordMoversDistance'\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","import os\n","os.chdir(repoPath)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9Fy9vIwdeEU2","colab_type":"text"},"source":["#DOWNLOAD FILES TO data/ FOLDER (1 time setup)"]},{"cell_type":"markdown","metadata":{"id":"PuM-bJYYnYZg","colab_type":"text"},"source":["##Input"]},{"cell_type":"code","metadata":{"id":"PsyI-aFye634","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592820238988,"user_tz":240,"elapsed":280,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}}},"source":["##############################\n","#centralized data folder\n","##############################\n","#centralized folder for data\n","folder_for_data_with_slash_at_end = 'data/'\n","\n","##############################\n","#product dataset related inputs\n","##############################\n","#url of csv dataset to download\n","url_of_dataset = 'https://query.data.world/s/iccclhgzbyedtv54p5lresglnun3qz'\n","\n","#desired filename of dataset\n","filename_of_original_dataset = 'crowdflower_product_dataset.csv'\n","\n","##############################\n","#word embedding related inputs\n","##############################\n","#pretrained word embeddings to load\n","url_to_word_embedding_zip = \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UAcnpsNjnm47","colab_type":"text"},"source":["##Code to download data to data folder"]},{"cell_type":"code","metadata":{"id":"c4duLOId9Lls","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1592820291145,"user_tz":240,"elapsed":292,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"outputId":"653caba2-7584-4145-a5c7-ee91b120cf88"},"source":["original_dataset_path = folder_for_data_with_slash_at_end+filename_of_original_dataset\n","\n","#############################################\n","#Download product dataset\n","def download_csv_from_url_to_local_directory(url_of_dataset: str, desired_path_of_csv_to_be_created: str, csv_encoding: str = \"ISO-8859-1\") -> bool:\n","  import os.path\n","  from os import path\n","  import pandas as pd\n","  if path.exists(desired_path_of_csv_to_be_created) == True:\n","    print('csv already exists at path')\n","    success_in_downloading_dataset = True \n","  else:\n","    product_dataset = pd.read_csv(url_of_dataset, encoding = csv_encoding)\n","    product_dataset.to_csv(desired_path_of_csv_to_be_created, index = False)\n","    if path.exists(desired_path_of_csv_to_be_created) == True:\n","      success_in_downloading_dataset = True\n","    else:\n","      success_in_downloading_dataset = False\n","      print('Failed to download product dataset: ', url_of_dataset)\n","  return success_in_downloading_dataset\n","\n","\n","\n","success_product_dataset_download = download_csv_from_url_to_local_directory(url_of_dataset, original_dataset_path)\n","\n","\n","\n","#############################################\n","##download model\n","# folder_for_data_with_slash_at_end = \"data/\"\n","download_success, word_embedding_file_path = download_and_unzip_pretrained_word_embedding(url_to_word_embedding_zip, folder_for_data_with_slash_at_end)\n","\n","\n","##############################################\n","\n","\n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["csv already exists at path\n","datasetAlreadyExists\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6azhHMhSMVfa","colab_type":"text"},"source":["#Experiment Setup:\n","Creates class that holds experiment related variables that will be used throughout experiment. \n","\n","Output:\n","\n","-sampled product dataframe csv file\n","\n","-queried products dataframe csv file"]},{"cell_type":"markdown","metadata":{"id":"pqBJJhsnqaCf","colab_type":"text"},"source":["##Input"]},{"cell_type":"code","metadata":{"id":"8p0hU2llqdk9","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592820296262,"user_tz":240,"elapsed":253,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}}},"source":["##############################\n","#centralized data folder\n","##############################\n","folder_for_data_with_slash_at_end = 'data/'\n","\n","\n","##############################\n","#experiment folder related inputs\n","##############################\n","experiment_output_folder_path = 'experiment_outputs/experiment3/'\n","\n","\n","##############################\n","#product dataset related inputs\n","##############################\n","#dataset to load\n","filename_of_original_dataset = 'crowdflower_product_dataset.csv'\n","list_of_indexes_to_sample_from_original_dataset = [i for i in range(0,500)]\n","\n","\n","#sampled product dataframe csv name\n","#path initializations\n","sampled_product_dataframe_csv_name = 'sampled_product_dataframe.csv'\n","\n","##############################\n","##Create Query Products List\n","##############################\n","#queried products file path\n","queried_products_desired_csv_path = 'queried_products_sample_dataframe.csv'"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cSCjS0bVqeZo","colab_type":"text"},"source":["##Code"]},{"cell_type":"code","metadata":{"id":"UZ3vQv9AKD9N","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592820304209,"user_tz":240,"elapsed":287,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}}},"source":["class ProductSimilarityExperiment:\n","    def __init__(self, folder_for_data_with_slash_at_end,  \n","                 filename_of_original_dataset, \n","                 list_of_indexes_to_sample_from_original_dataset,\n","                 experiment_output_folder_path,\n","                 sampled_product_dataframe_csv_name,\n","                 queried_products_desired_csv_path\n","                 ):\n","       \n","        ##############################\n","        #centralized data folder\n","        ##############################\n","        #centralized folder for data\n","        self.folder_for_data_with_slash_at_end = folder_for_data_with_slash_at_end\n","\n","        ##############################\n","        #experiment folder related inputs\n","        ##############################\n","        #folder path for experiment\n","        self.experiment_output_folder_path = experiment_output_folder_path\n","\n","        ##############################\n","        #product dataset related inputs\n","        ##############################\n","        #filename of dataset\n","        self.original_dataset_path = folder_for_data_with_slash_at_end + filename_of_original_dataset\n","\n","        #list of indexes to sample from original dataset\n","        self.list_of_indexes_to_sample_from_original_dataset = list_of_indexes_to_sample_from_original_dataset\n","\n","        #sampled product dataframe csv name\n","        self.sampled_product_dataframe_csv_path = experiment_output_folder_path + sampled_product_dataframe_csv_name\n","\n","        #queried products file path\n","        self.queried_products_sample_dataframe_csv_path = experiment_output_folder_path+queried_products_desired_csv_path\n","\n","        ##############################\n","        #preprocessing related inputs\n","        ##############################\n","        self.list_of_preprocessed_text_fields = [] #this is new_field_name_list\n","        self.product_dataframe_with_preprocessed_text_csv_file = ''\n","        self.product_dataframe_with_preprocessed_text_csv_path = ''\n","        self.dictionary_of_converters_for_read_csv = {}\n","\n","        #############################################\n","        #create folder for experiment\n","        import os.path\n","        from os import path\n","        if path.exists(experiment_output_folder_path) == True:\n","          print(\"Folder already exists, so folder creation failed\")\n","        else:\n","          !mkdir $experiment_output_folder_path\n","\n","        ##############################\n","        #initialization functions\n","        ##############################\n","        product_dataframe = load_csv_from_local_directory_to_dataframe(self.original_dataset_path)\n","        product_dataframe = product_dataframe.loc[self.list_of_indexes_to_sample_from_original_dataset,:]\n","        product_dataframe.to_csv(self.sampled_product_dataframe_csv_path, index=False)\n","\n","\n","\n","    def create_queried_products_csv_file_by_sampling_n_values_from_each_query(\n","        self, \n","        number_of_products_per_query_to_sample_from_sampled_product_dataframe,\n","        randomly_sample_from_each_query = False\n","        ):\n","      sampled_product_dataframe = load_csv_from_local_directory_to_dataframe(self.sampled_product_dataframe_csv_path)\n","\n","      queried_products_sample_dataframe = sample_n_products_from_all_query_fields_in_dataframe(sampled_product_dataframe, number_of_products_per_query_to_sample_from_sampled_product_dataframe, randomly_sample_from_each_query)\n","      queried_products_sample_dataframe.to_csv(self.queried_products_sample_dataframe_csv_path, index=False)\n","\n","    def create_queried_products_csv_file_by_giving_index_list_of_desired_queries(\n","        self, \n","        list_of_indexes_of_original_product_dataframe_to_use_as_queried_products,\n","        randomly_sample_from_each_query = False\n","        ):\n","      sampled_product_dataframe = load_csv_from_local_directory_to_dataframe(self.sampled_product_dataframe_csv_path)\n","\n","      queried_products_sample_dataframe = sample_n_products_from_all_query_fields_in_dataframe(sampled_product_dataframe, self.number_of_products_per_query_to_sample, self.randomly_sample_from_each_query)\n","      queried_products_sample_dataframe.to_csv(self.queried_products_sample_dataframe_csv_path, index=False)\n"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"tHFey9EyMV56","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592820312881,"user_tz":240,"elapsed":2181,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"outputId":"f9e492fd-8f68-4694-cf03-82da48982af7"},"source":["\n","##############################\n","##Setup Parameters and datafiles commonly used for experiment\n","##############################\n","experiment = ProductSimilarityExperiment(\n","    folder_for_data_with_slash_at_end = folder_for_data_with_slash_at_end, \n","    filename_of_original_dataset = filename_of_original_dataset, \n","    list_of_indexes_to_sample_from_original_dataset = list_of_indexes_to_sample_from_original_dataset,\n","    experiment_output_folder_path = experiment_output_folder_path,\n","    sampled_product_dataframe_csv_name = sampled_product_dataframe_csv_name,\n","    queried_products_desired_csv_path = queried_products_desired_csv_path\n","    )\n","\n","##############################\n","##Create Query Products List\n","##############################\n","number_of_products_per_query_to_sample_from_sampled_product_dataframe = 2\n","randomly_sample_from_each_query = False\n","experiment.create_queried_products_csv_file_by_sampling_n_values_from_each_query(number_of_products_per_query_to_sample_from_sampled_product_dataframe = 2, randomly_sample_from_each_query = False)\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Folder already exists, so folder creation failed\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qKIVNGWmL-9d","colab_type":"text"},"source":["#Preprocessing Text Function"]},{"cell_type":"markdown","metadata":{"id":"wZEDF2thpKR-","colab_type":"text"},"source":["##INPUT"]},{"cell_type":"code","metadata":{"id":"nLbANfhkMUoA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1592820318010,"user_tz":240,"elapsed":1820,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"outputId":"a3837715-d382-4d01-811c-069bb85cd2bf"},"source":["#preprocessing file path\n","product_dataframe_with_preprocessed_text_csv_file = 'product_dataframe_with_preprocessed_text.csv'\n","\n","#which preprocessing to use\n","list_of_fields_to_preprocess_in_order_of_appending = [['product_title'], ['product_description'], ['product_title', 'product_description'] ]\n","# list_of_fields_to_preprocess_in_order_of_appending = [['product_title']]\n","\n","\n","#choose list of preprocessing function with each element corresponding to same index field in list_of_fields_to_preprocess_in_order_of_appending\n","preprocess_data_object = PreprocessData()\n","preprocessing_function = preprocess_data_object.text_preprocessing_for_WMD\n","list_of_preprocessing_functions_for_each_field_in_order_of_appending = [preprocessing_function,preprocessing_function,preprocessing_function]"],"execution_count":11,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DZLbpYJLpsRd","colab_type":"text"},"source":["##Code"]},{"cell_type":"code","metadata":{"id":"G4sY3VwCpsmN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1592820322813,"user_tz":240,"elapsed":1498,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"outputId":"a4721ec1-c4cc-4e89-b649-b9b7f4ed51fd"},"source":["def preprocess_text_from_dataframe_fields_and_output_csv(\n","    product_similarity_experiment_object, list_of_fields_to_preprocess_in_order_of_appending, \n","    list_of_preprocessing_functions_for_each_field_in_order_of_appending, \n","    product_dataframe_with_preprocessed_text_csv_file\n","    ):\n","\n","  product_dataframe_with_preprocessed_text_csv_path = experiment_output_folder_path+product_dataframe_with_preprocessed_text_csv_file\n","  #below new_field_name is initialized based on list_of_fields_to_preprocess_in_order_of_appending\n","  list_of_preprocessed_text_fields = []\n","  for input_fields_list in list_of_fields_to_preprocess_in_order_of_appending:\n","    new_field_name = \"_AND_\".join(input_fields_list)+'_preprocessed'\n","    list_of_preprocessed_text_fields.append(new_field_name)\n","  dictionary_of_converters_for_read_csv={}\n","  for i in range(len(list_of_preprocessed_text_fields)):\n","    dict_key = list_of_preprocessed_text_fields[i]\n","    dictionary_of_converters_for_read_csv[dict_key] = eval\n","\n","\n","  #############################################\n","  # #preprocess the input dataset text. Save the preprocessing\n","  exp_obj = product_similarity_experiment_object\n","  product_dataframe = load_csv_from_local_directory_to_dataframe(exp_obj.sampled_product_dataframe_csv_path)\n","  preprocess_data_object = PreprocessData()\n","\n","  # list_of_fields_to_preprocess_in_order_of_appending = [['product_title'], ['product_description'], ['product_title', 'product_description'] ]\n","  product_dataframe_with_preprocessed_text = product_dataframe\n","  for i in range(len(list_of_fields_to_preprocess_in_order_of_appending)):\n","    input_fields_list = list_of_fields_to_preprocess_in_order_of_appending[i]\n","    preprocessing_function = list_of_preprocessing_functions_for_each_field_in_order_of_appending[i]\n","    # preprocessing_function = preprocess_data_object.text_preprocessing_for_WMD\n","    product_dataframe_with_preprocessed_text = preprocess_data_object.create_new_field_in_dataframe_with_preprocessed_text_from_list_of_existing_fields(product_dataframe_with_preprocessed_text, input_fields_list, list_of_preprocessed_text_fields[i], preprocessing_function)\n","\n","  product_dataframe_with_preprocessed_text.to_csv(product_dataframe_with_preprocessed_text_csv_path, index=False)\n","  return list_of_preprocessed_text_fields, dictionary_of_converters_for_read_csv\n","\n","list_of_preprocessed_text_fields, dictionary_of_converters_for_read_csv = preprocess_text_from_dataframe_fields_and_output_csv(\n","    product_similarity_experiment_object = experiment, \n","    list_of_fields_to_preprocess_in_order_of_appending = list_of_fields_to_preprocess_in_order_of_appending, \n","    list_of_preprocessing_functions_for_each_field_in_order_of_appending = list_of_preprocessing_functions_for_each_field_in_order_of_appending, \n","    product_dataframe_with_preprocessed_text_csv_file = product_dataframe_with_preprocessed_text_csv_file\n","    )\n","experiment.list_of_preprocessed_text_fields = list_of_preprocessed_text_fields #this is new_field_name_list\n","experiment.product_dataframe_with_preprocessed_text_csv_file = product_dataframe_with_preprocessed_text_csv_file\n","experiment.product_dataframe_with_preprocessed_text_csv_path = experiment.experiment_output_folder_path + experiment.product_dataframe_with_preprocessed_text_csv_file\n","experiment.dictionary_of_converters_for_read_csv = dictionary_of_converters_for_read_csv\n","  "],"execution_count":12,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4sIdx1MIpxdv","colab_type":"text"},"source":["#Base Similarity Score Creation"]},{"cell_type":"markdown","metadata":{"id":"i17fR2Q6qGcc","colab_type":"text"},"source":["##Input"]},{"cell_type":"code","metadata":{"id":"Eo4S8P3HqGJm","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592820326172,"user_tz":240,"elapsed":215,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}}},"source":["#similarity_matrix_path\n","word_embedding_file_name = 'GoogleNews-vectors-negative300.bin'\n","word_embedding_used = 'word2vec'\n","similarity_measure_type = 'wmd'"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XZcQSap0qIPL","colab_type":"text"},"source":["##Code"]},{"cell_type":"markdown","metadata":{"id":"iubO6euVXJRB","colab_type":"text"},"source":["###Load model separately because it takes a long time\n"]},{"cell_type":"code","metadata":{"id":"k48nIkDtW_Yg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1592820388006,"user_tz":240,"elapsed":58594,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"outputId":"5f793f2a-2f0e-4233-c364-6c7b706c1c30"},"source":["#load mode\n","word_embedding_file_path = experiment.folder_for_data_with_slash_at_end + word_embedding_file_name\n","# #load any pretrained word embedding models\n","word2vec_model = load_word2vec_word_embedding_into_KeyedVectors_model(word_embedding_file_path)\n","# model = load_fasttext_word_embedding_into_KeyedVectors_model(word_embedding_file_path)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Sy9RIQf-qFSN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1592821037451,"user_tz":240,"elapsed":584911,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"outputId":"f5b3071e-32a7-42e5-c3ef-4d06a1e65ed1"},"source":["def create_base_similarity_dataframe_csv_for_each_preprocessing_field_using_Word2Vec_and_WMD(\n","  experiment,\n","  word_embedding_file_name,\n","  word_embedding_used, \n","  similarity_measure_type,\n","  word2vec_model\n","  ):\n","  \n","  exp = experiment\n","\n","  similarity_dataframe_csv_path_list = []\n","  for preprocessing_used in exp.list_of_preprocessed_text_fields:\n","    list_of_text_to_combine_into_one_path_for_similarity_dataframe  = [exp.experiment_output_folder_path+'similarity_dataframe', preprocessing_used, word_embedding_used, similarity_measure_type,'.csv']\n","    similarity_dataframe_csv_path = \"_\".join(list_of_text_to_combine_into_one_path_for_similarity_dataframe)\n","    similarity_dataframe_csv_path_list.append(similarity_dataframe_csv_path)\n","  # print(list_of_preprocessed_text_fields)\n","  # print(similarity_dataframe_csv_path_list)\n","\n","\n","  word_embedding_file_path = experiment.folder_for_data_with_slash_at_end + word_embedding_file_name\n","  print(\"word_embedding_file_path\")\n","  ###################\n","  #############################################\n","  #create a similarity model that I can query\n","\n","  for i in range(len(exp.list_of_preprocessed_text_fields)):\n","    print(\"i = \", i)\n","    preprocessing_used = exp.list_of_preprocessed_text_fields[i]\n","    similarity_dataframe_csv_path = similarity_dataframe_csv_path_list[i]\n","\n","    #create corpus of words to calculate similarity from\n","    product_dataframe_with_preprocessed_text = pd.read_csv(exp.product_dataframe_with_preprocessed_text_csv_path, converters=exp.dictionary_of_converters_for_read_csv)\n","    field_name_of_selected_preprocessed_text_to_create_corpus_from = preprocessing_used\n","    # ipdb.set_trace()\n","    corpus = product_dataframe_with_preprocessed_text[field_name_of_selected_preprocessed_text_to_create_corpus_from].tolist()\n","\n","\n","\n","    wmd_object_for_description = WmdSimilarity(corpus, word2vec_model)\n","    # print(\"loaded wmd object\")\n","\n","    #############################################\n","    #Loop through all list of queries\n","    # product_dataframe_with_preprocessed_text = load_csv_from_local_directory_to_dataframe(product_dataframe_with_preprocessed_text_csv_path)\n","    product_dataframe_with_preprocessed_text = pd.read_csv(exp.product_dataframe_with_preprocessed_text_csv_path, converters=exp.dictionary_of_converters_for_read_csv)\n","    field_name_of_selected_preprocessed_text_to_create_corpus_from = preprocessing_used\n","    queried_products_sample_dataframe = load_csv_from_local_directory_to_dataframe(exp.queried_products_sample_dataframe_csv_path)\n","    num_rows_queried_products_sample_dataframe = queried_products_sample_dataframe.shape[0]\n","\n","    # similarity_dataframe = pd.DataFrame(columns=['searched_item_unit_id', 'similar_item_unit_id', 'similarity_score'])\n","    similarity_dictionary = {\n","        'searched_item_unit_id': [],\n","        'similar_item_unit_id': [],\n","        'similarity_score': []\n","    }\n","    for i in range(num_rows_queried_products_sample_dataframe):\n","      queried_product_index = queried_products_sample_dataframe['_unit_id'][i]\n","      # ipdb.set_trace()\n","      queried_product_preprocessed_text = product_dataframe_with_preprocessed_text.loc[product_dataframe_with_preprocessed_text['_unit_id'] == queried_product_index][field_name_of_selected_preprocessed_text_to_create_corpus_from].iloc[0]\n","      # print(queried_product_preprocessed_text)\n","      # queried_product_preprocessed_text = queried_products_sample_dataframe[field_name_of_selected_preprocessed_text_to_create_corpus_from][i]\n","      similar_item_indexes_and_similarity = wmd_object_for_description[queried_product_preprocessed_text]\n","      # print(similar_item_indexes_and_similarity)\n","      searched_item_unit_id = queried_products_sample_dataframe['_unit_id'][i]\n","      # ipdb.set_trace()\n","      for j in range(len(similar_item_indexes_and_similarity)):\n","        similar_item_index = j\n","        similar_item_unit_id = product_dataframe_with_preprocessed_text['_unit_id'][similar_item_index]\n","        similarity_score = similar_item_indexes_and_similarity[j]\n","        # dataframe_row_we_are_appending = pd.DataFrame(data = {'searched_item_unit_id': [searched_item_unit_id], 'similar_item_unit_id': [similar_item_unit_id], 'similarity_score': [similarity_score]})\n","        similarity_dictionary['searched_item_unit_id'].append(searched_item_unit_id)\n","        similarity_dictionary['similar_item_unit_id'].append(similar_item_unit_id)\n","        similarity_dictionary['similarity_score'].append(similarity_score)\n","        # ipdb.set_trace()\n","        # similarity_dataframe.append(dataframe_row_we_are_appending, ignore_index=True)\n","\n","    similarity_dataframe = pd.DataFrame(data=similarity_dictionary)\n","    similarity_dataframe.to_csv(similarity_dataframe_csv_path, index=False)\n","\n","create_base_similarity_dataframe_csv_for_each_preprocessing_field_using_Word2Vec_and_WMD(\n","  experiment = experiment,\n","  word_embedding_file_name = word_embedding_file_name,\n","  word_embedding_used = word_embedding_used, \n","  similarity_measure_type = similarity_measure_type,\n","  word2vec_model = word2vec_model\n","  )"],"execution_count":16,"outputs":[{"output_type":"stream","text":["word_embedding_file_path\n","i =  0\n","i =  1\n","i =  2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7MhiKpBbpyh4","colab_type":"text"},"source":["#Composite Similarity Score Creation"]},{"cell_type":"markdown","metadata":{"id":"1No0amiTpzAV","colab_type":"text"},"source":["##Input"]},{"cell_type":"code","metadata":{"id":"WRgJesT4fkXX","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592822243986,"user_tz":240,"elapsed":283,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}}},"source":["###########################################\n","#list of similarity dataframes\n","#input\n","list_of_files_in_experiment_output_folder_of_base_similarity_measure_csv_files = \\\n","['similarity_dataframe_product_description_preprocessed_word2vec_wmd_.csv',\n","'similarity_dataframe_product_title_preprocessed_word2vec_wmd_.csv']\n","\n","\n","###########################################\n","composite_similarity_dataframe_csv_name = 'composite_similarity_dataframe.csv'\n","\n","###########################################\n","relevance_threshold = 2\n","\n","\n","###########################################\n","num_most_similar_items_for_accuracy = 10\n","\n","\n","###########################################\n","#composite function of all the similarity scores\n","def composite_similarity_function(input_base_similarity_scores: List[float])-> float:\n","  return input_base_similarity_scores[0]*0.5+input_base_similarity_scores[1]*0.5\n"],"execution_count":29,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O2YVtkbKfk10","colab_type":"text"},"source":["##Code"]},{"cell_type":"code","metadata":{"id":"4AXvZOvQfmgM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1592822305619,"user_tz":240,"elapsed":58020,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"outputId":"edfa7b4d-0b3f-4b90-9331-42565886c0ee"},"source":["def create_composite_similarity_dataframe_csv_with_assuming_query_is_available(\n","  experiment,\n","  list_of_files_in_experiment_output_folder_of_base_similarity_measure_csv_files,\n","  composite_similarity_dataframe_csv_name, \n","  relevance_threshold, \n","  num_most_similar_items_for_accuracy, \n","  composite_similarity_function):\n","\n","\n","  #experiment folder path\n","  #input\n","  exp = experiment\n","\n","  experiment_output_folder_path = exp.experiment_output_folder_path\n","\n","  queried_products_sample_dataframe_csv_path = exp.queried_products_sample_dataframe_csv_path\n","\n","  sampled_product_dataframe_csv_path = exp.sampled_product_dataframe_csv_path\n","\n","  composite_similarity_dataframe_csv_path = exp.experiment_output_folder_path + 'composite_similarity_dataframe.csv'\n","\n","  ######################\n","\n","  #calculated based on input\n","  list_of_paths_in_experiment_output_folder_of_base_similarity_measure_csv_files = \\\n","  [experiment_output_folder_path+base_similarity for base_similarity in list_of_files_in_experiment_output_folder_of_base_similarity_measure_csv_files]\n","  print(list_of_paths_in_experiment_output_folder_of_base_similarity_measure_csv_files)\n","\n","  ######################\n","\n","  #load all similarity dataframes\n","  list_of_similarity_dataframes = []\n","  for file in list_of_paths_in_experiment_output_folder_of_base_similarity_measure_csv_files:\n","    list_of_similarity_dataframes.append(load_csv_from_local_directory_to_dataframe(file))\n","\n","  #get list of unique dataframes.\n","  ##load from query list\n","  queried_products_sample_dataframe = load_csv_from_local_directory_to_dataframe(\n","      queried_products_sample_dataframe_csv_path)\n","\n","  product_dataframe = load_csv_from_local_directory_to_dataframe(sampled_product_dataframe_csv_path)\n","\n","  num_rows_queried_products_sample_dataframe = queried_products_sample_dataframe.shape[0]\n","  num_rows_similarity_dataframe = list_of_similarity_dataframes[0].shape[0]\n","  num_rows_product_dataframe = product_dataframe.shape[0]\n","  # composite_similarity_and_accuracy_dataframe = load_csv_from_local_directory_to_dataframe(list_of_paths_in_experiment_output_folder_of_base_similarity_measure_csv_files[0])\n","\n","  composite_similarity_and_accuracy_dictionary = {\n","      'searched_item_unit_id': [None]*num_rows_similarity_dataframe,\n","      'similar_item_unit_id': [None]*num_rows_similarity_dataframe,\n","      'searched_item_query': [None]*num_rows_similarity_dataframe,\n","      'similar_item_query': [None]*num_rows_similarity_dataframe,\n","      'searched_item_relevance': [None]*num_rows_similarity_dataframe,\n","      'similar_item_relevance': [None]*num_rows_similarity_dataframe,\n","      'composite_similarity_score': [None]*num_rows_similarity_dataframe,\n","      'accuracy': [None]*num_rows_similarity_dataframe\n","  }\n","\n","  # temp_composite_similarity_dictionary_for_single_search_query = {\n","  #     'searched_item_unit_id': [None]*num_rows_similarity_dataframe,\n","  #     'similar_item_unit_id': [None]*num_rows_similarity_dataframe,\n","  #     'searched_item_query': [None]*num_rows_similarity_dataframe,\n","  #     'similar_item_query': [None]*num_rows_similarity_dataframe,\n","  #     'searched_item_relevance': [None]*num_rows_similarity_dataframe,\n","  #     'similar_item_relevance': [None]*num_rows_similarity_dataframe,\n","  #     'composite_similarity_score': [None]*num_rows_similarity_dataframe\n","  # }\n","\n","  composite_similarity_and_accuracy_dataframe = pd.DataFrame(data = composite_similarity_and_accuracy_dictionary)\n","\n","\n","  #loop through queries lits. do pandas filter by a certain value on the similarity dataframe to get indexes\n","  for i in range(num_rows_queried_products_sample_dataframe):\n","    queried_product_id = queried_products_sample_dataframe['_unit_id'][i]\n","    # ipdb.set_trace()\n","    \n","    #for a single query item index, loop through similar itemscreate a dataframe for all the composite similarity \n","    #cont: scores for that item. Append to list\n","    index_list = np.where(list_of_similarity_dataframes[0][\"searched_item_unit_id\"] == queried_product_id)[0].tolist()\n","    for j in index_list:\n","      scores_list_for_particular_index = [dataframe['similarity_score'][j] for dataframe in list_of_similarity_dataframes]\n","      # ipdb.set_trace()\n","      composite_similarity_and_accuracy_dataframe['composite_similarity_score'][j] = composite_similarity_function(scores_list_for_particular_index)\n","    \n","      searched_item_dataframe_row = product_dataframe.loc[product_dataframe['_unit_id'] == queried_product_id].iloc[0]\n","      similar_item_product_id = list_of_similarity_dataframes[0]['similar_item_unit_id'][j]\n","      similar_item_dataframe_row = product_dataframe.loc[product_dataframe['_unit_id'] == similar_item_product_id].iloc[0]\n","\n","      composite_similarity_and_accuracy_dataframe['searched_item_unit_id'][j] = queried_product_id\n","      composite_similarity_and_accuracy_dataframe['similar_item_unit_id'][j] = similar_item_product_id\n","      composite_similarity_and_accuracy_dataframe['searched_item_query'][j] = searched_item_dataframe_row['query']\n","      composite_similarity_and_accuracy_dataframe['similar_item_query'][j] = similar_item_dataframe_row['query']\n","      composite_similarity_and_accuracy_dataframe['searched_item_relevance'][j] = searched_item_dataframe_row['relevance']\n","      composite_similarity_and_accuracy_dataframe['similar_item_relevance'][j] = similar_item_dataframe_row['relevance']\n","      \n","    # ipdb.set_trace()\n","    temp_dataframe_for_single_search_query = composite_similarity_and_accuracy_dataframe.iloc[index_list,:]\n","    temp_dataframe_for_single_search_query = temp_dataframe_for_single_search_query.sort_values(by='composite_similarity_score', ascending=False).iloc[:num_most_similar_items_for_accuracy]\n","    accuracy, numMatchingCategory, numNonMatchingCategory = calculate_output_dataframe_accuracy_and_write_in_dataframe(temp_dataframe_for_single_search_query, relevance_threshold)\n","    # ipdb.set_trace()\n","    composite_similarity_and_accuracy_dataframe['accuracy'][index_list[0]] = accuracy\n","\n","  # df = DataFrame\n","  composite_similarity_and_accuracy_dataframe.to_csv(composite_similarity_dataframe_csv_path, index=False)\n","  return composite_similarity_dataframe_csv_path\n","    #create new dataframe from list. Calculate overall accuracy. Dont think i want to sort by similarity score yet\n","    #append list to overall dataframe\n","\n","composite_similarity_dataframe_csv_path = create_composite_similarity_dataframe_csv_with_assuming_query_is_available(\n","  experiment = experiment,\n","  list_of_files_in_experiment_output_folder_of_base_similarity_measure_csv_files = list_of_files_in_experiment_output_folder_of_base_similarity_measure_csv_files,\n","  composite_similarity_dataframe_csv_name = composite_similarity_dataframe_csv_name, \n","  relevance_threshold = relevance_threshold, \n","  num_most_similar_items_for_accuracy = num_most_similar_items_for_accuracy, \n","  composite_similarity_function = composite_similarity_function)\n","\n","experiment.composite_similarity_dataframe_csv_path = composite_similarity_dataframe_csv_path"],"execution_count":30,"outputs":[{"output_type":"stream","text":["['experiment_outputs/experiment3/similarity_dataframe_product_description_preprocessed_word2vec_wmd_.csv', 'experiment_outputs/experiment3/similarity_dataframe_product_title_preprocessed_word2vec_wmd_.csv']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ziyBUMGSqRZx","colab_type":"text"},"source":["#Accuracy Measure"]},{"cell_type":"markdown","metadata":{"id":"fmlrHo8jqRV_","colab_type":"text"},"source":["##Inputs"]},{"cell_type":"code","metadata":{"id":"IWJ7TYJmpjT8","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592824030560,"user_tz":240,"elapsed":220,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}}},"source":["###########################################\n","relevance_threshold = 2\n","\n","\n","\n","###########################################\n","num_most_similar_items_for_accuracy = 10\n","\n","\n","\n","###########################################\n","accuracy_dataframe_csv_path = experiment.experiment_output_folder_path+'accuracy_dataframe.csv'\n","experiment.accuracy_dataframe_csv_path = accuracy_dataframe_csv_path\n","\n","###########################################\n","\n"],"execution_count":35,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WLN3u9WoqQxr","colab_type":"text"},"source":["##Code"]},{"cell_type":"code","metadata":{"id":"I6WIwwRTpzr_","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592824325613,"user_tz":240,"elapsed":566,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}}},"source":["def calculate_accuracy_using_method1(\n","    experiment,\n","    relevance_threshold, \n","    num_most_similar_items_for_accuracy,\n","    accuracy_dataframe_csv_path\n","    ):\n","  exp = experiment\n","  #folder path for experiment\n","  experiment_output_folder_path = exp.experiment_output_folder_path\n","\n","  queried_products_sample_dataframe_csv_path = exp.queried_products_sample_dataframe_csv_path\n","\n","  sampled_product_dataframe_csv_path = exp.sampled_product_dataframe_csv_path\n","\n","  composite_similarity_dataframe_csv_path = exp.composite_similarity_dataframe_csv_path\n","\n","  list_of_preprocessed_text_fields = exp.list_of_preprocessed_text_fields\n","  dictionary_of_converters_for_read_csv = exp.dictionary_of_converters_for_read_csv\n","\n","  #preprocessing file path\n","  product_dataframe_with_preprocessed_text_csv_path = exp.product_dataframe_with_preprocessed_text_csv_path\n","\n","  ###########################################\n","\n","  #Load the composite similarity dataframe\n","  composite_similarity_and_accuracy_dataframe = load_csv_from_local_directory_to_dataframe(composite_similarity_dataframe_csv_path)\n","\n","\n","\n","  #load the search queries, and get the list of search queries\n","  queried_products_sample_dataframe = load_csv_from_local_directory_to_dataframe(\n","      queried_products_sample_dataframe_csv_path)\n","\n","  num_rows_queried_products_sample_dataframe = queried_products_sample_dataframe.shape[0]\n","\n","  #create an accuracy_dataframe with fields, search_item_unit_id, search_item_query, search item accuracy, total average accuracy\n","  accuracy_dictionary = {\n","      'searched_item_unit_id': [None]*num_rows_queried_products_sample_dataframe,\n","      'searched_item_query': [None]*num_rows_queried_products_sample_dataframe,\n","      'searched_item_accuracy': [None]*num_rows_queried_products_sample_dataframe,\n","      'total_average_accuracy_for_experiment': [None]*num_rows_queried_products_sample_dataframe\n","  }\n","  accuracy_dataframe = pd.DataFrame(data = accuracy_dictionary)\n","\n","\n","  #for each search query, index it out from the composite similarity dataframe, \n","  #and output accuracy into accuracy_dataframe\n","\n","  for i in range(num_rows_queried_products_sample_dataframe):\n","    queried_product_id = queried_products_sample_dataframe['_unit_id'][i]\n","    search_item_query = queried_products_sample_dataframe['query'][i]\n","    # ipdb.set_trace()\n","    \n","    temp_dataframe_for_single_search_query = composite_similarity_and_accuracy_dataframe.loc[composite_similarity_and_accuracy_dataframe['searched_item_unit_id'] == queried_product_id]\n","    temp_dataframe_for_single_search_query = temp_dataframe_for_single_search_query.sort_values(by='composite_similarity_score', ascending=False).iloc[:num_most_similar_items_for_accuracy]\n","    accuracy, numMatchingCategory, numNonMatchingCategory = calculate_output_dataframe_accuracy_and_write_in_dataframe(temp_dataframe_for_single_search_query, relevance_threshold)\n","    # ipdb.set_trace()\n","    accuracy_dataframe.iloc[i]['searched_item_unit_id']=queried_product_id\n","    accuracy_dataframe.iloc[i]['searched_item_query']=search_item_query\n","    accuracy_dataframe.iloc[i]['searched_item_accuracy']=accuracy\n","  nparray_of_accuracies = accuracy_dataframe['searched_item_accuracy'].to_numpy()\n","  mean = float(np.mean(nparray_of_accuracies))\n","  accuracy_dataframe.iloc[0]['total_average_accuracy_for_experiment']=mean\n","  accuracy_dataframe.to_csv(accuracy_dataframe_csv_path, index=False)\n","\n","\n","calculate_accuracy_using_method1(\n","    experiment = experiment,\n","    relevance_threshold = relevance_threshold, \n","    num_most_similar_items_for_accuracy = num_most_similar_items_for_accuracy,\n","    accuracy_dataframe_csv_path = accuracy_dataframe_csv_path\n","    )"],"execution_count":38,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QM7LwP27qmop","colab_type":"text"},"source":["##Create debug view"]},{"cell_type":"markdown","metadata":{"id":"53kSsK0gqml2","colab_type":"text"},"source":["##Input"]},{"cell_type":"code","metadata":{"id":"y_k-sckBq0jW","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592824751638,"user_tz":240,"elapsed":855,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}}},"source":["#list of similarity dataframes\n","#input\n","list_of_files_in_experiment_output_folder_of_base_similarity_measure_csv_files = \\\n","['similarity_dataframe_product_description_preprocessed_word2vec_wmd_.csv',\n","'similarity_dataframe_product_title_preprocessed_word2vec_wmd_.csv']\n","\n","\n","###########################################\n","view_to_debug_similarity_dataframe_csv_path = experiment_output_folder_path+'view_to_debug_similarity_dataframe.csv'"],"execution_count":40,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WkhvtLjvqmjW","colab_type":"text"},"source":["##Code"]},{"cell_type":"code","metadata":{"id":"IEUWTfwgq-Ul","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1592824758813,"user_tz":240,"elapsed":3970,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"outputId":"c9051280-4129-4404-dd2a-d6cd6b74be86"},"source":["def create_custom_results_view_for_crowdflower_experiment(\n","    experiment,\n","    list_of_files_in_experiment_output_folder_of_base_similarity_measure_csv_files,\n","    view_to_debug_similarity_dataframe_csv_path\n","):\n","\n","  exp = experiment\n","  \n","  experiment_output_folder_path = exp.experiment_output_folder_path\n","\n","  experiment_output_folder_path = exp.experiment_output_folder_path\n","\n","  queried_products_sample_dataframe_csv_path = exp.queried_products_sample_dataframe_csv_path\n","\n","  sampled_product_dataframe_csv_path = exp.sampled_product_dataframe_csv_path\n","\n","  composite_similarity_dataframe_csv_path = exp.composite_similarity_dataframe_csv_path\n","\n","  list_of_preprocessed_text_fields = exp.list_of_preprocessed_text_fields\n","  dictionary_of_converters_for_read_csv = exp.dictionary_of_converters_for_read_csv\n","\n","  #preprocessing file path\n","  product_dataframe_with_preprocessed_text_csv_path = exp.product_dataframe_with_preprocessed_text_csv_path\n","\n","  #calculated based on input\n","  list_of_paths_in_experiment_output_folder_of_base_similarity_measure_csv_files = \\\n","  [experiment_output_folder_path+base_similarity for base_similarity in list_of_files_in_experiment_output_folder_of_base_similarity_measure_csv_files]\n","  print(list_of_paths_in_experiment_output_folder_of_base_similarity_measure_csv_files)\n","\n","  #######################################\n","\n","  #THIS CODE GETS DEBUG LOG ASSOCIATED WITH A SPECIFIC ACCURACY MEASURE\n","\n","  #Load the composite similarity dataframe\n","  composite_similarity_dataframe = load_csv_from_local_directory_to_dataframe(composite_similarity_dataframe_csv_path)\n","\n","  num_rows_composite_similarity_dataframe = composite_similarity_dataframe.shape[0]\n","\n","  #Load the accuracy dataframe\n","  accuracy_dataframe = load_csv_from_local_directory_to_dataframe(accuracy_dataframe_csv_path)\n","\n","  num_rows_accuracy_dataframe = accuracy_dataframe.shape[0]\n","\n","  product_dataframe_with_preprocessed_text = pd.read_csv(product_dataframe_with_preprocessed_text_csv_path, converters=dictionary_of_converters_for_read_csv)\n","\n","  num_rows_view_to_debug_similarity_dictionary = num_most_similar_items_for_accuracy*num_rows_accuracy_dataframe\n","\n","  #create dataframe\n","  view_to_debug_similarity_dictionary = {\n","      'unit_id_searched_item': [None]*num_rows_view_to_debug_similarity_dictionary, \n","      'title_searched_item': [None]*num_rows_view_to_debug_similarity_dictionary, \n","      'query_searched_item': [None]*num_rows_view_to_debug_similarity_dictionary,\n","      'relevance_searched_item': [None]*num_rows_view_to_debug_similarity_dictionary,\n","      'description_searched_item': [None]*num_rows_view_to_debug_similarity_dictionary,\n","      # preprocessing_used+'_searched_item': target_description_preprocesed_list,\n","      'unit_id_similar_item': [None]*num_rows_view_to_debug_similarity_dictionary,\n","      'title_similar_item': [None]*num_rows_view_to_debug_similarity_dictionary,\n","      'query_similar_item': [None]*num_rows_view_to_debug_similarity_dictionary,\n","      'relevance_similar_item': [None]*num_rows_view_to_debug_similarity_dictionary,\n","      'description_similar_item': [None]*num_rows_view_to_debug_similarity_dictionary,\n","      # preprocessing_used+'_searched_item': dataframe_modified[new_field_name][similar_product_index].tolist(),\n","      'rank': [None]*num_rows_view_to_debug_similarity_dictionary,\n","      'composite_similarity_score': [None]*num_rows_view_to_debug_similarity_dictionary, \n","      'accuracy_for_item': [None]*num_rows_view_to_debug_similarity_dictionary,\n","      'total_average_accuracy_for_experiment': [None]*num_rows_view_to_debug_similarity_dictionary\n","      } \n","  for preprocessing_field_name in list_of_preprocessed_text_fields:\n","    searched_preprocessing_field = preprocessing_field_name+'_searched'\n","    similar_preprocessing_field = preprocessing_field_name+'_similar'\n","    view_to_debug_similarity_dictionary[searched_preprocessing_field] = [None]*num_rows_view_to_debug_similarity_dictionary\n","    view_to_debug_similarity_dictionary[similar_preprocessing_field] = [None]*num_rows_view_to_debug_similarity_dictionary\n","\n","  for base_similarity_measure in list_of_files_in_experiment_output_folder_of_base_similarity_measure_csv_files:\n","    view_to_debug_similarity_dictionary[base_similarity_measure] = [None]*num_rows_view_to_debug_similarity_dictionary\n","\n","  base_similarity_list_of_dataframes = []\n","  for base_similarity_measure_path in list_of_paths_in_experiment_output_folder_of_base_similarity_measure_csv_files:\n","    base_similarity_list_of_dataframes.append(load_csv_from_local_directory_to_dataframe(base_similarity_measure_path))\n","\n","  view_to_debug_similarity_dataframe = pd.DataFrame(data = view_to_debug_similarity_dictionary)\n","\n","  #loop through accuracy dataframe items\n","  for i in range(num_rows_accuracy_dataframe):\n","    #get rows from composite_similarity measure corresponding to a searched unit id of the accuracy dataframe\n","    ##sort by similarity rank and sample the num_most_similar_items_for_accuracy\n","    unit_id_searched_item = accuracy_dataframe.iloc[i]['searched_item_unit_id']\n","    row_searched_item = product_dataframe_with_preprocessed_text.loc[product_dataframe_with_preprocessed_text['_unit_id'] == unit_id_searched_item].iloc[0]\n","    title_searched_item = row_searched_item['product_title']\n","    query_searched_item = row_searched_item['query']\n","    relevance_searched_item = row_searched_item['relevance']\n","    description_searched_item = row_searched_item['product_description']\n","\n","    composite_similarity_dataframe_for_one_item = composite_similarity_dataframe.loc[composite_similarity_dataframe['searched_item_unit_id'] == unit_id_searched_item]\n","    composite_similarity_dataframe_for_one_item = composite_similarity_dataframe_for_one_item.sort_values(by='composite_similarity_score', ascending=False).iloc[:num_most_similar_items_for_accuracy]\n","    \n","    #loop through each of these rows\n","    for j in range(composite_similarity_dataframe_for_one_item.shape[0]):\n","      #get corresponding unit id row for both searched and similar product from product with preprocessed text dataframe and fill the following:\n","      ##unit_id_searched_item, title_searched_item, query_searched_item, relevance_searched_item, description_searched_item, composite_similarity, preprocessed text\n","      # ipdb.set_trace()\n","      unit_id_similar_item = composite_similarity_dataframe_for_one_item.iloc[j]['similar_item_unit_id']\n","      row_similar_item = product_dataframe_with_preprocessed_text.loc[product_dataframe_with_preprocessed_text['_unit_id'] == unit_id_similar_item].iloc[0]\n","      title_similar_item = row_similar_item['product_title']\n","      query_similar_item = row_similar_item['query']\n","      relevance_similar_item = row_similar_item['relevance']\n","      description_similar_item = row_similar_item['product_description']\n","\n","      #create a index for view_to_debug_similarity_dataframe\n","      index_for_view_to_debug_similarity_dataframe = i*num_most_similar_items_for_accuracy+j\n","\n","      view_to_debug_similarity_dataframe.iloc[index_for_view_to_debug_similarity_dataframe]['unit_id_searched_item'] = unit_id_searched_item\n","      view_to_debug_similarity_dataframe.iloc[index_for_view_to_debug_similarity_dataframe]['row_searched_item'] = row_searched_item\n","      view_to_debug_similarity_dataframe.iloc[index_for_view_to_debug_similarity_dataframe]['title_searched_item'] = title_searched_item\n","      view_to_debug_similarity_dataframe.iloc[index_for_view_to_debug_similarity_dataframe]['query_searched_item'] = query_searched_item\n","      view_to_debug_similarity_dataframe.iloc[index_for_view_to_debug_similarity_dataframe]['relevance_searched_item'] = relevance_searched_item\n","      view_to_debug_similarity_dataframe.iloc[index_for_view_to_debug_similarity_dataframe]['description_searched_item'] = description_searched_item\n","\n","      view_to_debug_similarity_dataframe.iloc[index_for_view_to_debug_similarity_dataframe]['unit_id_similar_item'] = unit_id_similar_item\n","      view_to_debug_similarity_dataframe.iloc[index_for_view_to_debug_similarity_dataframe]['row_similar_item'] = row_similar_item\n","      view_to_debug_similarity_dataframe.iloc[index_for_view_to_debug_similarity_dataframe]['title_similar_item'] = title_similar_item\n","      view_to_debug_similarity_dataframe.iloc[index_for_view_to_debug_similarity_dataframe]['query_similar_item'] = query_similar_item\n","      view_to_debug_similarity_dataframe.iloc[index_for_view_to_debug_similarity_dataframe]['relevance_similar_item'] = relevance_similar_item\n","      view_to_debug_similarity_dataframe.iloc[index_for_view_to_debug_similarity_dataframe]['description_similar_item'] = description_similar_item\n","\n","      # ipdb.set_trace()\n","\n","      #initialize the preprocessed text\n","      for k in range(len(list_of_preprocessed_text_fields)):\n","        searched_field = list_of_preprocessed_text_fields[k] + '_searched'\n","        similar_field = list_of_preprocessed_text_fields[k] + '_similar'\n","        view_to_debug_similarity_dataframe.iloc[index_for_view_to_debug_similarity_dataframe][searched_field] =  row_searched_item[list_of_preprocessed_text_fields[k]]\n","        view_to_debug_similarity_dataframe.iloc[index_for_view_to_debug_similarity_dataframe][similar_field] =  row_similar_item[list_of_preprocessed_text_fields[k]]\n","\n","      \n","      #from same row, get the base_similarities a well\n","      for k in range(len(list_of_files_in_experiment_output_folder_of_base_similarity_measure_csv_files)):\n","        base_similarity_measure = list_of_files_in_experiment_output_folder_of_base_similarity_measure_csv_files[k]\n","        base_similarity_dataframe = base_similarity_list_of_dataframes[k]\n","        # ipdb.set_trace()\n","        score = base_similarity_dataframe.loc[(base_similarity_dataframe['searched_item_unit_id'] == unit_id_searched_item) & (base_similarity_dataframe['similar_item_unit_id'] == unit_id_similar_item)].iloc[0]['similarity_score']\n","        view_to_debug_similarity_dataframe.iloc[index_for_view_to_debug_similarity_dataframe][base_similarity_measure] = score   \n","\n","      #get composite similarity measure\n","      # ipdb.set_trace()\n","      view_to_debug_similarity_dataframe.iloc[index_for_view_to_debug_similarity_dataframe]['composite_similarity_score'] = composite_similarity_dataframe_for_one_item.iloc[j]['composite_similarity_score']\n","\n","    index_min_of_write_in_view = i*num_most_similar_items_for_accuracy\n","    index_max_of_write_in_view = (i+1)*num_most_similar_items_for_accuracy\n","\n","    #fill in the accuracy_for_item\n","    view_to_debug_similarity_dataframe.loc[index_min_of_write_in_view,'accuracy_for_item'] = accuracy_dataframe.iloc[i]['searched_item_accuracy']\n","\n","    #create a rank\n","    view_to_debug_similarity_dataframe.loc[index_min_of_write_in_view:index_max_of_write_in_view,'rank'] = view_to_debug_similarity_dataframe.iloc[index_min_of_write_in_view:index_max_of_write_in_view]['composite_similarity_score'].rank(method='min', ascending = False)\n","    # ipdb.set_trace()\n","  #fill in total_average_accuracy\n","  # ipdb.set_trace()\n","  view_to_debug_similarity_dataframe.iloc[0]['total_average_accuracy_for_experiment'] = accuracy_dataframe.iloc[0]['total_average_accuracy_for_experiment']\n","  view_to_debug_similarity_dataframe.to_csv(view_to_debug_similarity_dataframe_csv_path, index=False)\n","\n","create_custom_results_view_for_crowdflower_experiment(\n","    experiment = experiment,\n","    list_of_files_in_experiment_output_folder_of_base_similarity_measure_csv_files = list_of_files_in_experiment_output_folder_of_base_similarity_measure_csv_files,\n","    view_to_debug_similarity_dataframe_csv_path = view_to_debug_similarity_dataframe_csv_path\n",")"],"execution_count":41,"outputs":[{"output_type":"stream","text":["['experiment_outputs/experiment3/similarity_dataframe_product_description_preprocessed_word2vec_wmd_.csv', 'experiment_outputs/experiment3/similarity_dataframe_product_title_preprocessed_word2vec_wmd_.csv']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"g3NuNPPXqmg0","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"FUIvKbfuqmd1","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"OTVvzKUp8nZ1","colab_type":"text"},"source":["##########################################################################"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DAHc55pgDXg4"},"source":["##########################################################################"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"836yXhoYDXpv"},"source":["##########################################################################"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qN4Zk5GpDXuZ"},"source":["##########################################################################"]},{"cell_type":"markdown","metadata":{"id":"-Qnz6ppmYEv2","colab_type":"text"},"source":["\n","MOUNT GOOGLE DRIVE SO YOU CAN ACCESS FILES FROM GOOGLE DRIVE"]},{"cell_type":"markdown","metadata":{"id":"KLcpc588YGsi","colab_type":"text"},"source":["Pip Install packages, and import general packages"]},{"cell_type":"code","metadata":{"id":"5-2lqDS31lFS","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592820273406,"user_tz":240,"elapsed":2059,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}}},"source":["\n","from collections import OrderedDict\n","import numpy as np\n","import spacy\n","from spacy.lang.en.stop_words import STOP_WORDS\n","\n","nlp = spacy.load('en_core_web_sm')\n","\n","class TextRank4Keyword():\n","    \"\"\"Extract keywords from text\"\"\"\n","    \n","    def __init__(self):\n","        self.d = 0.85 # damping coefficient, usually is .85\n","        self.min_diff = 1e-5 # convergence threshold\n","        self.steps = 10 # iteration steps\n","        self.node_weight = None # save keywords and its weight\n","\n","    \n","    def set_stopwords(self, stopwords):  \n","        \"\"\"Set stop words\"\"\"\n","        for word in STOP_WORDS.union(set(stopwords)):\n","            lexeme = nlp.vocab[word]\n","            lexeme.is_stop = True\n","    \n","    def sentence_segment(self, doc, candidate_pos, lower):\n","        \"\"\"Store those words only in cadidate_pos\"\"\"\n","        sentences = []\n","        for sent in doc.sents:\n","            selected_words = []\n","            for token in sent:\n","                # Store words only with cadidate POS tag\n","                if token.pos_ in candidate_pos and token.is_stop is False:\n","                    if lower is True:\n","                        selected_words.append(token.text.lower())\n","                    else:\n","                        selected_words.append(token.text)\n","            sentences.append(selected_words)\n","        return sentences\n","        \n","    def get_vocab(self, sentences):\n","        \"\"\"Get all tokens\"\"\"\n","        vocab = OrderedDict()\n","        i = 0\n","        for sentence in sentences:\n","            for word in sentence:\n","                if word not in vocab:\n","                    vocab[word] = i\n","                    i += 1\n","        return vocab\n","    \n","    def get_token_pairs(self, window_size, sentences):\n","        \"\"\"Build token_pairs from windows in sentences\"\"\"\n","        token_pairs = list()\n","        for sentence in sentences:\n","            for i, word in enumerate(sentence):\n","                for j in range(i+1, i+window_size):\n","                    if j >= len(sentence):\n","                        break\n","                    pair = (word, sentence[j])\n","                    if pair not in token_pairs:\n","                        token_pairs.append(pair)\n","        return token_pairs\n","        \n","    def symmetrize(self, a):\n","        return a + a.T - np.diag(a.diagonal())\n","    \n","    def get_matrix(self, vocab, token_pairs):\n","        \"\"\"Get normalized matrix\"\"\"\n","        # Build matrix\n","        vocab_size = len(vocab)\n","        g = np.zeros((vocab_size, vocab_size), dtype='float')\n","        for word1, word2 in token_pairs:\n","            i, j = vocab[word1], vocab[word2]\n","            g[i][j] = 1\n","            \n","        # Get Symmeric matrix\n","        g = self.symmetrize(g)\n","        \n","        # Normalize matrix by column\n","        norm = np.sum(g, axis=0)\n","        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n","        \n","        return g_norm\n","\n","    \n","    def get_keywords(self, number=10):\n","        \"\"\"Print top number keywords\"\"\"\n","        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n","        for i, (key, value) in enumerate(node_weight.items()):\n","            print(key + ' - ' + str(value))\n","            if i > number:\n","                break\n","\n","    def get_list_of_strings_of_the_top_textrank_keywords_in_order_of_textrank(self, number=10):\n","      list_of_top_keywords = []\n","\n","      \"\"\"Print top number keywords\"\"\"\n","      node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n","      for i, (key, value) in enumerate(node_weight.items()):\n","          #print(key + ' - ' + str(value))\n","          list_of_top_keywords.append(key.lower())\n","          if i > number:\n","            break\n","      return list_of_top_keywords       \n","        \n","    def analyze(self, text, \n","                candidate_pos=['NOUN', 'PROPN'], \n","                window_size=4, lower=False, stopwords=list()):\n","        \"\"\"Main function to analyze text\"\"\"\n","        \n","        # Set stop words\n","        self.set_stopwords(stopwords)\n","        \n","        # Pare text by spaCy\n","        doc = nlp(text)\n","        \n","        # Filter sentences\n","        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n","        \n","        # Build vocabulary\n","        vocab = self.get_vocab(sentences)\n","        \n","        # Get token_pairs from windows\n","        token_pairs = self.get_token_pairs(window_size, sentences)\n","        \n","        # Get normalized matrix\n","        g = self.get_matrix(vocab, token_pairs)\n","        \n","        # Initionlization for weight(pagerank value)\n","        pr = np.array([1] * len(vocab))\n","        \n","        # Iteration\n","        previous_pr = 0\n","        for epoch in range(self.steps):\n","            pr = (1-self.d) + self.d * np.dot(g, pr)\n","            if abs(previous_pr - sum(pr))  < self.min_diff:\n","                break\n","            else:\n","                previous_pr = sum(pr)\n","\n","        # Get weight for each node\n","        node_weight = dict()\n","        for word, index in vocab.items():\n","            node_weight[word] = pr[index]\n","        \n","        self.node_weight = node_weight"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"FWii_7GVXHEn","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592820266097,"user_tz":240,"elapsed":997,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}}},"source":["#download pretrained word embedding model and load it\n","##download a file\n","##unzip a file\n","def download_and_unzip_pretrained_word_embedding(url_to_word_embedding_zip: str, download_destination_folder_with_slash_at_end: str) -> Tuple[bool, str]:\n","  #download if unzipped file exists already\n","  #if not, unzip and check if file exists after unzipping\n","  #if file exists, return 1 for success\n","  import os.path\n","  from os import path\n","  zipped_file_name = os.path.basename(url_to_word_embedding_zip)\n","  unzipped_file_name = os.path.splitext(zipped_file_name)[0]\n","\n","  path_of_zipped_word_embedding = download_destination_folder_with_slash_at_end + zipped_file_name\n","  desired_path_of_unzipped_word_embedding = download_destination_folder_with_slash_at_end + unzipped_file_name\n","  zip_file_extension = os.path.splitext(url_to_word_embedding_zip)[1][1:]\n","\n","  if path.exists(desired_path_of_unzipped_word_embedding) == False:\n","    !wget -c $url_to_word_embedding_zip -P $download_destination_folder_with_slash_at_end\n","    if zip_file_extension == 'gz':\n","      !gunzip $path_of_zipped_word_embedding\n","    elif zip_file_extension == 'zip':\n","      print(\"zip_file_extension\", \" not implemented yet\")\n","      return (False, None)\n","    else:\n","      print(\"This function doesnt handle zipped files with extension: \", zip_file_extension)\n","      return (False, None)\n","  else:\n","    print(\"datasetAlreadyExists\")\n","\n","  if path.exists(desired_path_of_unzipped_word_embedding) == True:\n","    successful_in_creating_word_embedding = True\n","  else:\n","    successful_in_creating_word_embedding = False\n","  return (successful_in_creating_word_embedding, desired_path_of_unzipped_word_embedding)\n","\n","##load word embedding\n","def load_word_embedding_into_KeyedVectors_model(word_embedding_file_path: str) -> gensim.models.keyedvectors.Word2VecKeyedVectors:\n","  from gensim.models.keyedvectors import KeyedVectors\n","  model = KeyedVectors.load_word2vec_format(word_embedding_file_path, binary=True)\n","  return model\n","\n","def load_word2vec_word_embedding_into_KeyedVectors_model(word_embedding_file_path: str) -> gensim.models.keyedvectors.Word2VecKeyedVectors:\n","  model = KeyedVectors.load_word2vec_format(word_embedding_file_path, binary=True)\n","  return model\n","\n","def load_fasttext_word_embedding_into_KeyedVectors_model(word_embedding_file_path: str) -> gensim.models.keyedvectors.Word2VecKeyedVectors:\n","  model = FastText.load_fasttext_format(model_path)\n","  return model\n","\n","##delete below because not used\n","# #load dataset\n","# #extract desired features from dataset\n","# def load_dataset_into_pandas(url_of_dataset: str, tuple_of_starting_and_ending_exclusive_index_of_sample: tuple) -> pd.core.frame.DataFrame:\n","#   #Create pandas dataframe on dataset\n","#   import pandas as pd\n","#   #https://data.world/crowdflower/ecommerce-search-relevance\n","#   df = pd.read_csv(url_of_dataset, encoding = \"ISO-8859-1\")\n","#   start,end = tuple_of_starting_and_ending_exclusive_index_of_sample\n","#   df = df.iloc[start:end]\n","#   return df\n","\n","def load_csv_from_local_directory_to_dataframe(path_of_csv: str, csv_encoding: str = \"ISO-8859-1\") -> pd.core.frame.DataFrame:\n","  import os.path\n","  from os import path\n","  import pandas as pd\n","\n","  product_dataframe = pd.read_csv(path_of_csv, encoding = csv_encoding)\n","  return product_dataframe\n","\n","def sample_n_products_from_all_query_fields_in_dataframe(product_dataframe: pd.core.frame.DataFrame, number_of_products_per_query: int, randomly_sample_from_each_query: bool = False) -> pd.core.frame.DataFrame:\n","  from random import sample  \n","  list_of_queries_field = product_dataframe['query'].unique().tolist()\n","\n","  indexes_of_query_products = []\n","  for i in range(len(list_of_queries_field)):\n","    if isinstance(pd.Index(product_dataframe['query']).get_loc(list_of_queries_field[i]), int):\n","      indexes_of_query_products.append(pd.Index(product_dataframe['query']).get_loc(list_of_queries_field[i]))\n","    else:\n","      bool_array_indicating_dataframe_indexes_where_query_matches = pd.Index(product_dataframe['query']).get_loc(list_of_queries_field[i])\n","      list_of_indexes_for_particular_query = [i for i, x in enumerate(bool_array_indicating_dataframe_indexes_where_query_matches) if x]\n","      if randomly_sample_from_each_query:\n","        indexes_of_query_products.extend(sample(list_of_indexes_for_particular_query, k = number_of_products_per_query))\n","      else:\n","        indexes_of_query_products.extend(list_of_indexes_for_particular_query[0:number_of_products_per_query])\n","  \n","  num_rows_dataframe = product_dataframe.shape[0]\n","  total_indexes_set = set(range(0,num_rows_dataframe))\n","  indexes_of_query_products_set = set(indexes_of_query_products)\n","  indexes_of_nonquery_products = list(total_indexes_set.difference(indexes_of_query_products_set))\n","  \n","  queried_products_sample_dataframe = product_dataframe.copy()\n","  queried_products_sample_dataframe = queried_products_sample_dataframe.drop(indexes_of_nonquery_products)\n","  return queried_products_sample_dataframe \n","\n","\n","#preprocess data\n","class PreprocessData:\n","  \"\"\"\n","  A class used to find the orientation of the robot with respect to a lane over time.\n","  \"\"\"\n","  def __init__(self):\n","    \"\"\"\n","    Create a PreprocessData object\n","    Parameters:\n","      - \n","    \"\"\"\n","    from nltk import download\n","    from nltk import word_tokenize\n","    # Import and download stopwords from NLTK.\n","    from nltk.corpus import stopwords\n","    \n","    download('punkt')  # Download data for tokenizer.\n","    download('stopwords')  # Download stopwords list.\n","\n","    # Remove stopwords.\n","    self.stop_words = stopwords.words('english')\n","\n","  def make_text_lower_case(self, text: str) -> str:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    return text.lower()\n","\n","  def tokenize_string_to_list_of_separate_words(self, text_string: str) -> List[str]:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    from nltk import word_tokenize\n","    return word_tokenize(text_string) \n","\n","  def remove_stopwords_from_text(self, text_list: List[str]) -> List[str]:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    return [w for w in text_list if not w in self.stop_words]\n","\n","  def remove_numbers_and_punctuation_from_text(self, text_list: List[str]) -> List[str]:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    return [w for w in text_list if w.isalpha()]\n","\n","  def text_preprocessing_for_WMD(self, text: str) -> List[str]:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    text = self.make_text_lower_case(text)\n","    text_list = self.tokenize_string_to_list_of_separate_words(text)\n","    text_list = self.remove_stopwords_from_text(text_list)\n","    text_list = self.remove_numbers_and_punctuation_from_text(text_list)\n","    return text_list\n","\n","  def text_preprocessing_with_textrank_for_WMD(self, text: str) -> List[str]:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    text = self.make_text_lower_case(text)\n","    \n","    tr4w = TextRank4Keyword()\n","    tr4w.analyze(text, candidate_pos = ['NOUN', 'PROPN', 'VERB'], window_size=4, lower=False)\n","    text_list = tr4w.get_list_of_strings_of_the_top_textrank_keywords_in_order_of_textrank(10)\n","\n","    # text_list = self.tokenize_string_to_list_of_separate_words(text)\n","    text_list = self.remove_stopwords_from_text(text_list)\n","    text_list = self.remove_numbers_and_punctuation_from_text(text_list)\n","    return text_list\n","\n","  def create_new_field_with_preprocessed_text_in_dataframe(self, dataframe: pd.core.frame.DataFrame, field_to_preprocess: str, new_field_name: str) -> pd.core.frame.DataFrame:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    num_rows_dataframe = len(dataframe.index)\n","    preprocessed_text_list = [np.nan] * num_rows_dataframe\n","    empty_text = []\n","    for ind in dataframe.index:\n","      field_text = dataframe[field_to_preprocess][ind]\n","      #check if field is empty or nan\n","      if field_text == field_text:\n","        preprocessed_text = self.text_preprocessing_for_WMD(field_text)\n","        # preprocessed_text = self.text_preprocessing_with_textrank_for_WMD(field_text)\n","        if preprocessed_text != []:\n","          preprocessed_text_list[ind] = preprocessed_text\n","        else:\n","          preprocessed_text_list[ind] = empty_text\n","      else:\n","        preprocessed_text_list[ind] = empty_text\n","\n","    dataframe.insert(len(dataframe.columns), new_field_name, preprocessed_text_list, True)\n","    return dataframe\n","    \n","  def create_new_field_with_preprocessed_text_from_multiple_fields_in_dataframe(self, dataframe: pd.core.frame.DataFrame, list_of_fields_to_preprocess_in_order_of_appending: List[str], new_field_name: str) -> pd.core.frame.DataFrame:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    num_rows_dataframe = len(dataframe.index)\n","    preprocessed_text_list = [np.nan] * num_rows_dataframe\n","    empty_text = []\n","\n","    for ind in dataframe.index:\n","      list_of_field_text = []\n","      for i in range(len(list_of_fields_to_preprocess_in_order_of_appending)):\n","        if dataframe[list_of_fields_to_preprocess_in_order_of_appending[i]][ind] == dataframe[list_of_fields_to_preprocess_in_order_of_appending[i]][ind]:\n","          list_of_field_text.append(dataframe[list_of_fields_to_preprocess_in_order_of_appending[i]][ind])\n","      field_text = \" \".join(filter(None, list_of_field_text))\n","      #check if field is empty or nan\n","      if field_text == field_text:\n","        preprocessed_text = self.text_preprocessing_for_WMD(field_text)\n","        # preprocessed_text = self.text_preprocessing_with_textrank_for_WMD(field_text)\n","        if preprocessed_text != []:\n","          preprocessed_text_list[ind] = preprocessed_text\n","        else:\n","          preprocessed_text_list[ind] = empty_text\n","      else:\n","        preprocessed_text_list[ind] = empty_text\n","\n","    dataframe.insert(len(dataframe.columns), new_field_name, preprocessed_text_list, True)\n","    return dataframe     \n","\n","  def create_new_field_in_dataframe_with_preprocessed_text_from_list_of_existing_fields(self, dataframe: pd.core.frame.DataFrame, list_of_fields_to_preprocess_in_order_of_appending: List[str], new_field_name: str, preprocessing_function: Callable[[str],List[str]]) -> pd.core.frame.DataFrame:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    num_rows_dataframe = len(dataframe.index)\n","    preprocessed_text_list = [np.nan] * num_rows_dataframe\n","    empty_text = []\n","\n","    for ind in dataframe.index:\n","      list_of_field_text = []\n","      for i in range(len(list_of_fields_to_preprocess_in_order_of_appending)):\n","        if dataframe[list_of_fields_to_preprocess_in_order_of_appending[i]][ind] == dataframe[list_of_fields_to_preprocess_in_order_of_appending[i]][ind]:\n","          list_of_field_text.append(dataframe[list_of_fields_to_preprocess_in_order_of_appending[i]][ind])\n","      field_text = \" \".join(filter(None, list_of_field_text))\n","      #check if field is empty or nan\n","      if field_text == field_text:\n","        preprocessed_text = preprocessing_function(field_text)\n","        # preprocessed_text = self.text_preprocessing_for_WMD(field_text)\n","        # preprocessed_text = self.text_preprocessing_with_textrank_for_WMD(field_text)\n","        if preprocessed_text != []:\n","          preprocessed_text_list[ind] = preprocessed_text\n","        else:\n","          preprocessed_text_list[ind] = empty_text\n","      else:\n","        preprocessed_text_list[ind] = empty_text\n","\n","    dataframe.insert(len(dataframe.columns), new_field_name, preprocessed_text_list, True)\n","    return dataframe\n","\n","#find WMD similarity\n","class WordMoversDistanceSimilarity:\n","  \"\"\"\n","  A class used to find the orientation of the robot with respect to a lane over time.\n","  \"\"\"\n","  def __init__(self, keyed_vector_model: gensim.models.keyedvectors.Word2VecKeyedVectors, list_of_document_lists: List[List[str]], num_similar_queries: int):\n","    \"\"\"\n","    Create a PreprocessData object\n","    Parameters:\n","      - \n","    \"\"\"\n","    self.model = keyed_vector_model\n","    self.wmd_corpus = list_of_document_lists\n","    self.num_similar_queries = num_similar_queries\n","    self.preprocess_data_object = PreprocessData()\n","    self.similarityMatrix = 0\n","    self.similar_document_tuple_of_indexes_and_similarity_scores = 0\n","\n","  def create_similarity_matrix(self):\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    from gensim.similarities import WmdSimilarity\n","    self.similarityMatrix = WmdSimilarity(self.wmd_corpus, self.model, self.num_similar_queries)\n","\n","  def find_similar_document_index_and_similarity(self, query_text: str) -> List[tuple]:\n","    query_text = preprocess(source_doc)\n","    self.similar_document_tuple_of_indexes_and_similarity_scores = instance[query]  # A query is simply a \"look-up\" in the similarity class.\n","    return self.similar_document_tuple_of_indexes_and_similarity_scores\n","\n","class PrintingResultAndCalculateLoss:\n","  \"\"\"\n","  A class used to find the orientation of the robot with respect to a lane over time.\n","  \"\"\"\n","  def __init__(self, similar_document_tuple_of_indexes_and_similarity_scores: int, data_frame: pd.core.frame.DataFrame):\n","    \"\"\"\n","    Create a PreprocessData object\n","    Parameters:\n","      - \n","    \"\"\"\n","    self.similar_document_tuple_of_indexes_and_similarity_scores = similar_document_tuple_of_indexes_and_similarity_scores\n","    self.data_frame = data_frame\n","\n","  def calculate_accuracy_of_similarity_prediction_for_a_query_text_from_actual_(index_of_query_product, score_cutoff):\n","    \"\"\"\n","    Calculate accuracy for a given single similar products query. This query text must be the actual data from the table, so that it will have a \n","    label \"query\" field and \"relevance\" field.\n","\n","    if index_of_query_product \"relevance\" field < score_cutoff: #we dont want to use query items in which the similarity to the search query was low.\n","      return accuracy = None\n","    else: \n","      for every index in similar_document_tuple_of_indexes_and_similarity_scores:\n","        number_of_correct_similar_products += (index_of_query_product \"query\" field == similar_index \"query\" field)*(similar_index \"relevance\" field >= score_cutoff)\n","    accuracy = number_of_correct_similar_products/num_similar_queries\n","\n","    Parameters:\n","      - \n","    \"\"\"\n","  #in order to be classified correctly, the similar product has to be in same search query as target category\n","  #and has to have a relavance score >= score cutoff.\n","  #I know this is flawed because there might be a product in the wrong search query but is still similar to the target product.\n","  #but overall, im hoping this case to be less\n","\n","  #\n","    num_similar_queries = len(index_of_query_product)\n","\n","    search_query = df['query'][sims[i][0]]\n","    for i in range(num_similar_queries):\n","      index_of_\n","\n","def calculate_output_dataframe_accuracy_and_write_in_dataframe(output_dataframe, relevance_threshold):\n","  numRows = output_dataframe[\"searched_item_unit_id\"].shape[0]\n","  numMatchingCategory = 0\n","  numNonMatchingCategory = 0\n","  # ipdb.set_trace()\n","  if output_dataframe.iloc[0][\"searched_item_relevance\"] < relevance_threshold:\n","    accuracy, numMatchingCategory, numNonMatchingCategory = (-1,-1,-1)\n","    return accuracy, numMatchingCategory, numNonMatchingCategory\n","  for i in range(numRows):  \n","    if output_dataframe.iloc[0][\"searched_item_query\"] == output_dataframe.iloc[i][\"similar_item_query\"]:\n","      numMatchingCategory += 1\n","    else:\n","      numNonMatchingCategory +=1\n","  # print(\"numMatchingCategory = \", numMatchingCategory)\n","  # print(\"numNonMatchingCategory = \", numNonMatchingCategory)\n","  ##ipdb.set_trace()\n","  accuracy = numMatchingCategory/numRows\n","  # output_dataframe[\"WMDAccuracy\"][0] = accuracy\n","  return accuracy, numMatchingCategory, numNonMatchingCategory\n","\n"],"execution_count":5,"outputs":[]}]}