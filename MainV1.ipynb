{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MainV1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"xpu-w4fP_uKk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":88},"executionInfo":{"status":"ok","timestamp":1592568055063,"user_tz":240,"elapsed":629,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"outputId":"268ca8ef-0c76-4488-e3cb-187a257256d4"},"source":["import re\n","from nltk import word_tokenize\n","from nltk import download\n","download('punkt')  # Download data for tokenizer.\n","text = '''\n","eBay item number:371252240431\n","\n","\n","\tSeller assumes all responsibility for this listing.\n","\t\n","\t\tLast updated on\n","\t\t&nbsp;May 04, 2015 10:34:20 PDT&nbsp;\n","\t\tView all revisions\n","\t\t\n","\t\n","\n","\t\t\n","\t\t\t\n","\t\t\t\t\t<strong>Item specifics</strong>\n","\t\t\t\t\t<table>\n","\t\t\t\t\t\t\n","\t\t\t\t\t\t<tr>\n","\t\t\t\t\t\t\t\t\t<td>\n","\t\t\t\t\t\t\t\t\t \t\t\tCondition:</td>\n","\t\t\t\t\t\t\t\t\t\t\t \n","\t\t\t\t\t\t\t\t\t\t\t<td>\n","\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tNew: A brand-new, unused, unopened, undamaged item in its original packaging (where packaging is \n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tapplicable). Packaging should be the same as what is found in a retail store, unless the item is handmade or was packaged by the manufacturer in non-retail packaging, such as an unprinted box or plastic bag. See the seller's listing for full details.\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tSee all condition definitions<strong>- opens in a new window or tab</strong>\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t... Read more<strong>about the condition</strong>\t\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\t\t\t\t\t\t\t\t\t\t</td>\n","\t\t\t\t\t\t\t\t\t\t<td>\n","\t\t\t\t\t\t\t\t\t \t\t\tBrand: </td>\n","\t\t\t\t\t\t\t\t\t\t\t \n","\t\t\t\t\t\t\t\t\t\t\t<td>\n","\t\t\t\t\t\t\t\t\t\t\t\t<strong>Toshiba</strong></td>\n","\t\t\t\t\t\t\t\t\t\t</tr>\n","\t\t\t\t\t\t\t<tr>\n","\t\t\t\t\t\t\t\t\t<td>\n","\t\t\t\t\t\t\t\t\t \t\t\tMPN: </td>\n","\t\t\t\t\t\t\t\t\t\t\t \n","\t\t\t\t\t\t\t\t\t\t\t<td>\n","\t\t\t\t\t\t\t\t\t\t\t\t<strong>HDTB205XK3AA</strong></td>\n","\t\t\t\t\t\t\t\t\t\t<td>\n","\t\t\t\t\t\t\t\t\t \t\t\tUPC: </td>\n","\t\t\t\t\t\t\t\t\t\t\t \n","\t\t\t\t\t\t\t\t\t\t\t<td>\n","\t\t\t\t\t\t\t\t\t\t\t\t<strong>022265494950</strong></td>\n","\t\t\t\t\t\t\t\t\t\t</tr>\n","\t\t\t\t\t\t\t\n","\t\t\t\t\t\t\t</table>\n","\t\t\t\t\t\t\n","\t\t\t\t\t\n","\t\t\t\n","\t\n","\t\t\t\t\n","\t\t\t\t\t\t<strong>Adorama</strong>\n","\t\t\t\t\t\t\n","\t\t\t\t\t\t\tVisit my eBay store &nbsp;\n","              '''\n","pattern = '<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});'\n","cleanr = re.compile(pattern) \n","cleantext = re.sub(cleanr, '', text)\n","tokenized_clean= word_tokenize(cleantext)\n","print(tokenized_clean)\n","\n","###################################\n","# new_field_name_list"],"execution_count":11,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","['eBay', 'item', 'number:371252240431', 'Seller', 'assumes', 'all', 'responsibility', 'for', 'this', 'listing', '.', 'Last', 'updated', 'on', 'May', '04', ',', '2015', '10:34:20', 'PDT', 'View', 'all', 'revisions', 'Item', 'specifics', 'Condition', ':', 'New', ':', 'A', 'brand-new', ',', 'unused', ',', 'unopened', ',', 'undamaged', 'item', 'in', 'its', 'original', 'packaging', '(', 'where', 'packaging', 'is', 'applicable', ')', '.', 'Packaging', 'should', 'be', 'the', 'same', 'as', 'what', 'is', 'found', 'in', 'a', 'retail', 'store', ',', 'unless', 'the', 'item', 'is', 'handmade', 'or', 'was', 'packaged', 'by', 'the', 'manufacturer', 'in', 'non-retail', 'packaging', ',', 'such', 'as', 'an', 'unprinted', 'box', 'or', 'plastic', 'bag', '.', 'See', 'the', 'seller', \"'s\", 'listing', 'for', 'full', 'details', '.', 'See', 'all', 'condition', 'definitions-', 'opens', 'in', 'a', 'new', 'window', 'or', 'tab', '...', 'Read', 'moreabout', 'the', 'condition', 'Brand', ':', 'Toshiba', 'MPN', ':', 'HDTB205XK3AA', 'UPC', ':', '022265494950', 'Adorama', 'Visit', 'my', 'eBay', 'store']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cO7gZlDbXFdo","colab_type":"text"},"source":["This is version 1 of trying to modularize code so that we can run many experiments and collaborate on this colab.\n","The functions that are common to all experiments that need to be run first are at the bottom of the document beginning with a line of #'s like below:\n","##########################################################################"]},{"cell_type":"markdown","metadata":{"id":"4P3U5iGvDaGX","colab_type":"text"},"source":["#SETUP TO RUN CODE"]},{"cell_type":"markdown","metadata":{"id":"Cq5nIvkiDe2R","colab_type":"text"},"source":["##IMPORTS AND PIP INSTALLS"]},{"cell_type":"code","metadata":{"id":"ZdGKiaG8XHQ5","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592715557359,"user_tz":240,"elapsed":3073,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}}},"source":["#pip installs\n","! pip install ipdb -q\n","import ipdb \n","#place the line of code below from whichever line you want to start debugging\n","#type n for step over and s for step into\n","# ipdb.set_trace()\n","\n","from time import time\n","import gensim\n","import pandas as pd\n","from typing import List, Tuple, Callable\n","import numpy as np\n","from gensim.models.keyedvectors import KeyedVectors\n","from gensim.models.wrappers import FastText\n","from gensim.similarities import WmdSimilarity\n","from ast import literal_eval"],"execution_count":33,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fbhu9IOfDu3_","colab_type":"text"},"source":["##MOUNT GOOGLE DRIVE SO THAT WE CAN ACCESS FILES FROM GOOGLE DRIVE"]},{"cell_type":"code","metadata":{"id":"8pCZgWJlXzbx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1592715560950,"user_tz":240,"elapsed":318,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"outputId":"41b45c17-22c3-4076-efa4-061c439abc2d"},"source":["#repoPath should start with the path \n","#'/content/gdrive/My Drive/' and go to the folder where the git repo is, and should end with the name of the repo:\n","repoPath = '/content/gdrive/My Drive/TestingGitCloneIntoGoogleDrive/textSimilarityUsingWordMoversDistance'\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","import os\n","os.chdir(repoPath)"],"execution_count":34,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5dqKx_X96enC","colab_type":"text"},"source":["#EXPERIMENT 1\n"]},{"cell_type":"markdown","metadata":{"id":"92XR35HoXIhl","colab_type":"text"},"source":["##INPUTS FOR EXPERIMENT TO BE FILLED BY USER\n"]},{"cell_type":"code","metadata":{"id":"yMIIJM8XXIXm","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592715567580,"user_tz":240,"elapsed":343,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}}},"source":["#centralized folder for data\n","folder_for_data_with_slash_at_end = 'data/'\n","\n","\n","\n","#dataset to load\n","url_of_dataset = 'https://query.data.world/s/iccclhgzbyedtv54p5lresglnun3qz'\n","product_dataset_path = folder_for_data_with_slash_at_end+'crowdflower_product_dataset.csv'\n","tuple_of_starting_and_ending_exclusive_index_of_dataset_sample = (0,500)\n","\n","\n","\n","#folder path for experiment\n","experiment_output_folder_path = 'experiment_outputs/experiment1/'\n","\n","\n","\n","#path initializations\n","product_dataframe_csv_path = experiment_output_folder_path+'product_dataframe.csv'\n","\n","\n","\n","#pretrained word embeddings to load\n","url_to_word_embedding_zip = \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n","word_embedding_file_path = folder_for_data_with_slash_at_end+'GoogleNews-vectors-negative300.bin'\n","word_embedding_used = 'word2vec_pretrained' #'fasttext'\n","\n","\n","#queried products file path\n","queried_products_sample_dataframe_csv_path = experiment_output_folder_path+'queried_products_sample_dataframe.csv'\n","\n","\n","\n","#which preprocessing to use\n","fields_to_preprocess_in_order_of_appending = [['product_title'], ['product_description'], ['product_title', 'product_description'] ]\n","#below new_field_name is initialized based on fields_to_preprocess_in_order_of_appending\n","new_field_name_list = []\n","for input_fields_list in fields_to_preprocess_in_order_of_appending:\n","  new_field_name = \"_AND_\".join(input_fields_list)+'_preprocessed'\n","  new_field_name_list.append(new_field_name)\n","dictionary_of_converters_for_read_csv={}\n","for i in range(len(new_field_name_list)):\n","  dict_key = new_field_name_list[i]\n","  dictionary_of_converters_for_read_csv[dict_key] = eval\n","\n","\n","#preprocessing file path\n","product_dataframe_with_preprocessed_text_csv_path = experiment_output_folder_path+'product_dataframe_with_preprocessed_text.csv'\n","preprocessing_used = new_field_name_list[1]\n","\n","\n","\n","\n","#similarity_matrix_path\n","similarity_measure_type = 'wmd'\n","\n","list_of_text_to_combine_into_one_path_for_similarity_dataframe  = [experiment_output_folder_path+'similarity_dataframe', preprocessing_used, word_embedding_used, similarity_measure_type,'.csv']\n","similarity_dataframe_csv_path = \"_\".join(list_of_text_to_combine_into_one_path_for_similarity_dataframe) \n","# print(similarity_dataframe_csv_path)"],"execution_count":35,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3-Zwnub59Nvo","colab_type":"text"},"source":["##DOWNLOADING ALL RELEVANT DATA (1-TIME SETUP)"]},{"cell_type":"code","metadata":{"id":"c4duLOId9Lls","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1592332982117,"user_tz":240,"elapsed":645,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"outputId":"b49d07dc-5fab-4557-c55c-41efccbbce97"},"source":["#############################################\n","#Download product dataset\n","def download_csv_from_url_to_local_directory(url_of_dataset: str, desired_path_of_csv_to_be_created: str, csv_encoding: str = \"ISO-8859-1\") -> bool:\n","  import os.path\n","  from os import path\n","  import pandas as pd\n","  if path.exists(desired_path_of_csv_to_be_created) == True:\n","    print('csv already exists at path')\n","    success_in_downloading_dataset = True \n","  else:\n","    product_dataset = pd.read_csv(url_of_dataset, encoding = csv_encoding)\n","    product_dataset.to_csv(desired_path_of_csv_to_be_created, index = False)\n","    if path.exists(desired_path_of_csv_to_be_created) == True:\n","      success_in_downloading_dataset = True\n","    else:\n","      success_in_downloading_dataset = False\n","      print('Failed to download product dataset: ', url_of_dataset)\n","  return success_in_downloading_dataset\n","\n","success_product_dataset_download = download_csv_from_url_to_local_directory(url_of_dataset, product_dataset_path)\n","\n","\n","\n","#############################################\n","##download model\n","# folder_for_data_with_slash_at_end = \"data/\"\n","download_success, word_embedding_file_path = download_and_unzip_pretrained_word_embedding(url_to_word_embedding_zip, folder_for_data_with_slash_at_end)\n","\n","\n","\n","#############################################\n","#create folder for experiment\n","import os.path\n","from os import path\n","if path.exists(experiment_output_folder_path) == True:\n","  print(\"Folder already exists, so folder creation failed\")\n","else:\n","  !mkdir $experiment_output_folder_path\n","\n","##############################################\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["csv already exists at path\n","Folder already exists, so folder creation failed\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aIRp8HJ5LLC-","colab_type":"text"},"source":["##CREATION OF DATA FILES TO BE FED INTO MODEL"]},{"cell_type":"code","metadata":{"id":"6QubTCorLKbx","colab_type":"code","colab":{}},"source":["#############################################\n","#load input product dataset from dataframe, \n","#sample appropriate range, and save to csv\n","#\n","\n","product_dataframe = load_csv_from_local_directory_to_dataframe(product_dataset_path)\n","\n","start,end = tuple_of_starting_and_ending_exclusive_index_of_dataset_sample\n","product_dataframe = product_dataframe.iloc[start:end]\n","# product_dataframe.to_csv(product_dataframe_csv_path, index=True, index_label = 'relative_index')\n","product_dataframe.to_csv(product_dataframe_csv_path, index=False)\n","\n","#############################################\n","#Load dataframe of products that we are querying\n","#We will loop through querying these products\n","\n","number_of_products_per_query_to_sample = 2\n","product_dataframe = load_csv_from_local_directory_to_dataframe(product_dataframe_csv_path)\n","\n","queried_products_sample_dataframe = sample_n_products_from_all_query_fields_in_dataframe(product_dataframe, number_of_products_per_query_to_sample, randomly_sample_from_each_query = False)\n","queried_products_sample_dataframe.to_csv(queried_products_sample_dataframe_csv_path, index=False)\n","\n","\n","\n","#############################################\n","# #preprocess the input dataset text. Save the preprocessing\n","product_dataframe = load_csv_from_local_directory_to_dataframe(product_dataframe_csv_path)\n","preprocess_data_object = PreprocessData()\n","\n","# fields_to_preprocess_in_order_of_appending = [['product_title'], ['product_description'], ['product_title', 'product_description'] ]\n","product_dataframe_with_preprocessed_text = product_dataframe\n","for i in range(len(fields_to_preprocess_in_order_of_appending)):\n","  input_fields_list = fields_to_preprocess_in_order_of_appending[i]\n","  preprocessing_function = preprocess_data_object.text_preprocessing_for_WMD\n","  product_dataframe_with_preprocessed_text = preprocess_data_object.create_new_field_in_dataframe_with_preprocessed_text_from_list_of_existing_fields(product_dataframe_with_preprocessed_text, input_fields_list, new_field_name_list[i], preprocessing_function)\n","\n","product_dataframe_with_preprocessed_text.to_csv(product_dataframe_with_preprocessed_text_csv_path, index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YN61_xGj4243","colab_type":"text"},"source":["##Get Experiment Similarity Measure and Create Debug Log"]},{"cell_type":"code","metadata":{"id":"GBzuBPkA6g5p","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1592531010400,"user_tz":240,"elapsed":216854,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"outputId":"a3cf3d3d-d1f4-4972-a02e-dc1e731e5c9d"},"source":["#############################################\n","#create a similarity model that I can query\n","\n","# #load any pretrained word embedding models\n","model = load_word2vec_word_embedding_into_KeyedVectors_model(word_embedding_file_path)\n","# model = load_fasttext_word_embedding_into_KeyedVectors_model(word_embedding_file_path)\n","\n","\n","\n","#create corpus of words to calculate similarity from\n","product_dataframe_with_preprocessed_text = pd.read_csv(product_dataframe_with_preprocessed_text_csv_path, converters=dictionary_of_converters_for_read_csv)\n","field_name_of_selected_preprocessed_text_to_create_corpus_from = preprocessing_used\n","# ipdb.set_trace()\n","corpus = product_dataframe_with_preprocessed_text[field_name_of_selected_preprocessed_text_to_create_corpus_from].tolist()\n","\n","\n","\n","wmd_object_for_description = WmdSimilarity(corpus, model)\n","\n","\n","#############################################\n","#Loop through all list of queries\n","# product_dataframe_with_preprocessed_text = load_csv_from_local_directory_to_dataframe(product_dataframe_with_preprocessed_text_csv_path)\n","product_dataframe_with_preprocessed_text = pd.read_csv(product_dataframe_with_preprocessed_text_csv_path, converters=dictionary_of_converters_for_read_csv)\n","field_name_of_selected_preprocessed_text_to_create_corpus_from = preprocessing_used\n","queried_products_sample_dataframe = load_csv_from_local_directory_to_dataframe(queried_products_sample_dataframe_csv_path)\n","num_rows_queried_products_sample_dataframe = queried_products_sample_dataframe.shape[0]\n","\n","# similarity_dataframe = pd.DataFrame(columns=['searched_item_unit_id', 'similar_item_unit_id', 'similarity_score'])\n","similarity_dictionary = {\n","    'searched_item_unit_id': [],\n","    'similar_item_unit_id': [],\n","    'similarity_score': []\n","}\n","for i in range(num_rows_queried_products_sample_dataframe):\n","  queried_product_index = queried_products_sample_dataframe['_unit_id'][i]\n","  # ipdb.set_trace()\n","  queried_product_preprocessed_text = product_dataframe_with_preprocessed_text.loc[product_dataframe_with_preprocessed_text['_unit_id'] == queried_product_index][field_name_of_selected_preprocessed_text_to_create_corpus_from].iloc[0]\n","  # print(queried_product_preprocessed_text)\n","  # queried_product_preprocessed_text = queried_products_sample_dataframe[field_name_of_selected_preprocessed_text_to_create_corpus_from][i]\n","  similar_item_indexes_and_similarity = wmd_object_for_description[queried_product_preprocessed_text]\n","  # print(similar_item_indexes_and_similarity)\n","  searched_item_unit_id = queried_products_sample_dataframe['_unit_id'][i]\n","  # ipdb.set_trace()\n","  for j in range(len(similar_item_indexes_and_similarity)):\n","    similar_item_index = j\n","    similar_item_unit_id = product_dataframe_with_preprocessed_text['_unit_id'][similar_item_index]\n","    similarity_score = similar_item_indexes_and_similarity[j]\n","    # dataframe_row_we_are_appending = pd.DataFrame(data = {'searched_item_unit_id': [searched_item_unit_id], 'similar_item_unit_id': [similar_item_unit_id], 'similarity_score': [similarity_score]})\n","    similarity_dictionary['searched_item_unit_id'].append(searched_item_unit_id)\n","    similarity_dictionary['similar_item_unit_id'].append(similar_item_unit_id)\n","    similarity_dictionary['similarity_score'].append(similarity_score)\n","    # ipdb.set_trace()\n","    # similarity_dataframe.append(dataframe_row_we_are_appending, ignore_index=True)\n","\n","similarity_dataframe = pd.DataFrame(data=similarity_dictionary)\n","similarity_dataframe.to_csv(similarity_dataframe_csv_path, index=False)  \n","\n","\n","#############################################\n","# #loop through products we are querying in order to build experiment folder for \n","# #logs\n","# for source_doc_index in indexesOfTargetItems:\n","  \n","  \n","  \n","#   #output and save similarity measure output dataframe of items and their \n","#   #similarity scores to a particular target query\n","  \n","#   #Only do this for one measure at a time\n","\n","# #read the output similarity measure data frames, and output accuracy of methods\n","\n","# #read the output similar measure data frames, and output accuracy of methods, \n","# #and create a log\n","\n","# indexesOfTargetItems = \n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"P58T7w4NAofB","colab_type":"text"},"source":["#Calculate Final Composite Similarity Matrix and Associated Accuracy"]},{"cell_type":"markdown","metadata":{"id":"9IpWbkgNTTD5","colab_type":"text"},"source":["INPUTS"]},{"cell_type":"code","metadata":{"id":"jzNbdDXaApDN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1592591393171,"user_tz":240,"elapsed":481,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"outputId":"58900fb1-4eba-4a39-8844-d3e11b7ea5c1"},"source":["#experiment folder path\n","#input\n","experiment_output_folder_path = 'experiment_outputs/experiment1/'\n","\n","\n","###########################################\n","#list of similarity dataframes\n","#input\n","list_of_files_in_experiment_output_folder_of_base_similarity_measure_csv_files = \\\n","['similarity_dataframe_product_description_preprocessed_word2vec_pretrained_wmd_.csv',\n","'similarity_dataframe_product_title_preprocessed_word2vec_pretrained_wmd_.csv']\n","\n","#calculated based on input\n","list_of_paths_in_experiment_output_folder_of_base_similarity_measure_csv_files = \\\n","[experiment_output_folder_path+base_similarity for base_similarity in list_of_files_in_experiment_output_folder_of_base_similarity_measure_csv_files]\n","print(list_of_paths_in_experiment_output_folder_of_base_similarity_measure_csv_files)\n","\n","\n","\n","###########################################\n","queried_products_sample_dataframe_csv_path = experiment_output_folder_path+'queried_products_sample_dataframe.csv'\n","\n","\n","\n","###########################################\n","product_dataframe_csv_path = experiment_output_folder_path+'product_dataframe.csv'\n","\n","\n","\n","###########################################\n","composite_similarity_dataframe_csv_path = experiment_output_folder_path+'composite_similarity_dataframe.csv'\n","\n","\n","\n","###########################################\n","relevance_threshold = 2\n","\n","\n","\n","###########################################\n","num_most_similar_items_for_accuracy = 10\n","\n","\n","\n","###########################################\n","#composite function of all the similarity scores\n","def composite_similarity_function(input_base_similarity_scores: List[float])-> float:\n","  return input_base_similarity_scores[0]*0.5+input_base_similarity_scores[1]*0.5\n"],"execution_count":51,"outputs":[{"output_type":"stream","text":["['experiment_outputs/experiment1/similarity_dataframe_product_description_preprocessed_word2vec_pretrained_wmd_.csv', 'experiment_outputs/experiment1/similarity_dataframe_product_title_preprocessed_word2vec_pretrained_wmd_.csv']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SOQl-MwdTRW9","colab_type":"text"},"source":["CALCULATING SIMILARITY AND ASSOCIATED ACCURACY"]},{"cell_type":"code","metadata":{"id":"ZGDWIKPATRzT","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592591632184,"user_tz":240,"elapsed":58537,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}}},"source":["#load all similarity dataframes\n","list_of_similarity_dataframes = []\n","for file in list_of_paths_in_experiment_output_folder_of_base_similarity_measure_csv_files:\n","  list_of_similarity_dataframes.append(load_csv_from_local_directory_to_dataframe(file))\n","\n","#get list of unique dataframes.\n","##load from query list\n","queried_products_sample_dataframe = load_csv_from_local_directory_to_dataframe(\n","    queried_products_sample_dataframe_csv_path)\n","\n","product_dataframe = load_csv_from_local_directory_to_dataframe(product_dataframe_csv_path)\n","\n","num_rows_queried_products_sample_dataframe = queried_products_sample_dataframe.shape[0]\n","num_rows_similarity_dataframe = list_of_similarity_dataframes[0].shape[0]\n","num_rows_product_dataframe = product_dataframe.shape[0]\n","# composite_similarity_and_accuracy_dataframe = load_csv_from_local_directory_to_dataframe(list_of_paths_in_experiment_output_folder_of_base_similarity_measure_csv_files[0])\n","\n","composite_similarity_and_accuracy_dictionary = {\n","    'searched_item_unit_id': [None]*num_rows_similarity_dataframe,\n","    'similar_item_unit_id': [None]*num_rows_similarity_dataframe,\n","    'searched_item_query': [None]*num_rows_similarity_dataframe,\n","    'similar_item_query': [None]*num_rows_similarity_dataframe,\n","    'searched_item_relevance': [None]*num_rows_similarity_dataframe,\n","    'similar_item_relevance': [None]*num_rows_similarity_dataframe,\n","    'composite_similarity_score': [None]*num_rows_similarity_dataframe,\n","    'accuracy': [None]*num_rows_similarity_dataframe\n","}\n","\n","# temp_composite_similarity_dictionary_for_single_search_query = {\n","#     'searched_item_unit_id': [None]*num_rows_similarity_dataframe,\n","#     'similar_item_unit_id': [None]*num_rows_similarity_dataframe,\n","#     'searched_item_query': [None]*num_rows_similarity_dataframe,\n","#     'similar_item_query': [None]*num_rows_similarity_dataframe,\n","#     'searched_item_relevance': [None]*num_rows_similarity_dataframe,\n","#     'similar_item_relevance': [None]*num_rows_similarity_dataframe,\n","#     'composite_similarity_score': [None]*num_rows_similarity_dataframe\n","# }\n","\n","composite_similarity_and_accuracy_dataframe = pd.DataFrame(data = composite_similarity_and_accuracy_dictionary)\n","\n","\n","#loop through queries lits. do pandas filter by a certain value on the similarity dataframe to get indexes\n","for i in range(num_rows_queried_products_sample_dataframe):\n","  queried_product_id = queried_products_sample_dataframe['_unit_id'][i]\n","  # ipdb.set_trace()\n","  \n","  #for a single query item index, loop through similar itemscreate a dataframe for all the composite similarity \n","  #cont: scores for that item. Append to list\n","  index_list = np.where(list_of_similarity_dataframes[0][\"searched_item_unit_id\"] == queried_product_id)[0].tolist()\n","  for j in index_list:\n","    scores_list_for_particular_index = [dataframe['similarity_score'][j] for dataframe in list_of_similarity_dataframes]\n","    # ipdb.set_trace()\n","    composite_similarity_and_accuracy_dataframe['composite_similarity_score'][j] = composite_similarity_function(scores_list_for_particular_index)\n","   \n","    searched_item_dataframe_row = product_dataframe.loc[product_dataframe['_unit_id'] == queried_product_id].iloc[0]\n","    similar_item_product_id = list_of_similarity_dataframes[0]['similar_item_unit_id'][j]\n","    similar_item_dataframe_row = product_dataframe.loc[product_dataframe['_unit_id'] == similar_item_product_id].iloc[0]\n","\n","    composite_similarity_and_accuracy_dataframe['searched_item_unit_id'][j] = queried_product_id\n","    composite_similarity_and_accuracy_dataframe['similar_item_unit_id'][j] = similar_item_product_id\n","    composite_similarity_and_accuracy_dataframe['searched_item_query'][j] = searched_item_dataframe_row['query']\n","    composite_similarity_and_accuracy_dataframe['similar_item_query'][j] = similar_item_dataframe_row['query']\n","    composite_similarity_and_accuracy_dataframe['searched_item_relevance'][j] = searched_item_dataframe_row['relevance']\n","    composite_similarity_and_accuracy_dataframe['similar_item_relevance'][j] = similar_item_dataframe_row['relevance']\n","    \n","  # ipdb.set_trace()\n","  temp_dataframe_for_single_search_query = composite_similarity_and_accuracy_dataframe.iloc[index_list,:]\n","  temp_dataframe_for_single_search_query = temp_dataframe_for_single_search_query.sort_values(by='composite_similarity_score', ascending=False).iloc[:num_most_similar_items_for_accuracy]\n","  accuracy, numMatchingCategory, numNonMatchingCategory = calculate_output_dataframe_accuracy_and_write_in_dataframe(temp_dataframe_for_single_search_query, relevance_threshold)\n","  # ipdb.set_trace()\n","  composite_similarity_and_accuracy_dataframe['accuracy'][index_list[0]] = accuracy\n","\n","# df = DataFrame\n","composite_similarity_and_accuracy_dataframe.to_csv(composite_similarity_dataframe_csv_path, index=False)\n","  \n","  #create new dataframe from list. Calculate overall accuracy. Dont think i want to sort by similarity score yet\n","  #append list to overall dataframe\n"],"execution_count":54,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rktMQA8wFYHO","colab_type":"text"},"source":["#Final Output Similarity "]},{"cell_type":"markdown","metadata":{"id":"gXe-2rQzFt5a","colab_type":"text"},"source":["##INPUT"]},{"cell_type":"code","metadata":{"id":"sppT132Cgage","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1592715586774,"user_tz":240,"elapsed":314,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"outputId":"2abf43e6-72e0-469d-b70e-071833af21db"},"source":["#folder path for experiment\n","experiment_output_folder_path = 'experiment_outputs/experiment1/'\n","\n","\n","\n","###########################################\n","queried_products_sample_dataframe_csv_path = experiment_output_folder_path+'queried_products_sample_dataframe.csv'\n","\n","\n","\n","###########################################\n","product_dataframe_csv_path = experiment_output_folder_path+'product_dataframe.csv'\n","\n","\n","\n","###########################################\n","composite_similarity_dataframe_csv_path = experiment_output_folder_path+'composite_similarity_dataframe.csv'\n","\n","\n","###########################################\n","relevance_threshold = 2\n","\n","\n","\n","###########################################\n","num_most_similar_items_for_accuracy = 10\n","\n","\n","\n","###########################################\n","accuracy_dataframe_csv_path = experiment_output_folder_path+'accuracy_dataframe.csv'\n","\n","\n","\n","###########################################\n","#which preprocessing to use\n","fields_to_preprocess_in_order_of_appending = [['product_title'], ['product_description'], ['product_title', 'product_description'] ]\n","#below new_field_name is initialized based on fields_to_preprocess_in_order_of_appending\n","new_field_name_list = []\n","for input_fields_list in fields_to_preprocess_in_order_of_appending:\n","  new_field_name = \"_AND_\".join(input_fields_list)+'_preprocessed'\n","  new_field_name_list.append(new_field_name)\n","dictionary_of_converters_for_read_csv={}\n","for i in range(len(new_field_name_list)):\n","  dict_key = new_field_name_list[i]\n","  dictionary_of_converters_for_read_csv[dict_key] = eval\n","\n","###########################################\n","#list of similarity dataframes\n","#input\n","list_of_files_in_experiment_output_folder_of_base_similarity_measure_csv_files = \\\n","['similarity_dataframe_product_description_preprocessed_word2vec_pretrained_wmd_.csv',\n","'similarity_dataframe_product_title_preprocessed_word2vec_pretrained_wmd_.csv']\n","\n","#calculated based on input\n","list_of_paths_in_experiment_output_folder_of_base_similarity_measure_csv_files = \\\n","[experiment_output_folder_path+base_similarity for base_similarity in list_of_files_in_experiment_output_folder_of_base_similarity_measure_csv_files]\n","print(list_of_paths_in_experiment_output_folder_of_base_similarity_measure_csv_files)\n","\n","\n","\n","###########################################\n","#preprocessing file path\n","product_dataframe_with_preprocessed_text_csv_path = experiment_output_folder_path+'product_dataframe_with_preprocessed_text.csv'\n","\n","\n","###########################################\n","view_to_debug_similarity_dataframe_csv_path = experiment_output_folder_path+'view_to_debug_similarity_dataframe.csv'\n"],"execution_count":36,"outputs":[{"output_type":"stream","text":["['experiment_outputs/experiment1/similarity_dataframe_product_description_preprocessed_word2vec_pretrained_wmd_.csv', 'experiment_outputs/experiment1/similarity_dataframe_product_title_preprocessed_word2vec_pretrained_wmd_.csv']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kyCJoreXFwa3","colab_type":"text"},"source":["##GET ACCURACY FOR MEASURE"]},{"cell_type":"code","metadata":{"id":"ZX-8KB2FF-wZ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592598763354,"user_tz":240,"elapsed":1066,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}}},"source":["#Load the composite similarity dataframe\n","composite_similarity_dataframe = load_csv_from_local_directory_to_dataframe(composite_similarity_dataframe_csv_path)\n","\n","\n","\n","#load the search queries, and get the list of search queries\n","queried_products_sample_dataframe = load_csv_from_local_directory_to_dataframe(\n","    queried_products_sample_dataframe_csv_path)\n","\n","num_rows_queried_products_sample_dataframe = queried_products_sample_dataframe.shape[0]\n","\n","#create an accuracy_dataframe with fields, search_item_unit_id, search_item_query, search item accuracy, total average accuracy\n","accuracy_dictionary = {\n","    'searched_item_unit_id': [None]*num_rows_queried_products_sample_dataframe,\n","    'searched_item_query': [None]*num_rows_queried_products_sample_dataframe,\n","    'searched_item_accuracy': [None]*num_rows_queried_products_sample_dataframe,\n","    'total_average_accuracy_for_experiment': [None]*num_rows_queried_products_sample_dataframe\n","}\n","accuracy_dataframe = pd.DataFrame(data = accuracy_dictionary)\n","\n","\n","#for each search query, index it out from the composite similarity dataframe, \n","#and output accuracy into accuracy_dataframe\n","\n","for i in range(num_rows_queried_products_sample_dataframe):\n","  queried_product_id = queried_products_sample_dataframe['_unit_id'][i]\n","  search_item_query = queried_products_sample_dataframe['query'][i]\n","  # ipdb.set_trace()\n","  \n","  temp_dataframe_for_single_search_query = composite_similarity_and_accuracy_dataframe.loc[composite_similarity_and_accuracy_dataframe['searched_item_unit_id'] == queried_product_id]\n","  temp_dataframe_for_single_search_query = temp_dataframe_for_single_search_query.sort_values(by='composite_similarity_score', ascending=False).iloc[:num_most_similar_items_for_accuracy]\n","  accuracy, numMatchingCategory, numNonMatchingCategory = calculate_output_dataframe_accuracy_and_write_in_dataframe(temp_dataframe_for_single_search_query, relevance_threshold)\n","  # ipdb.set_trace()\n","  accuracy_dataframe.iloc[i]['searched_item_unit_id']=queried_product_id\n","  accuracy_dataframe.iloc[i]['searched_item_query']=search_item_query\n","  accuracy_dataframe.iloc[i]['searched_item_accuracy']=accuracy\n","nparray_of_accuracies = accuracy_dataframe['searched_item_accuracy'].to_numpy()\n","mean = float(np.mean(nparray_of_accuracies))\n","accuracy_dataframe.iloc[0]['total_average_accuracy_for_experiment']=mean\n","accuracy_dataframe.to_csv(accuracy_dataframe_csv_path, index=False)"],"execution_count":65,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2bAZg49gF_A9","colab_type":"text"},"source":["GET MOST SIMILAR ITEMS TO EACH QUERY ITEM RANKED IN ORDER, AND SHOW DEBUGGING OUTPUT"]},{"cell_type":"markdown","metadata":{"id":"2MMCSC2DCveF","colab_type":"text"},"source":["#Get debug logs"]},{"cell_type":"code","metadata":{"id":"QkP4M2lGDKqP","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592715596146,"user_tz":240,"elapsed":4108,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}}},"source":["#THIS CODE GETS DEBUG LOG ASSOCIATED WITH A SPECIFIC ACCURACY MEASURE\n","\n","#Load the composite similarity dataframe\n","composite_similarity_dataframe = load_csv_from_local_directory_to_dataframe(composite_similarity_dataframe_csv_path)\n","\n","num_rows_composite_similarity_dataframe = composite_similarity_dataframe.shape[0]\n","\n","#Load the accuracy dataframe\n","accuracy_dataframe = load_csv_from_local_directory_to_dataframe(accuracy_dataframe_csv_path)\n","\n","num_rows_accuracy_dataframe = accuracy_dataframe.shape[0]\n","\n","product_dataframe_with_preprocessed_text = pd.read_csv(product_dataframe_with_preprocessed_text_csv_path, converters=dictionary_of_converters_for_read_csv)\n","\n","num_rows_view_to_debug_similarity_dictionary = num_most_similar_items_for_accuracy*num_rows_accuracy_dataframe\n","\n","#create dataframe\n","view_to_debug_similarity_dictionary = {\n","    'unit_id_searched_item': [None]*num_rows_view_to_debug_similarity_dictionary, \n","    'title_searched_item': [None]*num_rows_view_to_debug_similarity_dictionary, \n","    'query_searched_item': [None]*num_rows_view_to_debug_similarity_dictionary,\n","    'relevance_searched_item': [None]*num_rows_view_to_debug_similarity_dictionary,\n","    'description_searched_item': [None]*num_rows_view_to_debug_similarity_dictionary,\n","    # preprocessing_used+'_searched_item': target_description_preprocesed_list,\n","    'unit_id_similar_item': [None]*num_rows_view_to_debug_similarity_dictionary,\n","    'title_similar_item': [None]*num_rows_view_to_debug_similarity_dictionary,\n","    'query_similar_item': [None]*num_rows_view_to_debug_similarity_dictionary,\n","    'relevance_similar_item': [None]*num_rows_view_to_debug_similarity_dictionary,\n","    'description_similar_item': [None]*num_rows_view_to_debug_similarity_dictionary,\n","    # preprocessing_used+'_searched_item': dataframe_modified[new_field_name][similar_product_index].tolist(),\n","    'rank': [None]*num_rows_view_to_debug_similarity_dictionary,\n","    'composite_similarity_score': [None]*num_rows_view_to_debug_similarity_dictionary, \n","    'accuracy_for_item': [None]*num_rows_view_to_debug_similarity_dictionary,\n","    'total_average_accuracy_for_experiment': [None]*num_rows_view_to_debug_similarity_dictionary\n","    } \n","for preprocessing_field_name in new_field_name_list:\n","  searched_preprocessing_field = preprocessing_field_name+'_searched'\n","  similar_preprocessing_field = preprocessing_field_name+'_similar'\n","  view_to_debug_similarity_dictionary[searched_preprocessing_field] = [None]*num_rows_view_to_debug_similarity_dictionary\n","  view_to_debug_similarity_dictionary[similar_preprocessing_field] = [None]*num_rows_view_to_debug_similarity_dictionary\n","\n","for base_similarity_measure in list_of_files_in_experiment_output_folder_of_base_similarity_measure_csv_files:\n","  view_to_debug_similarity_dictionary[base_similarity_measure] = [None]*num_rows_view_to_debug_similarity_dictionary\n","\n","base_similarity_list_of_dataframes = []\n","for base_similarity_measure_path in list_of_paths_in_experiment_output_folder_of_base_similarity_measure_csv_files:\n","  base_similarity_list_of_dataframes.append(load_csv_from_local_directory_to_dataframe(base_similarity_measure_path))\n","\n","view_to_debug_similarity_dataframe = pd.DataFrame(data = view_to_debug_similarity_dictionary)\n","\n","#loop through accuracy dataframe items\n","for i in range(num_rows_accuracy_dataframe):\n","  #get rows from composite_similarity measure corresponding to a searched unit id of the accuracy dataframe\n","  ##sort by similarity rank and sample the num_most_similar_items_for_accuracy\n","  unit_id_searched_item = accuracy_dataframe.iloc[i]['searched_item_unit_id']\n","  row_searched_item = product_dataframe_with_preprocessed_text.loc[product_dataframe_with_preprocessed_text['_unit_id'] == unit_id_searched_item].iloc[0]\n","  title_searched_item = row_searched_item['product_title']\n","  query_searched_item = row_searched_item['query']\n","  relevance_searched_item = row_searched_item['relevance']\n","  description_searched_item = row_searched_item['product_description']\n","\n","  composite_similarity_dataframe_for_one_item = composite_similarity_dataframe.loc[composite_similarity_dataframe['searched_item_unit_id'] == unit_id_searched_item]\n","  composite_similarity_dataframe_for_one_item = composite_similarity_dataframe_for_one_item.sort_values(by='composite_similarity_score', ascending=False).iloc[:num_most_similar_items_for_accuracy]\n","  \n","  #loop through each of these rows\n","  for j in range(composite_similarity_dataframe_for_one_item.shape[0]):\n","    #get corresponding unit id row for both searched and similar product from product with preprocessed text dataframe and fill the following:\n","    ##unit_id_searched_item, title_searched_item, query_searched_item, relevance_searched_item, description_searched_item, composite_similarity, preprocessed text\n","    # ipdb.set_trace()\n","    unit_id_similar_item = composite_similarity_dataframe_for_one_item.iloc[j]['similar_item_unit_id']\n","    row_similar_item = product_dataframe_with_preprocessed_text.loc[product_dataframe_with_preprocessed_text['_unit_id'] == unit_id_similar_item].iloc[0]\n","    title_similar_item = row_similar_item['product_title']\n","    query_similar_item = row_similar_item['query']\n","    relevance_similar_item = row_similar_item['relevance']\n","    description_similar_item = row_similar_item['product_description']\n","\n","    #create a index for view_to_debug_similarity_dataframe\n","    index_for_view_to_debug_similarity_dataframe = i*num_most_similar_items_for_accuracy+j\n","\n","    view_to_debug_similarity_dataframe.iloc[index_for_view_to_debug_similarity_dataframe]['unit_id_searched_item'] = unit_id_searched_item\n","    view_to_debug_similarity_dataframe.iloc[index_for_view_to_debug_similarity_dataframe]['row_searched_item'] = row_searched_item\n","    view_to_debug_similarity_dataframe.iloc[index_for_view_to_debug_similarity_dataframe]['title_searched_item'] = title_searched_item\n","    view_to_debug_similarity_dataframe.iloc[index_for_view_to_debug_similarity_dataframe]['query_searched_item'] = query_searched_item\n","    view_to_debug_similarity_dataframe.iloc[index_for_view_to_debug_similarity_dataframe]['relevance_searched_item'] = relevance_searched_item\n","    view_to_debug_similarity_dataframe.iloc[index_for_view_to_debug_similarity_dataframe]['description_searched_item'] = description_searched_item\n","\n","    view_to_debug_similarity_dataframe.iloc[index_for_view_to_debug_similarity_dataframe]['unit_id_similar_item'] = unit_id_similar_item\n","    view_to_debug_similarity_dataframe.iloc[index_for_view_to_debug_similarity_dataframe]['row_similar_item'] = row_similar_item\n","    view_to_debug_similarity_dataframe.iloc[index_for_view_to_debug_similarity_dataframe]['title_similar_item'] = title_similar_item\n","    view_to_debug_similarity_dataframe.iloc[index_for_view_to_debug_similarity_dataframe]['query_similar_item'] = query_similar_item\n","    view_to_debug_similarity_dataframe.iloc[index_for_view_to_debug_similarity_dataframe]['relevance_similar_item'] = relevance_similar_item\n","    view_to_debug_similarity_dataframe.iloc[index_for_view_to_debug_similarity_dataframe]['description_similar_item'] = description_similar_item\n","\n","    # ipdb.set_trace()\n","\n","    #initialize the preprocessed text\n","    for k in range(len(new_field_name_list)):\n","      searched_field = new_field_name_list[k] + '_searched'\n","      similar_field = new_field_name_list[k] + '_similar'\n","      view_to_debug_similarity_dataframe.iloc[index_for_view_to_debug_similarity_dataframe][searched_field] =  row_searched_item[new_field_name_list[k]]\n","      view_to_debug_similarity_dataframe.iloc[index_for_view_to_debug_similarity_dataframe][similar_field] =  row_similar_item[new_field_name_list[k]]\n","\n","    \n","    #from same row, get the base_similarities a well\n","    for k in range(len(list_of_files_in_experiment_output_folder_of_base_similarity_measure_csv_files)):\n","      base_similarity_measure = list_of_files_in_experiment_output_folder_of_base_similarity_measure_csv_files[k]\n","      base_similarity_dataframe = base_similarity_list_of_dataframes[k]\n","      # ipdb.set_trace()\n","      score = base_similarity_dataframe.loc[(base_similarity_dataframe['searched_item_unit_id'] == unit_id_searched_item) & (base_similarity_dataframe['similar_item_unit_id'] == unit_id_similar_item)].iloc[0]['similarity_score']\n","      view_to_debug_similarity_dataframe.iloc[index_for_view_to_debug_similarity_dataframe][base_similarity_measure] = score   \n","\n","    #get composite similarity measure\n","    # ipdb.set_trace()\n","    view_to_debug_similarity_dataframe.iloc[index_for_view_to_debug_similarity_dataframe]['composite_similarity_score'] = composite_similarity_dataframe_for_one_item.iloc[j]['composite_similarity_score']\n","\n","  index_min_of_write_in_view = i*num_most_similar_items_for_accuracy\n","  index_max_of_write_in_view = (i+1)*num_most_similar_items_for_accuracy\n","\n","  #fill in the accuracy_for_item\n","  view_to_debug_similarity_dataframe.loc[index_min_of_write_in_view,'accuracy_for_item'] = accuracy_dataframe.iloc[i]['searched_item_accuracy']\n","\n","  #create a rank\n","  view_to_debug_similarity_dataframe.loc[index_min_of_write_in_view:index_max_of_write_in_view,'rank'] = view_to_debug_similarity_dataframe.iloc[index_min_of_write_in_view:index_max_of_write_in_view]['composite_similarity_score'].rank(method='min', ascending = False)\n","  # ipdb.set_trace()\n","#fill in total_average_accuracy\n","# ipdb.set_trace()\n","view_to_debug_similarity_dataframe.iloc[0]['total_average_accuracy_for_experiment'] = accuracy_dataframe.iloc[0]['total_average_accuracy_for_experiment']\n","view_to_debug_similarity_dataframe.to_csv(view_to_debug_similarity_dataframe_csv_path, index=False)"],"execution_count":37,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OTVvzKUp8nZ1","colab_type":"text"},"source":["##########################################################################"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DAHc55pgDXg4"},"source":["##########################################################################"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"836yXhoYDXpv"},"source":["##########################################################################"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qN4Zk5GpDXuZ"},"source":["##########################################################################"]},{"cell_type":"markdown","metadata":{"id":"-Qnz6ppmYEv2","colab_type":"text"},"source":["\n","MOUNT GOOGLE DRIVE SO YOU CAN ACCESS FILES FROM GOOGLE DRIVE"]},{"cell_type":"markdown","metadata":{"id":"KLcpc588YGsi","colab_type":"text"},"source":["Pip Install packages, and import general packages"]},{"cell_type":"code","metadata":{"id":"5-2lqDS31lFS","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592677955504,"user_tz":240,"elapsed":2241,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}}},"source":["\n","from collections import OrderedDict\n","import numpy as np\n","import spacy\n","from spacy.lang.en.stop_words import STOP_WORDS\n","\n","nlp = spacy.load('en_core_web_sm')\n","\n","class TextRank4Keyword():\n","    \"\"\"Extract keywords from text\"\"\"\n","    \n","    def __init__(self):\n","        self.d = 0.85 # damping coefficient, usually is .85\n","        self.min_diff = 1e-5 # convergence threshold\n","        self.steps = 10 # iteration steps\n","        self.node_weight = None # save keywords and its weight\n","\n","    \n","    def set_stopwords(self, stopwords):  \n","        \"\"\"Set stop words\"\"\"\n","        for word in STOP_WORDS.union(set(stopwords)):\n","            lexeme = nlp.vocab[word]\n","            lexeme.is_stop = True\n","    \n","    def sentence_segment(self, doc, candidate_pos, lower):\n","        \"\"\"Store those words only in cadidate_pos\"\"\"\n","        sentences = []\n","        for sent in doc.sents:\n","            selected_words = []\n","            for token in sent:\n","                # Store words only with cadidate POS tag\n","                if token.pos_ in candidate_pos and token.is_stop is False:\n","                    if lower is True:\n","                        selected_words.append(token.text.lower())\n","                    else:\n","                        selected_words.append(token.text)\n","            sentences.append(selected_words)\n","        return sentences\n","        \n","    def get_vocab(self, sentences):\n","        \"\"\"Get all tokens\"\"\"\n","        vocab = OrderedDict()\n","        i = 0\n","        for sentence in sentences:\n","            for word in sentence:\n","                if word not in vocab:\n","                    vocab[word] = i\n","                    i += 1\n","        return vocab\n","    \n","    def get_token_pairs(self, window_size, sentences):\n","        \"\"\"Build token_pairs from windows in sentences\"\"\"\n","        token_pairs = list()\n","        for sentence in sentences:\n","            for i, word in enumerate(sentence):\n","                for j in range(i+1, i+window_size):\n","                    if j >= len(sentence):\n","                        break\n","                    pair = (word, sentence[j])\n","                    if pair not in token_pairs:\n","                        token_pairs.append(pair)\n","        return token_pairs\n","        \n","    def symmetrize(self, a):\n","        return a + a.T - np.diag(a.diagonal())\n","    \n","    def get_matrix(self, vocab, token_pairs):\n","        \"\"\"Get normalized matrix\"\"\"\n","        # Build matrix\n","        vocab_size = len(vocab)\n","        g = np.zeros((vocab_size, vocab_size), dtype='float')\n","        for word1, word2 in token_pairs:\n","            i, j = vocab[word1], vocab[word2]\n","            g[i][j] = 1\n","            \n","        # Get Symmeric matrix\n","        g = self.symmetrize(g)\n","        \n","        # Normalize matrix by column\n","        norm = np.sum(g, axis=0)\n","        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n","        \n","        return g_norm\n","\n","    \n","    def get_keywords(self, number=10):\n","        \"\"\"Print top number keywords\"\"\"\n","        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n","        for i, (key, value) in enumerate(node_weight.items()):\n","            print(key + ' - ' + str(value))\n","            if i > number:\n","                break\n","\n","    def get_list_of_strings_of_the_top_textrank_keywords_in_order_of_textrank(self, number=10):\n","      list_of_top_keywords = []\n","\n","      \"\"\"Print top number keywords\"\"\"\n","      node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n","      for i, (key, value) in enumerate(node_weight.items()):\n","          #print(key + ' - ' + str(value))\n","          list_of_top_keywords.append(key.lower())\n","          if i > number:\n","            break\n","      return list_of_top_keywords       \n","        \n","    def analyze(self, text, \n","                candidate_pos=['NOUN', 'PROPN'], \n","                window_size=4, lower=False, stopwords=list()):\n","        \"\"\"Main function to analyze text\"\"\"\n","        \n","        # Set stop words\n","        self.set_stopwords(stopwords)\n","        \n","        # Pare text by spaCy\n","        doc = nlp(text)\n","        \n","        # Filter sentences\n","        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n","        \n","        # Build vocabulary\n","        vocab = self.get_vocab(sentences)\n","        \n","        # Get token_pairs from windows\n","        token_pairs = self.get_token_pairs(window_size, sentences)\n","        \n","        # Get normalized matrix\n","        g = self.get_matrix(vocab, token_pairs)\n","        \n","        # Initionlization for weight(pagerank value)\n","        pr = np.array([1] * len(vocab))\n","        \n","        # Iteration\n","        previous_pr = 0\n","        for epoch in range(self.steps):\n","            pr = (1-self.d) + self.d * np.dot(g, pr)\n","            if abs(previous_pr - sum(pr))  < self.min_diff:\n","                break\n","            else:\n","                previous_pr = sum(pr)\n","\n","        # Get weight for each node\n","        node_weight = dict()\n","        for word, index in vocab.items():\n","            node_weight[word] = pr[index]\n","        \n","        self.node_weight = node_weight"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"FWii_7GVXHEn","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592677961712,"user_tz":240,"elapsed":1009,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}}},"source":["#download pretrained word embedding model and load it\n","##download a file\n","##unzip a file\n","def download_and_unzip_pretrained_word_embedding(url_to_word_embedding_zip: str, download_destination_folder_with_slash_at_end: str) -> Tuple[bool, str]:\n","  #download if unzipped file exists already\n","  #if not, unzip and check if file exists after unzipping\n","  #if file exists, return 1 for success\n","  import os.path\n","  from os import path\n","  zipped_file_name = os.path.basename(url_to_word_embedding_zip)\n","  unzipped_file_name = os.path.splitext(zipped_file_name)[0]\n","\n","  path_of_zipped_word_embedding = download_destination_folder_with_slash_at_end + zipped_file_name\n","  desired_path_of_unzipped_word_embedding = download_destination_folder_with_slash_at_end + unzipped_file_name\n","  zip_file_extension = os.path.splitext(url_to_word_embedding_zip)[1][1:]\n","\n","  if path.exists(desired_path_of_unzipped_word_embedding) == False:\n","    !wget -c $url_to_word_embedding_zip -P $download_destination_folder_with_slash_at_end\n","    if zip_file_extension == 'gz':\n","      !gunzip $path_of_zipped_word_embedding\n","    elif zip_file_extension == 'zip':\n","      print(\"zip_file_extension\", \" not implemented yet\")\n","      return (False, None)\n","    else:\n","      print(\"This function doesnt handle zipped files with extension: \", zip_file_extension)\n","      return (False, None)\n","  else:\n","    print(\"datasetAlreadyExists\")\n","\n","  if path.exists(desired_path_of_unzipped_word_embedding) == True:\n","    successful_in_creating_word_embedding = True\n","  else:\n","    successful_in_creating_word_embedding = False\n","  return (successful_in_creating_word_embedding, desired_path_of_unzipped_word_embedding)\n","\n","##load word embedding\n","def load_word_embedding_into_KeyedVectors_model(word_embedding_file_path: str) -> gensim.models.keyedvectors.Word2VecKeyedVectors:\n","  from gensim.models.keyedvectors import KeyedVectors\n","  model = KeyedVectors.load_word2vec_format(word_embedding_file_path, binary=True)\n","  return model\n","\n","def load_word2vec_word_embedding_into_KeyedVectors_model(word_embedding_file_path: str) -> gensim.models.keyedvectors.Word2VecKeyedVectors:\n","  model = KeyedVectors.load_word2vec_format(word_embedding_file_path, binary=True)\n","  return model\n","\n","def load_fasttext_word_embedding_into_KeyedVectors_model(word_embedding_file_path: str) -> gensim.models.keyedvectors.Word2VecKeyedVectors:\n","  model = FastText.load_fasttext_format(model_path)\n","  return model\n","\n","#load dataset\n","#extract desired features from dataset\n","def load_dataset_into_pandas(url_of_dataset: str, tuple_of_starting_and_ending_exclusive_index_of_sample: tuple) -> pd.core.frame.DataFrame:\n","  #Create pandas dataframe on dataset\n","  import pandas as pd\n","  #https://data.world/crowdflower/ecommerce-search-relevance\n","  df = pd.read_csv(url_of_dataset, encoding = \"ISO-8859-1\")\n","  start,end = tuple_of_starting_and_ending_exclusive_index_of_sample\n","  df = df.iloc[start:end]\n","  return df\n","\n","def load_csv_from_local_directory_to_dataframe(path_of_csv: str, csv_encoding: str = \"ISO-8859-1\") -> pd.core.frame.DataFrame:\n","  import os.path\n","  from os import path\n","  import pandas as pd\n","\n","  product_dataframe = pd.read_csv(path_of_csv, encoding = csv_encoding)\n","  return product_dataframe\n","\n","def sample_n_products_from_all_query_fields_in_dataframe(product_dataframe: pd.core.frame.DataFrame, number_of_products_per_query: int, randomly_sample_from_each_query: bool = False) -> pd.core.frame.DataFrame:\n","  from random import sample  \n","  list_of_queries_field = product_dataframe['query'].unique().tolist()\n","\n","  indexes_of_query_products = []\n","  for i in range(len(list_of_queries_field)):\n","    if isinstance(pd.Index(product_dataframe['query']).get_loc(list_of_queries_field[i]), int):\n","      indexes_of_query_products.append(pd.Index(product_dataframe['query']).get_loc(list_of_queries_field[i]))\n","    else:\n","      bool_array_indicating_dataframe_indexes_where_query_matches = pd.Index(product_dataframe['query']).get_loc(list_of_queries_field[i])\n","      list_of_indexes_for_particular_query = [i for i, x in enumerate(bool_array_indicating_dataframe_indexes_where_query_matches) if x]\n","      if randomly_sample_from_each_query:\n","        indexes_of_query_products.extend(sample(list_of_indexes_for_particular_query, k = number_of_products_per_query))\n","      else:\n","        indexes_of_query_products.extend(list_of_indexes_for_particular_query[0:number_of_products_per_query])\n","  \n","  num_rows_dataframe = product_dataframe.shape[0]\n","  total_indexes_set = set(range(0,num_rows_dataframe))\n","  indexes_of_query_products_set = set(indexes_of_query_products)\n","  indexes_of_nonquery_products = list(total_indexes_set.difference(indexes_of_query_products_set))\n","  \n","  queried_products_sample_dataframe = product_dataframe.copy()\n","  queried_products_sample_dataframe = queried_products_sample_dataframe.drop(indexes_of_nonquery_products)\n","  return queried_products_sample_dataframe \n","\n","\n","#preprocess data\n","class PreprocessData:\n","  \"\"\"\n","  A class used to find the orientation of the robot with respect to a lane over time.\n","  \"\"\"\n","  def __init__(self):\n","    \"\"\"\n","    Create a PreprocessData object\n","    Parameters:\n","      - \n","    \"\"\"\n","    from nltk import download\n","    from nltk import word_tokenize\n","    # Import and download stopwords from NLTK.\n","    from nltk.corpus import stopwords\n","    \n","    download('punkt')  # Download data for tokenizer.\n","    download('stopwords')  # Download stopwords list.\n","\n","    # Remove stopwords.\n","    self.stop_words = stopwords.words('english')\n","\n","  def make_text_lower_case(self, text: str) -> str:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    return text.lower()\n","\n","  def tokenize_string_to_list_of_separate_words(self, text_string: str) -> List[str]:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    from nltk import word_tokenize\n","    return word_tokenize(text_string) \n","\n","  def remove_stopwords_from_text(self, text_list: List[str]) -> List[str]:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    return [w for w in text_list if not w in self.stop_words]\n","\n","  def remove_numbers_and_punctuation_from_text(self, text_list: List[str]) -> List[str]:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    return [w for w in text_list if w.isalpha()]\n","\n","  def text_preprocessing_for_WMD(self, text: str) -> List[str]:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    text = self.make_text_lower_case(text)\n","    text_list = self.tokenize_string_to_list_of_separate_words(text)\n","    text_list = self.remove_stopwords_from_text(text_list)\n","    text_list = self.remove_numbers_and_punctuation_from_text(text_list)\n","    return text_list\n","\n","  def text_preprocessing_with_textrank_for_WMD(self, text: str) -> List[str]:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    text = self.make_text_lower_case(text)\n","    \n","    tr4w = TextRank4Keyword()\n","    tr4w.analyze(text, candidate_pos = ['NOUN', 'PROPN', 'VERB'], window_size=4, lower=False)\n","    text_list = tr4w.get_list_of_strings_of_the_top_textrank_keywords_in_order_of_textrank(10)\n","\n","    # text_list = self.tokenize_string_to_list_of_separate_words(text)\n","    text_list = self.remove_stopwords_from_text(text_list)\n","    text_list = self.remove_numbers_and_punctuation_from_text(text_list)\n","    return text_list\n","\n","  def create_new_field_with_preprocessed_text_in_dataframe(self, dataframe: pd.core.frame.DataFrame, field_to_preprocess: str, new_field_name: str) -> pd.core.frame.DataFrame:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    num_rows_dataframe = len(dataframe.index)\n","    preprocessed_text_list = [np.nan] * num_rows_dataframe\n","    empty_text = []\n","    for ind in dataframe.index:\n","      field_text = dataframe[field_to_preprocess][ind]\n","      #check if field is empty or nan\n","      if field_text == field_text:\n","        preprocessed_text = self.text_preprocessing_for_WMD(field_text)\n","        # preprocessed_text = self.text_preprocessing_with_textrank_for_WMD(field_text)\n","        if preprocessed_text != []:\n","          preprocessed_text_list[ind] = preprocessed_text\n","        else:\n","          preprocessed_text_list[ind] = empty_text\n","      else:\n","        preprocessed_text_list[ind] = empty_text\n","\n","    dataframe.insert(len(dataframe.columns), new_field_name, preprocessed_text_list, True)\n","    return dataframe\n","    \n","  def create_new_field_with_preprocessed_text_from_multiple_fields_in_dataframe(self, dataframe: pd.core.frame.DataFrame, fields_to_preprocess_in_order_of_appending: List[str], new_field_name: str) -> pd.core.frame.DataFrame:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    num_rows_dataframe = len(dataframe.index)\n","    preprocessed_text_list = [np.nan] * num_rows_dataframe\n","    empty_text = []\n","\n","    for ind in dataframe.index:\n","      list_of_field_text = []\n","      for i in range(len(fields_to_preprocess_in_order_of_appending)):\n","        if dataframe[fields_to_preprocess_in_order_of_appending[i]][ind] == dataframe[fields_to_preprocess_in_order_of_appending[i]][ind]:\n","          list_of_field_text.append(dataframe[fields_to_preprocess_in_order_of_appending[i]][ind])\n","      field_text = \" \".join(filter(None, list_of_field_text))\n","      #check if field is empty or nan\n","      if field_text == field_text:\n","        preprocessed_text = self.text_preprocessing_for_WMD(field_text)\n","        # preprocessed_text = self.text_preprocessing_with_textrank_for_WMD(field_text)\n","        if preprocessed_text != []:\n","          preprocessed_text_list[ind] = preprocessed_text\n","        else:\n","          preprocessed_text_list[ind] = empty_text\n","      else:\n","        preprocessed_text_list[ind] = empty_text\n","\n","    dataframe.insert(len(dataframe.columns), new_field_name, preprocessed_text_list, True)\n","    return dataframe     \n","\n","  def create_new_field_in_dataframe_with_preprocessed_text_from_list_of_existing_fields(self, dataframe: pd.core.frame.DataFrame, fields_to_preprocess_in_order_of_appending: List[str], new_field_name: str, preprocessing_function: Callable[[str],List[str]]) -> pd.core.frame.DataFrame:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    num_rows_dataframe = len(dataframe.index)\n","    preprocessed_text_list = [np.nan] * num_rows_dataframe\n","    empty_text = []\n","\n","    for ind in dataframe.index:\n","      list_of_field_text = []\n","      for i in range(len(fields_to_preprocess_in_order_of_appending)):\n","        if dataframe[fields_to_preprocess_in_order_of_appending[i]][ind] == dataframe[fields_to_preprocess_in_order_of_appending[i]][ind]:\n","          list_of_field_text.append(dataframe[fields_to_preprocess_in_order_of_appending[i]][ind])\n","      field_text = \" \".join(filter(None, list_of_field_text))\n","      #check if field is empty or nan\n","      if field_text == field_text:\n","        preprocessed_text = preprocessing_function(field_text)\n","        # preprocessed_text = self.text_preprocessing_for_WMD(field_text)\n","        # preprocessed_text = self.text_preprocessing_with_textrank_for_WMD(field_text)\n","        if preprocessed_text != []:\n","          preprocessed_text_list[ind] = preprocessed_text\n","        else:\n","          preprocessed_text_list[ind] = empty_text\n","      else:\n","        preprocessed_text_list[ind] = empty_text\n","\n","    dataframe.insert(len(dataframe.columns), new_field_name, preprocessed_text_list, True)\n","    return dataframe\n","\n","#find WMD similarity\n","class WordMoversDistanceSimilarity:\n","  \"\"\"\n","  A class used to find the orientation of the robot with respect to a lane over time.\n","  \"\"\"\n","  def __init__(self, keyed_vector_model: gensim.models.keyedvectors.Word2VecKeyedVectors, list_of_document_lists: List[List[str]], num_similar_queries: int):\n","    \"\"\"\n","    Create a PreprocessData object\n","    Parameters:\n","      - \n","    \"\"\"\n","    self.model = keyed_vector_model\n","    self.wmd_corpus = list_of_document_lists\n","    self.num_similar_queries = num_similar_queries\n","    self.preprocess_data_object = PreprocessData()\n","    self.similarityMatrix = 0\n","    self.similar_document_tuple_of_indexes_and_similarity_scores = 0\n","\n","  def create_similarity_matrix(self):\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    from gensim.similarities import WmdSimilarity\n","    self.similarityMatrix = WmdSimilarity(self.wmd_corpus, self.model, self.num_similar_queries)\n","\n","  def find_similar_document_index_and_similarity(self, query_text: str) -> List[tuple]:\n","    query_text = preprocess(source_doc)\n","    self.similar_document_tuple_of_indexes_and_similarity_scores = instance[query]  # A query is simply a \"look-up\" in the similarity class.\n","    return self.similar_document_tuple_of_indexes_and_similarity_scores\n","\n","class PrintingResultAndCalculateLoss:\n","  \"\"\"\n","  A class used to find the orientation of the robot with respect to a lane over time.\n","  \"\"\"\n","  def __init__(self, similar_document_tuple_of_indexes_and_similarity_scores: int, data_frame: pd.core.frame.DataFrame):\n","    \"\"\"\n","    Create a PreprocessData object\n","    Parameters:\n","      - \n","    \"\"\"\n","    self.similar_document_tuple_of_indexes_and_similarity_scores = similar_document_tuple_of_indexes_and_similarity_scores\n","    self.data_frame = data_frame\n","\n","  def calculate_accuracy_of_similarity_prediction_for_a_query_text_from_actual_(index_of_query_product, score_cutoff):\n","    \"\"\"\n","    Calculate accuracy for a given single similar products query. This query text must be the actual data from the table, so that it will have a \n","    label \"query\" field and \"relevance\" field.\n","\n","    if index_of_query_product \"relevance\" field < score_cutoff: #we dont want to use query items in which the similarity to the search query was low.\n","      return accuracy = None\n","    else: \n","      for every index in similar_document_tuple_of_indexes_and_similarity_scores:\n","        number_of_correct_similar_products += (index_of_query_product \"query\" field == similar_index \"query\" field)*(similar_index \"relevance\" field >= score_cutoff)\n","    accuracy = number_of_correct_similar_products/num_similar_queries\n","\n","    Parameters:\n","      - \n","    \"\"\"\n","  #in order to be classified correctly, the similar product has to be in same search query as target category\n","  #and has to have a relavance score >= score cutoff.\n","  #I know this is flawed because there might be a product in the wrong search query but is still similar to the target product.\n","  #but overall, im hoping this case to be less\n","\n","  #\n","    num_similar_queries = len(index_of_query_product)\n","\n","    search_query = df['query'][sims[i][0]]\n","    for i in range(num_similar_queries):\n","      index_of_\n","\n","def calculate_output_dataframe_accuracy_and_write_in_dataframe(output_dataframe, relevance_threshold):\n","  numRows = output_dataframe[\"searched_item_unit_id\"].shape[0]\n","  numMatchingCategory = 0\n","  numNonMatchingCategory = 0\n","  # ipdb.set_trace()\n","  if output_dataframe.iloc[0][\"searched_item_relevance\"] < relevance_threshold:\n","    accuracy, numMatchingCategory, numNonMatchingCategory = (-1,-1,-1)\n","    return accuracy, numMatchingCategory, numNonMatchingCategory\n","  for i in range(numRows):  \n","    if output_dataframe.iloc[0][\"searched_item_query\"] == output_dataframe.iloc[i][\"similar_item_query\"]:\n","      numMatchingCategory += 1\n","    else:\n","      numNonMatchingCategory +=1\n","  # print(\"numMatchingCategory = \", numMatchingCategory)\n","  # print(\"numNonMatchingCategory = \", numNonMatchingCategory)\n","  ##ipdb.set_trace()\n","  accuracy = numMatchingCategory/numRows\n","  # output_dataframe[\"WMDAccuracy\"][0] = accuracy\n","  return accuracy, numMatchingCategory, numNonMatchingCategory\n","\n"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"txrgnzo39vTH","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"ZQwx32um_SJy","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"uDibAX_C_SF9","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"6Zw9Zdhk9vKA","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"Ybq5Fed_Lim2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":88},"executionInfo":{"status":"ok","timestamp":1592191464157,"user_tz":240,"elapsed":64351,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"outputId":"9a81b080-b18e-42c2-e297-406e6ce26079"},"source":["\n","############################\n","##download model and load model\n","##Inputs\n","url_to_word_embedding_zip = \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n","download_destination_folder_with_slash_at_end = \"data/\"\n","\n","download_success, desired_path_of_unzipped_word_embedding = download_and_unzip_pretrained_word_embedding(url_to_word_embedding_zip, download_destination_folder_with_slash_at_end)\n","word_embedding_file_path = desired_path_of_unzipped_word_embedding\n","model = load_word_embedding_into_KeyedVectors_model(word_embedding_file_path)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["datasetAlreadyExists\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Az_10UKtRVRs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1592201641042,"user_tz":240,"elapsed":2628,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"outputId":"b25b884b-fafb-4627-a4bc-d0f3bcb09bb2"},"source":["\n","#setup pretrained model\n","\n","# unitIdToGetIndexesOf = [711158459, 711158484]\n","\n","############################\n","#setup dataset\n","##Inputs\n","url_of_dataset = 'https://query.data.world/s/iccclhgzbyedtv54p5lresglnun3qz'\n","tuple_of_starting_and_ending_exclusive_index_of_sample = (0,500)\n","\n","dataset_frame = load_dataset_into_pandas(url_of_dataset, tuple_of_starting_and_ending_exclusive_index_of_sample)\n","list_of_queries_field = dataset_frame['query'].unique().tolist()\n","\n","indexesOfTargetItems = []\n","for i in range(len(list_of_queries_field)):\n","  # unitId = list_of_queries_field[i]\n","  # indexesOfTargetItems.append(pd.Index(dataset_frame['_unit_id']).get_loc(unitId))\n","  # print(i)\n","  # print(pd.Index(dataset_frame['query']).get_loc(list_of_queries_field[i]))\n","  if isinstance(pd.Index(dataset_frame['query']).get_loc(list_of_queries_field[i]), int):\n","    indexesOfTargetItems.append(pd.Index(dataset_frame['query']).get_loc(list_of_queries_field[i]))\n","  else:\n","    indexesOfTargetItems.append(pd.Index(dataset_frame['query']).get_loc(list_of_queries_field[i]).argmax())\n","print(list_of_queries_field)\n","# ipdb.set_trace()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['playstation 4', 'eye cream', 'routers', 'shaver panasonic', 'coffee grinder', 'high chairs', 'face cream', 'Brett Favre NY Titans jersey blue', 'aroma diffuser', 'Bluray Hobbit extended', 'white plain dinner set', 'double stroller', 'Vanilla Scented Perfumes', 'wireless mouse', 'spiderman', 'bedspreads', 'kitchen rugs', 'LED monitor', 'PS2 controller USB', 'extenal hardisk 500 gb', 'laptop lenovo']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tY8eF67X4aiQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592202277399,"user_tz":240,"elapsed":616421,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"outputId":"2bb723c5-e63e-455b-f946-a25a9c7c12d3"},"source":["\n","############################\n","for source_doc_index in indexesOfTargetItems:\n","  url_of_dataset = 'https://query.data.world/s/iccclhgzbyedtv54p5lresglnun3qz'\n","  tuple_of_starting_and_ending_exclusive_index_of_sample = (0,500)\n","\n","  dataset_frame = load_dataset_into_pandas(url_of_dataset, tuple_of_starting_and_ending_exclusive_index_of_sample)\n","\n","  ############################\n","  ##preprocess text\n","  ##create a new field that has\n","  ##Inputs\n","  # dataframe = dataset_frame\n","  # field_to_preprocess = 'product_description'\n","  # new_field_name = 'product_description_preprocessed'\n","\n","  # preprocess_data_object = PreprocessData()\n","  # #create_new_field_with_preprocessed_text_in_dataframe(self, dataframe: pd.core.frame.DataFrame, field_to_preprocess: str, new_field_name: str) -> pd.core.frame.DataFrame:\n","  # dataframe_modified = preprocess_data_object.create_new_field_with_preprocessed_text_in_dataframe(dataframe, field_to_preprocess, new_field_name)\n","\n","  ############################\n","  #alternate preprocess text\n","  ##Inputs\n","  dataframe = dataset_frame\n","  # fields_to_preprocess_in_order_of_appending = ['product_title', 'product_description']\n","  fields_to_preprocess_in_order_of_appending = ['product_title']\n","  new_field_name = \",\".join(fields_to_preprocess_in_order_of_appending)+'_preprocessed'\n","\n","  print(\"new_field_name = \", new_field_name)\n","\n","  preprocess_data_object = PreprocessData()\n","  #create_new_field_with_preprocessed_text_in_dataframe(self, dataframe: pd.core.frame.DataFrame, field_to_preprocess: str, new_field_name: str) -> pd.core.frame.DataFrame:\n","  dataframe_modified = preprocess_data_object.create_new_field_with_preprocessed_text_from_multiple_fields_in_dataframe(dataframe, fields_to_preprocess_in_order_of_appending, new_field_name)\n","\n","  ############################\n","  #Create Word movers distance similarity matrix\n","  #Inputs\n","  num_best_var = 20\n","  # source_doc_index = 0\n","  print(\"source_doc_index = \", source_doc_index)\n","\n","  source_doc_preprocessed = dataframe_modified[new_field_name][source_doc_index]\n","  print(source_doc_preprocessed)\n","\n","  from gensim.similarities import WmdSimilarity\n","  wmd_corpus = dataframe_modified[new_field_name].tolist()\n","  wmd_object_for_description = WmdSimilarity(wmd_corpus, model, num_best = num_best_var)\n","\n","  similar_item_indexes_and_similarity = wmd_object_for_description[source_doc_preprocessed]\n","\n","  similar_product_index = []\n","  similar_product_similarities =[]\n","\n","  for i in range(num_best_var):\n","    print(i)\n","    similar_product_index.append(similar_item_indexes_and_similarity[i][0])\n","    similar_product_similarities.append(similar_item_indexes_and_similarity[i][1])\n","\n","  target_index = ['']*num_best_var\n","  target_title_list = ['']*num_best_var\n","  target_search_query_list = ['']*num_best_var\n","  target_search_relevance_to_query = ['']*num_best_var\n","  target_description_list = ['']*num_best_var\n","  target_description_preprocesed_list = ['']*num_best_var\n","  wmd_accuracy = ['']*num_best_var\n","\n","  target_index[0] = source_doc_index\n","  target_title_list[0] = dataframe_modified['product_title'][source_doc_index]\n","  target_search_query_list[0] = dataframe_modified['query'][source_doc_index]\n","  target_search_relevance_to_query[0] = dataframe_modified['relevance'][source_doc_index]\n","  target_description_list[0] = dataframe_modified['product_description'][source_doc_index]\n","  target_description_preprocesed_list[0] = dataframe_modified[new_field_name][source_doc_index]\n","\n","  data = {'targetIndex': target_index, \n","          'targetTitle': target_title_list, \n","          'targetSearchQuery': target_search_query_list,\n","          'targetRelevanceToQuery': target_search_relevance_to_query, \n","          'targetDescription': target_description_list,\n","          'target'+new_field_name: target_description_preprocesed_list,\n","          'similarIndex': similar_product_index,\n","          'similarTitle': dataframe_modified['product_title'][similar_product_index].tolist(),\n","          'similarSearchQuery': dataframe_modified['query'][similar_product_index].tolist(),\n","          'similarRelevanceToQuery': dataframe_modified['relevance'][similar_product_index].tolist(),\n","          'similarDescription': dataframe_modified['product_description'][similar_product_index].tolist(),\n","          'similar'+new_field_name: dataframe_modified[new_field_name][similar_product_index].tolist(),\n","          'WMDRank': list(range(0, num_best_var)),\n","          'WMDSimilarity': similar_product_similarities, \n","          'WMDAccuracy': wmd_accuracy} \n","  output_dataframe = pd.DataFrame(data)\n","  relevance_threshold = 2\n","  accuracy, numMatchingCategory, numNonMatchingCategory = calculate_output_dataframe_accuracy_and_write_in_dataframe(output_dataframe, relevance_threshold)\n","  output_csv_name = 'csv_logs_only_title_06_14_20/searchQuery_'+dataframe_modified['query'][source_doc_index]+'_numMatches_'+str(num_best_var)\n","\n","  output_dataframe.to_csv(output_csv_name+'.csv')\n","\n","\n","\n","# wmd_corpus_for_description = create_corpus_from_preprocessed_text_field(self, dataframe: pd.core.frame.DataFrame, ) -> List[List[str]]\n","#   dataframe['preprocessed_description'].toList()\n","# WMD_object_for_description = create_WMD_object(wmd_corpus, model, num_best)\n","# similar_item_indexes_and_similarity = get_similar_item_indexes_and_similarity()\n","\n","# print_out_similar_item_details_to_dataframe\n","\n","\n","# #Find the similarities for one item in the search query, and display the fields of title, relevance, description, wmd ranking, and wmd rating\n","# download_and_unzip_pretrained_word_embedding"],"execution_count":null,"outputs":[{"output_type":"stream","text":["new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  0\n","['sony', 'playstation', 'latest', 'model', 'gb', 'jet', 'black', 'console']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  20\n","numNonMatchingCategory =  0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:299: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"],"name":"stderr"},{"output_type":"stream","text":["new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  25\n","['new', 'clinique', 'repairwear', 'laser', 'focus', 'wrinkle', 'correcting', 'eye', 'cream']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  17\n","numNonMatchingCategory =  3\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  50\n","['router']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  20\n","numNonMatchingCategory =  0\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  75\n","['panasonic', 'cordless', 'rechargeable', 'men', 'electric', 'shaver']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  20\n","numNonMatchingCategory =  0\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  100\n","['baratza', 'solis', 'maestro', 'conical', 'burr', 'coffee', 'bean', 'grinder', 'works', 'great', 'nice', 'cond']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  20\n","numNonMatchingCategory =  0\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  125\n","['brand', 'new', 'graco', 'tablefit', 'highchair', 'three', 'reclining', 'options', 'finley']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  5\n","numNonMatchingCategory =  15\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  150\n","['luminesce', 'advance', 'night', 'repair', 'cream', 'jeunesse', 'face', 'mosturizer', 'ml', 'wrinkle']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  12\n","numNonMatchingCategory =  8\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  175\n","['men', 'nwt', 'vintage', 'reebok', 'brett', 'favre', 'new', 'york', 'jets', 'titans', 'jersey', 'size', 'xl']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  20\n","numNonMatchingCategory =  0\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  200\n","['color', 'ultrasonic', 'home', 'aroma', 'humidifier', 'air', 'diffuser', 'purifier', 'lonizer', 'atomizer']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  20\n","numNonMatchingCategory =  0\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  225\n","['unexpected', 'journey', 'extended', 'edition', 'dvd']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  20\n","numNonMatchingCategory =  0\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  250\n","['corelle', 'plain', 'white', 'dinner', 'plate', 'set']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  20\n","numNonMatchingCategory =  0\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  275\n","['graco', 'classic', 'connect', 'lx', 'double', 'stroller', 'fiji']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  20\n","numNonMatchingCategory =  0\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  300\n","['vanilla', 'tahitian', 'bottle', 'perfume', 'body', 'oil', 'strong', 'scented', 'fragrance']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  20\n","numNonMatchingCategory =  0\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  324\n","['red', 'wireless', 'optical', 'mouse', 'mice', 'usb', 'receiver', 'pc', 'laptop', 'macbook']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  20\n","numNonMatchingCategory =  0\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  349\n","['hand', 'marvel', 'legends', 'infinite', 'series', 'hobgoblin', 'baf', 'spiderman']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  20\n","numNonMatchingCategory =  0\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  374\n","['english', 'roses', 'bedding', 'quilt', 'bedspread', 'coverlet', 'piece', 'reversible', 'king', 'set']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  19\n","numNonMatchingCategory =  1\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  399\n","['modern', 'indoor', 'cushion', 'kitchen', 'rug', 'floor', 'mat', 'actual', 'x']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  20\n","numNonMatchingCategory =  0\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  424\n","['new', 'samsung', 'series', 'white', 'high', 'glossy', 'toc', 'led', 'hdmi', 'monitor']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  20\n","numNonMatchingCategory =  0\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  449\n","['psx', 'pc', 'usb', 'dual', 'controller', 'adapter', 'converter']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  20\n","numNonMatchingCategory =  0\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  474\n","['glyph', 'hard', 'drive']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  20\n","numNonMatchingCategory =  0\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  499\n","['ibm', 'thinkpad', 'lenovo', 'laptop', 'ghz', 'gb', 'win', 'wifi']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  1\n","numNonMatchingCategory =  19\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Wi4QIDV2bsOZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":717},"executionInfo":{"status":"ok","timestamp":1592223647756,"user_tz":240,"elapsed":4785,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"outputId":"20f4d2c8-3883-4597-be8e-22d78a60f46a"},"source":["import numpy as np\n","import pandas as pd\n","import glob\n","import ipdb\n","def calculate_average_accuracy_from_csv_logs(folder_path_of_csv_logs: str) ->  Tuple[float, pd.core.frame.DataFrame]:\n","  #get paths of all the csv files in the folder_path_of_csv_logs directory\n","  all_files = glob.glob(folder_path_of_csv_logs + \"*.csv\")\n","\n","  #initialize lists to feed into output dataframe later\n","  search_query_list = []\n","  accuracy_list = []\n","\n","  #for each file:\n","  for filename in all_files:\n","    #load the file into pandas\n","    df = pd.read_csv(filename)\n","     #read the accuracy value, and save it into a pandas dataframe with fields: search query, accuracy\n","    accuracy_list.append(df['WMDAccuracy'][0])\n","    search_query_list.append(df['targetSearchQuery'][0])\n","  \n","  #calculate average of all the accuracies\n","  average_accuracy = sum(accuracy_list)/len(accuracy_list)\n","   \n","  #return dictionary of dataframe, as well as average accuracy\n","  search_query_accuracy_dataframe = {\n","      'searchQuery': search_query_list, \n","      'accuracy': accuracy_list, \n","      } \n","  output_dataframe = pd.DataFrame(search_query_accuracy_dataframe)\n","\n","  return average_accuracy, output_dataframe\n","#####################\n","folder_path_of_csv_logs = 'csv_logs_only_title_06_14_20/'\n","# ipdb.set_trace()\n","average_accuracy, output_dataframe = calculate_average_accuracy_from_csv_logs(folder_path_of_csv_logs)\n","print(\"average accuracy of \", folder_path_of_csv_logs, \" = \", average_accuracy)\n","output_dataframe"],"execution_count":null,"outputs":[{"output_type":"stream","text":["average accuracy of  csv_logs_only_title_06_14_20/  =  0.8904761904761904\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>searchQuery</th>\n","      <th>accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>playstation 4</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>eye cream</td>\n","      <td>0.85</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>routers</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>shaver panasonic</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>coffee grinder</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>high chairs</td>\n","      <td>0.25</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>face cream</td>\n","      <td>0.60</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Brett Favre NY Titans jersey blue</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>aroma diffuser</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Bluray Hobbit extended</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>white plain dinner set</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>double stroller</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>Vanilla Scented Perfumes</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>wireless mouse</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>spiderman</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>bedspreads</td>\n","      <td>0.95</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>kitchen rugs</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>LED monitor</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>PS2 controller USB</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>extenal hardisk 500 gb</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>laptop lenovo</td>\n","      <td>0.05</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                          searchQuery  accuracy\n","0                       playstation 4      1.00\n","1                           eye cream      0.85\n","2                             routers      1.00\n","3                    shaver panasonic      1.00\n","4                      coffee grinder      1.00\n","5                         high chairs      0.25\n","6                          face cream      0.60\n","7   Brett Favre NY Titans jersey blue      1.00\n","8                      aroma diffuser      1.00\n","9              Bluray Hobbit extended      1.00\n","10             white plain dinner set      1.00\n","11                    double stroller      1.00\n","12           Vanilla Scented Perfumes      1.00\n","13                     wireless mouse      1.00\n","14                          spiderman      1.00\n","15                         bedspreads      0.95\n","16                       kitchen rugs      1.00\n","17                        LED monitor      1.00\n","18                 PS2 controller USB      1.00\n","19             extenal hardisk 500 gb      1.00\n","20                      laptop lenovo      0.05"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"5fkjKIDq2cal","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1591875337158,"user_tz":240,"elapsed":3390,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"outputId":"a7f64216-36df-4828-fc81-bb024864122e"},"source":["import unittest\n","class MyTest(unittest.TestCase):\n","  @unittest.skip(\"demonstrating skipping\")\n","  def test_download_and_unzip_pretrained_word_embedding(self):\n","    url_to_word_embedding_zip = \"http://dumps.wikimedia.org/other/articlefeedback/aa_combined-20110321.csv.gz\"\n","    download_destination_path = \"data/\"\n","    path_to_word_embedding  = download_destination_path + \"aa_combined-20110321.csv\"\n","    #ipdb.set_trace()\n","    !rm $path_to_word_embedding\n","    success, desired_path_of_unzipped_word_embedding = download_and_unzip_pretrained_word_embedding(url_to_word_embedding_zip, download_destination_path)\n","    self.assertEqual(success, 1)\n","\n","  @unittest.skip(\"demonstrating skipping\")\n","  def test_load_word_embedding_into_KeyedVectors_model(self):\n","    url_to_word_embedding_zip = \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n","    download_destination_folder_with_slash_at_end = \"data/\"\n","\n","    download_success, desired_path_of_unzipped_word_embedding = download_and_unzip_pretrained_word_embedding(url_to_word_embedding_zip, download_destination_folder_with_slash_at_end)\n","    if download_success == 0:\n","      success = 0\n","    else:\n","      word_embedding_file_path = desired_path_of_unzipped_word_embedding\n","      model = load_word_embedding_into_KeyedVectors_model(word_embedding_file_path)\n","      \n","      if isinstance(model, gensim.models.keyedvectors.Word2VecKeyedVectors):\n","        success =1\n","      else:\n","        success = 0\n","    \n","    self.assertEqual(success, 1)\n","\n","  @unittest.skip(\"demonstrating skipping\")\n","  def test_load_dataset_into_pandas(self):\n","    url_of_dataset = 'https://query.data.world/s/iccclhgzbyedtv54p5lresglnun3qz'\n","    tuple_of_starting_and_ending_exclusive_index_of_sample = (0,10)\n","    dataset_frame = load_dataset_into_pandas(url_of_dataset, tuple_of_starting_and_ending_exclusive_index_of_sample)\n","    \n","    #pd.set_option('display.max_columns', None)\n","    #print(dataset_frame)\n","    \n","    self.assertEqual(isinstance(dataset_frame, pd.core.frame.DataFrame),1) \n","    self.assertEqual(dataset_frame.shape[0] == (tuple_of_starting_and_ending_exclusive_index_of_sample[1]-tuple_of_starting_and_ending_exclusive_index_of_sample[0]), 1)\n","  \n","  def test_create_new_field_with_preprocessed_text_in_dataframe(self):\n","    url_of_dataset = 'https://query.data.world/s/iccclhgzbyedtv54p5lresglnun3qz'\n","    tuple_of_starting_and_ending_exclusive_index_of_sample = (0,10)\n","    dataset_frame = load_dataset_into_pandas(url_of_dataset, tuple_of_starting_and_ending_exclusive_index_of_sample)\n","    \n","    dataframe = dataset_frame\n","    field_to_preprocess = 'product_description'\n","    new_field_name = 'product_description_preprocessed'\n","    \n","    preprocess_data_object = PreprocessData()\n","    #create_new_field_with_preprocessed_text_in_dataframe(self, dataframe: pd.core.frame.DataFrame, field_to_preprocess: str, new_field_name: str) -> pd.core.frame.DataFrame:\n","    dataframe_modified = preprocess_data_object.create_new_field_with_preprocessed_text_in_dataframe(dataframe, field_to_preprocess, new_field_name)\n","    #end of function\n","    #self.assertEqual(isinstance(dataframe_modified, pd.core.frame.DataFrame),1) \n","    pd.set_option('display.max_columns', None)\n","    print(dataframe_modified)\n","    \n","\n","if __name__ == '__main__':\n","  unittest.main(argv=['first-arg-is-ignored'], exit=False)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":[".sss"],"name":"stderr"},{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","    _unit_id  relevance  relevance:variance  \\\n","0  711158459       3.67               0.471   \n","1  711158460       4.00               0.000   \n","2  711158461       4.00               0.000   \n","3  711158462       3.67               0.471   \n","4  711158463       3.33               0.471   \n","5  711158464       3.20               0.748   \n","6  711158465       4.00               0.000   \n","7  711158466       4.00               0.000   \n","8  711158467       3.75               0.433   \n","9  711158468       2.33               0.471   \n","\n","                                       product_image  \\\n","0  http://thumbs2.ebaystatic.com/d/l225/m/mzvzEUI...   \n","1  http://thumbs3.ebaystatic.com/d/l225/m/mJNDmSy...   \n","2  http://thumbs4.ebaystatic.com/d/l225/m/m10NZXA...   \n","3  http://thumbs2.ebaystatic.com/d/l225/m/mZZXTmA...   \n","4  http://thumbs3.ebaystatic.com/d/l225/m/mzvzEUI...   \n","5  http://thumbs4.ebaystatic.com/d/l225/m/mzvzEUI...   \n","6  http://thumbs4.ebaystatic.com/d/l225/m/m9TQTiW...   \n","7  http://thumbs4.ebaystatic.com/d/l225/m/mTZYG5N...   \n","8  http://thumbs2.ebaystatic.com/d/l225/m/mX5Qphr...   \n","9  http://thumbs2.ebaystatic.com/d/l225/m/mGjN4Ir...   \n","\n","                                        product_link  \\\n","0  http://www.ebay.com/itm/Sony-PlayStation-4-PS4...   \n","1  http://www.ebay.com/itm/Sony-PlayStation-4-Lat...   \n","2  http://www.ebay.com/itm/Sony-PlayStation-4-PS4...   \n","3  http://www.ebay.com/itm/Sony-PlayStation-4-500...   \n","4  http://www.ebay.com/itm/Sony-PlayStation-4-PS4...   \n","5  http://www.ebay.com/itm/Sony-PlayStation-4-PS4...   \n","6  http://www.ebay.com/itm/BRAND-NEW-Sony-PlaySta...   \n","7  http://www.ebay.com/itm/Sony-PlayStation-4-500...   \n","8  http://www.ebay.com/itm/Sony-PlayStation-4-Lat...   \n","9  http://www.ebay.com/itm/Sony-PlayStation-4-Lat...   \n","\n","                   product_price  \\\n","0                       $329.98    \n","1                       $324.84    \n","2                       $324.83    \n","3                       $350.00    \n","4  $308.00\\nTrending at\\n$319.99   \n","5                       $310.00    \n","6                       $910.00    \n","7                       $299.99    \n","8                       $350.00    \n","9                       $469.97    \n","\n","                                       product_title          query  rank  \\\n","0  Sony PlayStation 4 (PS4) (Latest Model)- 500 G...  playstation 4     1   \n","1  Sony PlayStation 4 (Latest Model)- 500 GB Jet ...  playstation 4     2   \n","2    Sony PlayStation 4 PS4 500 GB Jet Black Console  playstation 4     3   \n","3  Sony - PlayStation 4 500GB The Last of Us Rema...  playstation 4     4   \n","4  Sony PlayStation 4 (PS4) (Latest Model)- 500 G...  playstation 4     5   \n","5  Sony PlayStation 4 (PS4) (Latest Model)- 500 G...  playstation 4     6   \n","6          BRAND NEW Sony PlayStation 4 BUNDLE 500gb  playstation 4     7   \n","7  Sony PlayStation 4 500GB, Dualshock Wireless C...  playstation 4     8   \n","8  Sony PlayStation 4 (Latest Model)- 500 GB Jet ...  playstation 4     9   \n","9  Sony PlayStation 4 (Latest Model)- 500 GB Jet ...  playstation 4    10   \n","\n","  source                                                url  \\\n","0   eBay  http://www.ebay.com/sch/i.html?_from=R40&_trks...   \n","1   eBay  http://www.ebay.com/sch/i.html?_from=R40&_trks...   \n","2   eBay  http://www.ebay.com/sch/i.html?_from=R40&_trks...   \n","3   eBay  http://www.ebay.com/sch/i.html?_from=R40&_trks...   \n","4   eBay  http://www.ebay.com/sch/i.html?_from=R40&_trks...   \n","5   eBay  http://www.ebay.com/sch/i.html?_from=R40&_trks...   \n","6   eBay  http://www.ebay.com/sch/i.html?_from=R40&_trks...   \n","7   eBay  http://www.ebay.com/sch/i.html?_from=R40&_trks...   \n","8   eBay  http://www.ebay.com/sch/i.html?_from=R40&_trks...   \n","9   eBay  http://www.ebay.com/sch/i.html?_from=R40&_trks...   \n","\n","                                 product_description  \\\n","0  The PlayStation 4 system opens the door to an ...   \n","1  The PlayStation 4 system opens the door to an ...   \n","2  The PlayStation 4 system opens the door to an ...   \n","3                                                NaN   \n","4  The PlayStation 4 system opens the door to an ...   \n","5  The PlayStation 4 system opens the door to an ...   \n","6                                                NaN   \n","7  The PlayStation 4 system opens the door to an ...   \n","8                                                NaN   \n","9  The PlayStation 4 system opens the door to an ...   \n","\n","                    product_description_preprocessed  \n","0  [playstation, system, opens, door, incredible,...  \n","1  [playstation, system, opens, door, incredible,...  \n","2  [playstation, system, opens, door, incredible,...  \n","3                                                NaN  \n","4  [playstation, system, opens, door, incredible,...  \n","5  [playstation, system, opens, door, incredible,...  \n","6                                                NaN  \n","7  [playstation, system, opens, door, incredible,...  \n","8                                                NaN  \n","9  [playstation, system, opens, door, incredible,...  \n"],"name":"stdout"},{"output_type":"stream","text":["\n","----------------------------------------------------------------------\n","Ran 4 tests in 2.871s\n","\n","OK (skipped=3)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"hCabShfxUEVw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1591912183149,"user_tz":240,"elapsed":882,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"outputId":"647288b5-9721-41f3-db7f-09a8cbc1e3bc"},"source":["dataframe_modified['product_title'][0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Sony PlayStation 4 (PS4) (Latest Model)- 500 GB Jet Black Console'"]},"metadata":{"tags":[]},"execution_count":143}]},{"cell_type":"code","metadata":{"id":"1y_JaquUYS7G","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1591915667083,"user_tz":240,"elapsed":477,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"outputId":"97611e88-fb87-4091-d2ea-975f1e5bef55"},"source":["print(output_dataframe[\"similarDescription\"].shape)\n","numMatchingCategory"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(20,)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["11"]},"metadata":{"tags":[]},"execution_count":154}]},{"cell_type":"code","metadata":{"id":"NCgEBYErYS4q","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4be0wAJnYS2B","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6RT-Tmv66Lwh","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aApkK-yw6Ltw","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d4w0UxoO6LrO","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kQmVap1r6LpS","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"26EnIj176LmW","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_ZWwatEN6LkY","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0VRI9yck6Lhv","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xM1ZfUU_6LfQ","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dkYsoZMW6LcJ","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}