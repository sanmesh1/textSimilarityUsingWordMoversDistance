{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CreateViews.ipynb","provenance":[],"collapsed_sections":["f0f0pbSbceId"],"authorship_tag":"ABX9TyMEmcoJp9CjxT2/3Bad3rvY"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"ZdGKiaG8XHQ5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592886721667,"user_tz":240,"elapsed":5906,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"outputId":"c6e4f921-fc3d-4fcb-db5c-4316e2051c2e"},"source":["#pip installs\n","! pip install ipdb -q\n","import ipdb \n","#place the line of code below from whichever line you want to start debugging\n","#type n for step over and s for step into\n","# ipdb.set_trace()\n","\n","from time import time\n","import gensim\n","import pandas as pd\n","from typing import List, Tuple, Callable\n","import numpy as np\n","from gensim.models.keyedvectors import KeyedVectors\n","from gensim.models.wrappers import FastText\n","from gensim.similarities import WmdSimilarity\n","from ast import literal_eval"],"execution_count":2,"outputs":[{"output_type":"stream","text":["  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8pCZgWJlXzbx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1592887115325,"user_tz":240,"elapsed":20504,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"outputId":"91b0df2d-d054-41c0-d293-ceccda1ae35e"},"source":["  # print(\"numMatchingCategory = \", numMatchingCategory)\n","#repoPath should start with the path \n","#'/content/gdrive/My Drive/' and go to the folder where the git repo is, and should end with the name of the repo:\n","repoPath = '/content/gdrive/My Drive/TestingGitCloneIntoGoogleDrive/textSimilarityUsingWordMoversDistance'\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","import os\n","os.chdir(repoPath)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_4J1DOsfST_z","colab_type":"text"},"source":["#create accuracy view"]},{"cell_type":"markdown","metadata":{"id":"iUyqBQ_QU0yk","colab_type":"text"},"source":["##Input"]},{"cell_type":"code","metadata":{"id":"CgjpWp1VU2_M","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592890622647,"user_tz":240,"elapsed":232,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}}},"source":["#input list of paths for each accuracy dataframe from the experiments\n","list_of_paths_for_each_experiment_folder = ['experiment_outputs/experiment3/', 'experiment_outputs/experiment4/', 'experiment_outputs/experiment5/']\n","\n","#input list of names of the experiments\n","list_of_names_of_each_experiment = ['word2vec_wmd', 'word2vec_cosine_similarity', 'fasttext_cosine_similarity']\n","\n","#\n","desired_path_of_csv_to_be_created = 'inter_experiment_logs/accuracyAcrossDifferentAlgorithms.csv'"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NGnpr9WFU3TX","colab_type":"text"},"source":["##Code"]},{"cell_type":"code","metadata":{"id":"_g3XYmwNSM3F","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592890624180,"user_tz":240,"elapsed":305,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}}},"source":["#input list of paths for each accuracy dataframe from the experiments\n","list_of_paths_for_each_experiment_accuracy = [s+'accuracy_dataframe.csv' for s in list_of_paths_for_each_experiment_folder]\n","\n","#input names of the experiments\n","\n","#load all the dataframes\n","list_of_accuracy_dataframes = []\n","for i in range(len(list_of_paths_for_each_experiment_accuracy)):\n","  list_of_accuracy_dataframes.append(load_csv_from_local_directory_to_dataframe(list_of_paths_for_each_experiment_accuracy[i]))\n","\n","\n","#searched_item_unit_id, searched_item_query, for accuracy per item create three fields with one for each of the experiments, for total average accuracy create three fields with one for each of the experiments\n","##create template file\n","accuracy_comparison_dictionary = {\n","    'searched_item_unit_id': list_of_accuracy_dataframes[0]['searched_item_unit_id'].tolist(),\n","    'searched_item_query': list_of_accuracy_dataframes[0]['searched_item_query'].tolist()\n","}\n","\n","#\n","for i in range(len(list_of_paths_for_each_experiment_accuracy)):\n","  name_of_each_experiment = list_of_names_of_each_experiment[i]\n","  item_accuracy_name = name_of_each_experiment+'_searched_item_accuracy'\n","  total_average_accuracy = name_of_each_experiment+'_total_average_accuracy'\n","  accuracy_comparison_dictionary[item_accuracy_name] = list_of_accuracy_dataframes[i]['searched_item_accuracy'].tolist()\n","  accuracy_comparison_dictionary[total_average_accuracy] = list_of_accuracy_dataframes[i]['total_average_accuracy_for_experiment'].tolist()\n","\n","\n","\n","accuracy_comparison_dataframe = pd.DataFrame(data=accuracy_comparison_dictionary)\n","accuracy_comparison_dataframe.to_csv(desired_path_of_csv_to_be_created, index = False)"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HKRMf7-MlP-C","colab_type":"text"},"source":["#ShowTopSimilar Products for each algorithm"]},{"cell_type":"markdown","metadata":{"id":"QeKuQboklZly","colab_type":"text"},"source":["##Input"]},{"cell_type":"code","metadata":{"id":"XQXQBvr2lXKC","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592889994410,"user_tz":240,"elapsed":307,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}}},"source":["#input list of paths for each accuracy dataframe from the experiments\n","list_of_paths_for_each_experiment_folder = ['experiment_outputs/experiment3/', 'experiment_outputs/experiment4/', 'experiment_outputs/experiment5/']\n","\n","#input list of names of the experiments\n","list_of_names_of_each_experiment = ['word2vec_wmd', 'word2vec_cosine_similarity', 'fasttext_cosine_similarity']\n","\n","#\n","desired_path_of_csv_to_be_created_top_similar = 'inter_experiment_logs/topNSimilarItemsForEachAlgo.csv'"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RTQ1zpf4lbKK","colab_type":"text"},"source":["##Code"]},{"cell_type":"code","metadata":{"id":"rNToJg0Glbxi","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592890556195,"user_tz":240,"elapsed":274,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}}},"source":["#input list of paths for each accuracy dataframe from the experiments\n","list_of_paths_for_each_experiment_view_to_debug_similarity = [s+'view_to_debug_similarity_dataframe.csv' for s in list_of_paths_for_each_experiment_folder]\n","\n","#input names of the experiments\n","\n","#load all the dataframes\n","list_of_view_to_debug_similarity_dataframes = []\n","for i in range(len(list_of_paths_for_each_experiment_view_to_debug_similarity)):\n","  list_of_view_to_debug_similarity_dataframes.append(load_csv_from_local_directory_to_dataframe(list_of_paths_for_each_experiment_view_to_debug_similarity[i]))\n","\n","\n","#searched_item_unit_id, searched_item_query, for accuracy per item create three fields with one for each of the experiments, for total average accuracy create three fields with one for each of the experiments\n","##create template file\n","similarity_comparison_dictionary = {\n","    'title_searched_item': list_of_view_to_debug_similarity_dataframes[0]['title_searched_item'].tolist(),\n","    'query_searched_item': list_of_view_to_debug_similarity_dataframes[0]['query_searched_item'].tolist(),\n","}\n","\n","#\n","for i in range(len(list_of_paths_for_each_experiment_view_to_debug_similarity)):\n","  name_of_each_experiment = list_of_names_of_each_experiment[i]\n","\n","  title_similar_item_name = name_of_each_experiment+'_title_similar_item'\n","  query_similar_item_name = name_of_each_experiment+'_query_similar_item'\n","  composite_similarity_score_name = name_of_each_experiment+'_composite_similarity_score'\n","  rank_name = name_of_each_experiment+'_rank'\n","  accuracy_for_item_name = name_of_each_experiment+'_accuracy_for_item'\n","\n","  similarity_comparison_dictionary[title_similar_item_name] = list_of_view_to_debug_similarity_dataframes[i]['title_similar_item'].tolist()\n","  similarity_comparison_dictionary[query_similar_item_name] = list_of_view_to_debug_similarity_dataframes[i]['query_similar_item'].tolist()\n","  # similarity_comparison_dictionary[composite_similarity_score_name] = list_of_view_to_debug_similarity_dataframes[i]['composite_similarity_score'].tolist()\n","  # similarity_comparison_dictionary[rank_name] = list_of_view_to_debug_similarity_dataframes[i]['rank'].tolist()\n","  # similarity_comparison_dictionary[accuracy_for_item_name] = list_of_view_to_debug_similarity_dataframes[i]['accuracy_for_item'].tolist()\n","\n","\n","similarity_comparison_dataframe = pd.DataFrame(data=similarity_comparison_dictionary)\n","similarity_comparison_dataframe.to_csv(desired_path_of_csv_to_be_created_top_similar, index = False)"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f0f0pbSbceId","colab_type":"text"},"source":["#Background Functions"]},{"cell_type":"code","metadata":{"id":"FWii_7GVXHEn","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592886728412,"user_tz":240,"elapsed":1132,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}}},"source":["#download pretrained word embedding model and load it\n","##download a file\n","##unzip a file\n","def download_and_unzip_pretrained_word_embedding(url_to_word_embedding_zip: str, download_destination_folder_with_slash_at_end: str) -> Tuple[bool, str]:\n","  #download if unzipped file exists already\n","  #if not, unzip and check if file exists after unzipping\n","  #if file exists, return 1 for success\n","  import os.path\n","  from os import path\n","  zipped_file_name = os.path.basename(url_to_word_embedding_zip)\n","  unzipped_file_name = os.path.splitext(zipped_file_name)[0]\n","\n","  path_of_zipped_word_embedding = download_destination_folder_with_slash_at_end + zipped_file_name\n","  desired_path_of_unzipped_word_embedding = download_destination_folder_with_slash_at_end + unzipped_file_name\n","  zip_file_extension = os.path.splitext(url_to_word_embedding_zip)[1][1:]\n","\n","  if path.exists(desired_path_of_unzipped_word_embedding) == False:\n","    !wget -c $url_to_word_embedding_zip -P $download_destination_folder_with_slash_at_end\n","    if zip_file_extension == 'gz':\n","      !gunzip $path_of_zipped_word_embedding\n","    elif zip_file_extension == 'zip':\n","      !unzip $path_of_zipped_word_embedding -d $download_destination_folder_with_slash_at_end\n","    else:\n","      print(\"This function doesnt handle zipped files with extension: \", zip_file_extension)\n","      return (False, None)\n","  else:\n","    print(\"datasetAlreadyExists\")\n","\n","  if path.exists(desired_path_of_unzipped_word_embedding) == True:\n","    successful_in_creating_word_embedding = True\n","  else:\n","    successful_in_creating_word_embedding = False\n","  return (successful_in_creating_word_embedding, desired_path_of_unzipped_word_embedding)\n","\n","##load word embedding\n","def load_word_embedding_into_KeyedVectors_model(word_embedding_file_path: str) -> gensim.models.keyedvectors.Word2VecKeyedVectors:\n","  from gensim.models.keyedvectors import KeyedVectors\n","  model = KeyedVectors.load_word2vec_format(word_embedding_file_path, binary=True)\n","  return model\n","\n","def load_word2vec_word_embedding_into_KeyedVectors_model(word_embedding_file_path: str) -> gensim.models.keyedvectors.Word2VecKeyedVectors:\n","  model = KeyedVectors.load_word2vec_format(word_embedding_file_path, binary=True)\n","  return model\n","\n","def load_fasttext_word_embedding_into_KeyedVectors_model(word_embedding_file_path: str) -> gensim.models.keyedvectors.Word2VecKeyedVectors:\n","  model = FastText.load_fasttext_format(model_path)\n","  return model\n","\n","##delete below because not used\n","# #load dataset\n","# #extract desired features from dataset\n","# def load_dataset_into_pandas(url_of_dataset: str, tuple_of_starting_and_ending_exclusive_index_of_sample: tuple) -> pd.core.frame.DataFrame:\n","#   #Create pandas dataframe on dataset\n","#   import pandas as pd\n","#   #https://data.world/crowdflower/ecommerce-search-relevance\n","#   df = pd.read_csv(url_of_dataset, encoding = \"ISO-8859-1\")\n","#   start,end = tuple_of_starting_and_ending_exclusive_index_of_sample\n","#   df = df.iloc[start:end]\n","#   return df\n","\n","def load_csv_from_local_directory_to_dataframe(path_of_csv: str, csv_encoding: str = \"ISO-8859-1\") -> pd.core.frame.DataFrame:\n","  import os.path\n","  from os import path\n","  import pandas as pd\n","\n","  product_dataframe = pd.read_csv(path_of_csv, encoding = csv_encoding)\n","  return product_dataframe\n","\n","def sample_n_products_from_all_query_fields_in_dataframe(product_dataframe: pd.core.frame.DataFrame, number_of_products_per_query: int, randomly_sample_from_each_query: bool = False) -> pd.core.frame.DataFrame:\n","  from random import sample  \n","  list_of_queries_field = product_dataframe['query'].unique().tolist()\n","\n","  indexes_of_query_products = []\n","  for i in range(len(list_of_queries_field)):\n","    if isinstance(pd.Index(product_dataframe['query']).get_loc(list_of_queries_field[i]), int):\n","      indexes_of_query_products.append(pd.Index(product_dataframe['query']).get_loc(list_of_queries_field[i]))\n","    else:\n","      bool_array_indicating_dataframe_indexes_where_query_matches = pd.Index(product_dataframe['query']).get_loc(list_of_queries_field[i])\n","      list_of_indexes_for_particular_query = [i for i, x in enumerate(bool_array_indicating_dataframe_indexes_where_query_matches) if x]\n","      if randomly_sample_from_each_query:\n","        indexes_of_query_products.extend(sample(list_of_indexes_for_particular_query, k = number_of_products_per_query))\n","      else:\n","        indexes_of_query_products.extend(list_of_indexes_for_particular_query[0:number_of_products_per_query])\n","  \n","  num_rows_dataframe = product_dataframe.shape[0]\n","  total_indexes_set = set(range(0,num_rows_dataframe))\n","  indexes_of_query_products_set = set(indexes_of_query_products)\n","  indexes_of_nonquery_products = list(total_indexes_set.difference(indexes_of_query_products_set))\n","  \n","  queried_products_sample_dataframe = product_dataframe.copy()\n","  queried_products_sample_dataframe = queried_products_sample_dataframe.drop(indexes_of_nonquery_products)\n","  return queried_products_sample_dataframe \n","\n","\n","#preprocess data\n","class PreprocessData:\n","  \"\"\"\n","  A class used to find the orientation of the robot with respect to a lane over time.\n","  \"\"\"\n","  def __init__(self):\n","    \"\"\"\n","    Create a PreprocessData object\n","    Parameters:\n","      - \n","    \"\"\"\n","    from nltk import download\n","    from nltk import word_tokenize\n","    # Import and download stopwords from NLTK.\n","    from nltk.corpus import stopwords\n","    \n","    download('punkt')  # Download data for tokenizer.\n","    download('stopwords')  # Download stopwords list.\n","\n","    # Remove stopwords.\n","    self.stop_words = stopwords.words('english')\n","\n","  def make_text_lower_case(self, text: str) -> str:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    return text.lower()\n","\n","  def tokenize_string_to_list_of_separate_words(self, text_string: str) -> List[str]:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    from nltk import word_tokenize\n","    return word_tokenize(text_string) \n","\n","  def remove_stopwords_from_text(self, text_list: List[str]) -> List[str]:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    return [w for w in text_list if not w in self.stop_words]\n","\n","  def remove_numbers_and_punctuation_from_text(self, text_list: List[str]) -> List[str]:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    return [w for w in text_list if w.isalpha()]\n","\n","  def text_preprocessing_for_WMD(self, text: str) -> List[str]:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    text = self.make_text_lower_case(text)\n","    text_list = self.tokenize_string_to_list_of_separate_words(text)\n","    text_list = self.remove_stopwords_from_text(text_list)\n","    text_list = self.remove_numbers_and_punctuation_from_text(text_list)\n","    return text_list\n","\n","  def text_preprocessing_with_textrank_for_WMD(self, text: str) -> List[str]:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    text = self.make_text_lower_case(text)\n","    \n","    tr4w = TextRank4Keyword()\n","    tr4w.analyze(text, candidate_pos = ['NOUN', 'PROPN', 'VERB'], window_size=4, lower=False)\n","    text_list = tr4w.get_list_of_strings_of_the_top_textrank_keywords_in_order_of_textrank(10)\n","\n","    # text_list = self.tokenize_string_to_list_of_separate_words(text)\n","    text_list = self.remove_stopwords_from_text(text_list)\n","    text_list = self.remove_numbers_and_punctuation_from_text(text_list)\n","    return text_list\n","\n","  def create_new_field_with_preprocessed_text_in_dataframe(self, dataframe: pd.core.frame.DataFrame, field_to_preprocess: str, new_field_name: str) -> pd.core.frame.DataFrame:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    num_rows_dataframe = len(dataframe.index)\n","    preprocessed_text_list = [np.nan] * num_rows_dataframe\n","    empty_text = []\n","    for ind in dataframe.index:\n","      field_text = dataframe[field_to_preprocess][ind]\n","      #check if field is empty or nan\n","      if field_text == field_text:\n","        preprocessed_text = self.text_preprocessing_for_WMD(field_text)\n","        # preprocessed_text = self.text_preprocessing_with_textrank_for_WMD(field_text)\n","        if preprocessed_text != []:\n","          preprocessed_text_list[ind] = preprocessed_text\n","        else:\n","          preprocessed_text_list[ind] = empty_text\n","      else:\n","        preprocessed_text_list[ind] = empty_text\n","\n","    dataframe.insert(len(dataframe.columns), new_field_name, preprocessed_text_list, True)\n","    return dataframe\n","    \n","  def create_new_field_with_preprocessed_text_from_multiple_fields_in_dataframe(self, dataframe: pd.core.frame.DataFrame, list_of_fields_to_preprocess_in_order_of_appending: List[str], new_field_name: str) -> pd.core.frame.DataFrame:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    num_rows_dataframe = len(dataframe.index)\n","    preprocessed_text_list = [np.nan] * num_rows_dataframe\n","    empty_text = []\n","\n","    for ind in dataframe.index:\n","      list_of_field_text = []\n","      for i in range(len(list_of_fields_to_preprocess_in_order_of_appending)):\n","        if dataframe[list_of_fields_to_preprocess_in_order_of_appending[i]][ind] == dataframe[list_of_fields_to_preprocess_in_order_of_appending[i]][ind]:\n","          list_of_field_text.append(dataframe[list_of_fields_to_preprocess_in_order_of_appending[i]][ind])\n","      field_text = \" \".join(filter(None, list_of_field_text))\n","      #check if field is empty or nan\n","      if field_text == field_text:\n","        preprocessed_text = self.text_preprocessing_for_WMD(field_text)\n","        # preprocessed_text = self.text_preprocessing_with_textrank_for_WMD(field_text)\n","        if preprocessed_text != []:\n","          preprocessed_text_list[ind] = preprocessed_text\n","        else:\n","          preprocessed_text_list[ind] = empty_text\n","      else:\n","        preprocessed_text_list[ind] = empty_text\n","\n","    dataframe.insert(len(dataframe.columns), new_field_name, preprocessed_text_list, True)\n","    return dataframe     \n","\n","  def create_new_field_in_dataframe_with_preprocessed_text_from_list_of_existing_fields(self, dataframe: pd.core.frame.DataFrame, list_of_fields_to_preprocess_in_order_of_appending: List[str], new_field_name: str, preprocessing_function: Callable[[str],List[str]]) -> pd.core.frame.DataFrame:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    num_rows_dataframe = len(dataframe.index)\n","    preprocessed_text_list = [np.nan] * num_rows_dataframe\n","    empty_text = []\n","\n","    for ind in dataframe.index:\n","      list_of_field_text = []\n","      for i in range(len(list_of_fields_to_preprocess_in_order_of_appending)):\n","        if dataframe[list_of_fields_to_preprocess_in_order_of_appending[i]][ind] == dataframe[list_of_fields_to_preprocess_in_order_of_appending[i]][ind]:\n","          list_of_field_text.append(dataframe[list_of_fields_to_preprocess_in_order_of_appending[i]][ind])\n","      field_text = \" \".join(filter(None, list_of_field_text))\n","      #check if field is empty or nan\n","      if field_text == field_text:\n","        preprocessed_text = preprocessing_function(field_text)\n","        # preprocessed_text = self.text_preprocessing_for_WMD(field_text)\n","        # preprocessed_text = self.text_preprocessing_with_textrank_for_WMD(field_text)\n","        if preprocessed_text != []:\n","          preprocessed_text_list[ind] = preprocessed_text\n","        else:\n","          preprocessed_text_list[ind] = empty_text\n","      else:\n","        preprocessed_text_list[ind] = empty_text\n","\n","    dataframe.insert(len(dataframe.columns), new_field_name, preprocessed_text_list, True)\n","    return dataframe\n","\n","#find WMD similarity\n","class WordMoversDistanceSimilarity:\n","  \"\"\"\n","  A class used to find the orientation of the robot with respect to a lane over time.\n","  \"\"\"\n","  def __init__(self, keyed_vector_model: gensim.models.keyedvectors.Word2VecKeyedVectors, list_of_document_lists: List[List[str]], num_similar_queries: int):\n","    \"\"\"\n","    Create a PreprocessData object\n","    Parameters:\n","      - \n","    \"\"\"\n","    self.model = keyed_vector_model\n","    self.wmd_corpus = list_of_document_lists\n","    self.num_similar_queries = num_similar_queries\n","    self.preprocess_data_object = PreprocessData()\n","    self.similarityMatrix = 0\n","    self.similar_document_tuple_of_indexes_and_similarity_scores = 0\n","\n","  def create_similarity_matrix(self):\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    from gensim.similarities import WmdSimilarity\n","    self.similarityMatrix = WmdSimilarity(self.wmd_corpus, self.model, self.num_similar_queries)\n","\n","  def find_similar_document_index_and_similarity(self, query_text: str) -> List[tuple]:\n","    query_text = preprocess(source_doc)\n","    self.similar_document_tuple_of_indexes_and_similarity_scores = instance[query]  # A query is simply a \"look-up\" in the similarity class.\n","    return self.similar_document_tuple_of_indexes_and_similarity_scores\n","\n","class PrintingResultAndCalculateLoss:\n","  \"\"\"\n","  A class used to find the orientation of the robot with respect to a lane over time.\n","  \"\"\"\n","  def __init__(self, similar_document_tuple_of_indexes_and_similarity_scores: int, data_frame: pd.core.frame.DataFrame):\n","    \"\"\"\n","    Create a PreprocessData object\n","    Parameters:\n","      - \n","    \"\"\"\n","    self.similar_document_tuple_of_indexes_and_similarity_scores = similar_document_tuple_of_indexes_and_similarity_scores\n","    self.data_frame = data_frame\n","\n","  def calculate_accuracy_of_similarity_prediction_for_a_query_text_from_actual_(index_of_query_product, score_cutoff):\n","    \"\"\"\n","    Calculate accuracy for a given single similar products query. This query text must be the actual data from the table, so that it will have a \n","    label \"query\" field and \"relevance\" field.\n","\n","    if index_of_query_product \"relevance\" field < score_cutoff: #we dont want to use query items in which the similarity to the search query was low.\n","      return accuracy = None\n","    else: \n","      for every index in similar_document_tuple_of_indexes_and_similarity_scores:\n","        number_of_correct_similar_products += (index_of_query_product \"query\" field == similar_index \"query\" field)*(similar_index \"relevance\" field >= score_cutoff)\n","    accuracy = number_of_correct_similar_products/num_similar_queries\n","\n","    Parameters:\n","      - \n","    \"\"\"\n","  #in order to be classified correctly, the similar product has to be in same search query as target category\n","  #and has to have a relavance score >= score cutoff.\n","  #I know this is flawed because there might be a product in the wrong search query but is still similar to the target product.\n","  #but overall, im hoping this case to be less\n","\n","  #\n","    num_similar_queries = len(index_of_query_product)\n","\n","    search_query = df['query'][sims[i][0]]\n","    for i in range(num_similar_queries):\n","      index_of_\n","\n","def calculate_output_dataframe_accuracy_and_write_in_dataframe(output_dataframe, relevance_threshold):\n","  numRows = output_dataframe[\"searched_item_unit_id\"].shape[0]\n","  numMatchingCategory = 0\n","  numNonMatchingCategory = 0\n","  # ipdb.set_trace()\n","  if output_dataframe.iloc[0][\"searched_item_relevance\"] < relevance_threshold:\n","    accuracy, numMatchingCategory, numNonMatchingCategory = (-1,-1,-1)\n","    return accuracy, numMatchingCategory, numNonMatchingCategory\n","  for i in range(numRows):  \n","    if output_dataframe.iloc[0][\"searched_item_query\"] == output_dataframe.iloc[i][\"similar_item_query\"]:\n","      numMatchingCategory += 1\n","    else:\n","      numNonMatchingCategory +=1\n","  # print(\"numMatchingCategory = \", numMatchingCategory)\n","  # print(\"numNonMatchingCategory = \", numNonMatchingCategory)\n","  ##ipdb.set_trace()\n","  accuracy = numMatchingCategory/numRows\n","  # output_dataframe[\"WMDAccuracy\"][0] = accuracy\n","  return accuracy, numMatchingCategory, numNonMatchingCategory\n","\n"],"execution_count":3,"outputs":[]}]}