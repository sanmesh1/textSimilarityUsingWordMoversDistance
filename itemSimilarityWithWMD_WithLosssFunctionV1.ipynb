{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"itemSimilarityWithWMD_WithLosssFunctionV1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"cO7gZlDbXFdo","colab_type":"text"},"source":["Terminal"]},{"cell_type":"code","metadata":{"id":"V0rJqGFG3bUh","colab_type":"code","outputId":"f302a18b-c4fd-4cf5-cfa6-43ebcf114ed3","executionInfo":{"status":"ok","timestamp":1592220203461,"user_tz":240,"elapsed":1526,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# %cd data\n","# !pwd\n","# !gunzip GoogleNews-vectors-negative300.bin\n","# %cd ..\n","# url_to_word_embedding_zip = 'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz'\n","# import os.path\n","# from os import path\n","# url_with_extention_removed = os.path.splitext(url_to_word_embedding_zip)[0]\n","# unzipped_file_name = os.path.basename(url_with_extention_removed)\n","# desired_path_of_unzipped_word_embedding = \"data/\" + unzipped_file_name+'.gz'\n","# print(url_to_word_embedding_zip)\n","# !gunzip $url_to_word_embedding_zip $desired_path_of_unzipped_word_embedding\n","\n","# !pwd\n","# !ls \"data/\"\n","# !wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\" -P \"data/\"\n","# !ls\n","# !gunzip \"data/GoogleNews-vectors-negative300.bin\"\n","# !ls \"data/\"\n","\n","!ls \"data/\""],"execution_count":1,"outputs":[{"output_type":"stream","text":["ls: cannot access 'data/': No such file or directory\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"92XR35HoXIhl","colab_type":"text"},"source":["INPUTS"]},{"cell_type":"code","metadata":{"id":"yMIIJM8XXIXm","colab_type":"code","colab":{}},"source":["#repoPath should start with the path \n","#'/content/gdrive/My Drive/' and go to the folder where the git repo is, and should end with the name of the repo:\n","repoPath = '/content/gdrive/My Drive/TestingGitCloneIntoGoogleDrive/textSimilarityUsingWordMoversDistance'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Qnz6ppmYEv2","colab_type":"text"},"source":["Mount google drive"]},{"cell_type":"code","metadata":{"id":"8pCZgWJlXzbx","colab_type":"code","outputId":"082b02c8-e148-49c8-996f-c618493011b1","executionInfo":{"status":"ok","timestamp":1592223502480,"user_tz":240,"elapsed":23229,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","import os\n","os.chdir(repoPath)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KLcpc588YGsi","colab_type":"text"},"source":["Pip Install packages, and import general packages"]},{"cell_type":"code","metadata":{"id":"ZdGKiaG8XHQ5","colab_type":"code","outputId":"569d91da-b566-432e-b637-95febe3b3630","executionInfo":{"status":"ok","timestamp":1592223474510,"user_tz":240,"elapsed":5796,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#pip installs\n","! pip install ipdb -q\n","import ipdb \n","#place the line of code below from whichever line you want to start debugging\n","#type n for step over and s for step into\n","# ipdb.set_trace()\n","\n","from time import time\n","import gensim\n","import pandas as pd\n","from typing import List, Tuple\n","import numpy as np"],"execution_count":4,"outputs":[{"output_type":"stream","text":["  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gb6hQQY6tSkF","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5-2lqDS31lFS","colab_type":"code","colab":{}},"source":["\n","from collections import OrderedDict\n","import numpy as np\n","import spacy\n","from spacy.lang.en.stop_words import STOP_WORDS\n","\n","nlp = spacy.load('en_core_web_sm')\n","\n","class TextRank4Keyword():\n","    \"\"\"Extract keywords from text\"\"\"\n","    \n","    def __init__(self):\n","        self.d = 0.85 # damping coefficient, usually is .85\n","        self.min_diff = 1e-5 # convergence threshold\n","        self.steps = 10 # iteration steps\n","        self.node_weight = None # save keywords and its weight\n","\n","    \n","    def set_stopwords(self, stopwords):  \n","        \"\"\"Set stop words\"\"\"\n","        for word in STOP_WORDS.union(set(stopwords)):\n","            lexeme = nlp.vocab[word]\n","            lexeme.is_stop = True\n","    \n","    def sentence_segment(self, doc, candidate_pos, lower):\n","        \"\"\"Store those words only in cadidate_pos\"\"\"\n","        sentences = []\n","        for sent in doc.sents:\n","            selected_words = []\n","            for token in sent:\n","                # Store words only with cadidate POS tag\n","                if token.pos_ in candidate_pos and token.is_stop is False:\n","                    if lower is True:\n","                        selected_words.append(token.text.lower())\n","                    else:\n","                        selected_words.append(token.text)\n","            sentences.append(selected_words)\n","        return sentences\n","        \n","    def get_vocab(self, sentences):\n","        \"\"\"Get all tokens\"\"\"\n","        vocab = OrderedDict()\n","        i = 0\n","        for sentence in sentences:\n","            for word in sentence:\n","                if word not in vocab:\n","                    vocab[word] = i\n","                    i += 1\n","        return vocab\n","    \n","    def get_token_pairs(self, window_size, sentences):\n","        \"\"\"Build token_pairs from windows in sentences\"\"\"\n","        token_pairs = list()\n","        for sentence in sentences:\n","            for i, word in enumerate(sentence):\n","                for j in range(i+1, i+window_size):\n","                    if j >= len(sentence):\n","                        break\n","                    pair = (word, sentence[j])\n","                    if pair not in token_pairs:\n","                        token_pairs.append(pair)\n","        return token_pairs\n","        \n","    def symmetrize(self, a):\n","        return a + a.T - np.diag(a.diagonal())\n","    \n","    def get_matrix(self, vocab, token_pairs):\n","        \"\"\"Get normalized matrix\"\"\"\n","        # Build matrix\n","        vocab_size = len(vocab)\n","        g = np.zeros((vocab_size, vocab_size), dtype='float')\n","        for word1, word2 in token_pairs:\n","            i, j = vocab[word1], vocab[word2]\n","            g[i][j] = 1\n","            \n","        # Get Symmeric matrix\n","        g = self.symmetrize(g)\n","        \n","        # Normalize matrix by column\n","        norm = np.sum(g, axis=0)\n","        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n","        \n","        return g_norm\n","\n","    \n","    def get_keywords(self, number=10):\n","        \"\"\"Print top number keywords\"\"\"\n","        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n","        for i, (key, value) in enumerate(node_weight.items()):\n","            print(key + ' - ' + str(value))\n","            if i > number:\n","                break\n","\n","    def get_list_of_strings_of_the_top_textrank_keywords_in_order_of_textrank(self, number=10):\n","      list_of_top_keywords = []\n","\n","      \"\"\"Print top number keywords\"\"\"\n","      node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n","      for i, (key, value) in enumerate(node_weight.items()):\n","          #print(key + ' - ' + str(value))\n","          list_of_top_keywords.append(key.lower())\n","          if i > number:\n","            break\n","      return list_of_top_keywords       \n","        \n","    def analyze(self, text, \n","                candidate_pos=['NOUN', 'PROPN'], \n","                window_size=4, lower=False, stopwords=list()):\n","        \"\"\"Main function to analyze text\"\"\"\n","        \n","        # Set stop words\n","        self.set_stopwords(stopwords)\n","        \n","        # Pare text by spaCy\n","        doc = nlp(text)\n","        \n","        # Filter sentences\n","        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n","        \n","        # Build vocabulary\n","        vocab = self.get_vocab(sentences)\n","        \n","        # Get token_pairs from windows\n","        token_pairs = self.get_token_pairs(window_size, sentences)\n","        \n","        # Get normalized matrix\n","        g = self.get_matrix(vocab, token_pairs)\n","        \n","        # Initionlization for weight(pagerank value)\n","        pr = np.array([1] * len(vocab))\n","        \n","        # Iteration\n","        previous_pr = 0\n","        for epoch in range(self.steps):\n","            pr = (1-self.d) + self.d * np.dot(g, pr)\n","            if abs(previous_pr - sum(pr))  < self.min_diff:\n","                break\n","            else:\n","                previous_pr = sum(pr)\n","\n","        # Get weight for each node\n","        node_weight = dict()\n","        for word, index in vocab.items():\n","            node_weight[word] = pr[index]\n","        \n","        self.node_weight = node_weight"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OHh8aE7vtgCU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"outputId":"b002a48f-c4d9-423e-81a9-6c3374aa4cd9","executionInfo":{"status":"ok","timestamp":1592186509285,"user_tz":240,"elapsed":362,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}}},"source":["text = '''\n","Sony PlayStation 4 (PS4) (Latest Model)- 500 GB Jet Black Console The PlayStation 4 system opens the door to an incredible journey through immersive new gaming worlds and a deeply connected gaming community. Step into living, breathing worlds where you are hero of your epic journey. Explore gritty urban environments, vast galactic landscapes, and fantastic historical settings brought to life on an epic scale, without limits. With an astounding launch lineup and over 180 games in development the PS4 system offers more top-tier blockbusters and inventive indie hits than any other next-gen console. The PS4 system is developer inspired, gamer focused. The PS4 system learns how you play and intuitively curates the content you use most often. Fire it up, and your PS4 system points the way to new, amazing experiences you can jump into alone or with friends. Create your own legend using a sophisticated, intuitive network built for gamers. Broadcast your gameplay live and direct to the world, complete with your commentary. Or immortalize your most epic moments and share at the press of a button. Access the best in music, movies, sports and television. PS4 system doesn t require a membership fee to access your digital entertainment subscriptions. You get the full spectrum of entertainment that matters to you on the PS4 system. PlayStation 4: The Best Place to Play The PlayStation 4 system provides dynamic, connected gaming, powerful graphics and speed, intelligent personalization, deeply integrated social capabilities, and innovative second-screen features. Combining unparalleled content, immersive gaming experiences, all of your favorite digital entertainment apps, and PlayStation exclusives, the PS4 system focuses on the gamers.Gamer Focused, Developer InspiredThe PS4 system focuses on the gamer, ensuring that the very best games and the most immersive experiences are possible on the platform.<br>Read more about the PS4 on ebay guides.</br>\n","'''\n","tr4w = TextRank4Keyword()\n","tr4w.analyze(text, candidate_pos = ['NOUN', 'PROPN'], window_size=4, lower=False)\n","tr4w.get_keywords(10)"],"execution_count":25,"outputs":[{"output_type":"stream","text":["system - 3.9298789569107666\n","PS4 - 3.2873652208751394\n","gaming - 2.1534122414704107\n","PlayStation - 1.777032178296869\n","entertainment - 1.6504670134534094\n","experiences - 1.5937993202166947\n","games - 1.5757554934271645\n","journey - 1.346523341839697\n","worlds - 1.2433484842414528\n","gamers - 1.2261288426700112\n","settings - 1.1900458333333335\n","life - 1.1900458333333335\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FWii_7GVXHEn","colab_type":"code","colab":{}},"source":["#download pretrained word embedding model and load it\n","##download a file\n","##unzip a file\n","def download_and_unzip_pretrained_word_embedding(url_to_word_embedding_zip: str, download_destination_folder_with_slash_at_end: str) -> Tuple[bool, str]:\n","  #download if unzipped file exists already\n","  #if not, unzip and check if file exists after unzipping\n","  #if file exists, return 1 for success\n","  import os.path\n","  from os import path\n","  zipped_file_name = os.path.basename(url_to_word_embedding_zip)\n","  unzipped_file_name = os.path.splitext(zipped_file_name)[0]\n","\n","  path_of_zipped_word_embedding = download_destination_folder_with_slash_at_end + zipped_file_name\n","  desired_path_of_unzipped_word_embedding = download_destination_folder_with_slash_at_end + unzipped_file_name\n","  zip_file_extension = os.path.splitext(url_to_word_embedding_zip)[1][1:]\n","\n","  if path.exists(desired_path_of_unzipped_word_embedding) == False:\n","    !wget -c $url_to_word_embedding_zip -P $download_destination_folder_with_slash_at_end\n","    if zip_file_extension == 'gz':\n","      !gunzip $path_of_zipped_word_embedding\n","    elif zip_file_extension == 'zip':\n","      print(\"zip_file_extension\", \" not implemented yet\")\n","      return (False, None)\n","    else:\n","      print(\"This function doesnt handle zipped files with extension: \", zip_file_extension)\n","      return (False, None)\n","  else:\n","    print(\"datasetAlreadyExists\")\n","\n","  if path.exists(desired_path_of_unzipped_word_embedding) == True:\n","    successful_in_creating_word_embedding = True\n","  else:\n","    successful_in_creating_word_embedding = False\n","  return (successful_in_creating_word_embedding, desired_path_of_unzipped_word_embedding)\n","\n","##load word embedding\n","def load_word_embedding_into_KeyedVectors_model(word_embedding_file_path: str) -> gensim.models.keyedvectors.Word2VecKeyedVectors:\n","  from gensim.models.keyedvectors import KeyedVectors\n","  model = KeyedVectors.load_word2vec_format(word_embedding_file_path, binary=True)\n","  return model\n","\n","#load dataset\n","#extract desired features from dataset\n","def load_dataset_into_pandas(url_of_dataset: str, tuple_of_starting_and_ending_exclusive_index_of_sample: tuple) -> pd.core.frame.DataFrame:\n","  #Create pandas dataframe on dataset\n","  import pandas as pd\n","  #https://data.world/crowdflower/ecommerce-search-relevance\n","  df = pd.read_csv(url_of_dataset, encoding = \"ISO-8859-1\")\n","  start,end = tuple_of_starting_and_ending_exclusive_index_of_sample\n","  df = df.iloc[start:end]\n","  return df\n","\n","\n","#preprocess data\n","class PreprocessData:\n","  \"\"\"\n","  A class used to find the orientation of the robot with respect to a lane over time.\n","  \"\"\"\n","  def __init__(self):\n","    \"\"\"\n","    Create a PreprocessData object\n","    Parameters:\n","      - \n","    \"\"\"\n","    from nltk import download\n","    from nltk import word_tokenize\n","    # Import and download stopwords from NLTK.\n","    from nltk.corpus import stopwords\n","    \n","    download('punkt')  # Download data for tokenizer.\n","    download('stopwords')  # Download stopwords list.\n","\n","    # Remove stopwords.\n","    self.stop_words = stopwords.words('english')\n","\n","  def make_text_lower_case(self, text: str) -> str:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    return text.lower()\n","\n","  def tokenize_string_to_list_of_separate_words(self, text_string: str) -> List[str]:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    from nltk import word_tokenize\n","    return word_tokenize(text_string) \n","\n","  def remove_stopwords_from_text(self, text_list: List[str]) -> List[str]:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    return [w for w in text_list if not w in self.stop_words]\n","\n","  def remove_numbers_and_punctuation_from_text(self, text_list: List[str]) -> List[str]:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    return [w for w in text_list if w.isalpha()]\n","\n","  def text_preprocessing_for_WMD(self, text: str) -> List[str]:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    text = self.make_text_lower_case(text)\n","    text_list = self.tokenize_string_to_list_of_separate_words(text)\n","    text_list = self.remove_stopwords_from_text(text_list)\n","    text_list = self.remove_numbers_and_punctuation_from_text(text_list)\n","    return text_list\n","\n","  def text_preprocessing_with_textrank_for_WMD(self, text: str) -> List[str]:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    text = self.make_text_lower_case(text)\n","    \n","    tr4w = TextRank4Keyword()\n","    tr4w.analyze(text, candidate_pos = ['NOUN', 'PROPN', 'VERB'], window_size=4, lower=False)\n","    text_list = tr4w.get_list_of_strings_of_the_top_textrank_keywords_in_order_of_textrank(10)\n","\n","    # text_list = self.tokenize_string_to_list_of_separate_words(text)\n","    text_list = self.remove_stopwords_from_text(text_list)\n","    text_list = self.remove_numbers_and_punctuation_from_text(text_list)\n","    return text_list\n","\n","  def create_new_field_with_preprocessed_text_in_dataframe(self, dataframe: pd.core.frame.DataFrame, field_to_preprocess: str, new_field_name: str) -> pd.core.frame.DataFrame:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    num_rows_dataframe = len(dataframe.index)\n","    preprocessed_text_list = [np.nan] * num_rows_dataframe\n","    empty_text = []\n","    for ind in dataframe.index:\n","      field_text = dataframe[field_to_preprocess][ind]\n","      #check if field is empty or nan\n","      if field_text == field_text:\n","        preprocessed_text = self.text_preprocessing_for_WMD(field_text)\n","        # preprocessed_text = self.text_preprocessing_with_textrank_for_WMD(field_text)\n","        if preprocessed_text != []:\n","          preprocessed_text_list[ind] = preprocessed_text\n","        else:\n","          preprocessed_text_list[ind] = empty_text\n","      else:\n","        preprocessed_text_list[ind] = empty_text\n","\n","    dataframe.insert(len(dataframe.columns), new_field_name, preprocessed_text_list, True)\n","    return dataframe\n","    \n","  def create_new_field_with_preprocessed_text_from_multiple_fields_in_dataframe(self, dataframe: pd.core.frame.DataFrame, fields_to_preprocess_in_order_of_appending: List[str], new_field_name: str) -> pd.core.frame.DataFrame:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    num_rows_dataframe = len(dataframe.index)\n","    preprocessed_text_list = [np.nan] * num_rows_dataframe\n","    empty_text = []\n","\n","    for ind in dataframe.index:\n","      list_of_field_text = []\n","      for i in range(len(fields_to_preprocess_in_order_of_appending)):\n","        if dataframe[fields_to_preprocess_in_order_of_appending[i]][ind] == dataframe[fields_to_preprocess_in_order_of_appending[i]][ind]:\n","          list_of_field_text.append(dataframe[fields_to_preprocess_in_order_of_appending[i]][ind])\n","      field_text = \" \".join(filter(None, list_of_field_text))\n","      #check if field is empty or nan\n","      if field_text == field_text:\n","        preprocessed_text = self.text_preprocessing_for_WMD(field_text)\n","        # preprocessed_text = self.text_preprocessing_with_textrank_for_WMD(field_text)\n","        if preprocessed_text != []:\n","          preprocessed_text_list[ind] = preprocessed_text\n","        else:\n","          preprocessed_text_list[ind] = empty_text\n","      else:\n","        preprocessed_text_list[ind] = empty_text\n","\n","    dataframe.insert(len(dataframe.columns), new_field_name, preprocessed_text_list, True)\n","    return dataframe     \n","\n","#find WMD similarity\n","class WordMoversDistanceSimilarity:\n","  \"\"\"\n","  A class used to find the orientation of the robot with respect to a lane over time.\n","  \"\"\"\n","  def __init__(self, keyed_vector_model: gensim.models.keyedvectors.Word2VecKeyedVectors, list_of_document_lists: List[List[str]], num_similar_queries: int):\n","    \"\"\"\n","    Create a PreprocessData object\n","    Parameters:\n","      - \n","    \"\"\"\n","    self.model = keyed_vector_model\n","    self.wmd_corpus = list_of_document_lists\n","    self.num_similar_queries = num_similar_queries\n","    self.preprocess_data_object = PreprocessData()\n","    self.similarityMatrix = 0\n","    self.similar_document_tuple_of_indexes_and_similarity_scores = 0\n","\n","  def create_similarity_matrix(self):\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    from gensim.similarities import WmdSimilarity\n","    self.similarityMatrix = WmdSimilarity(self.wmd_corpus, self.model, self.num_similar_queries)\n","\n","  def find_similar_document_index_and_similarity(self, query_text: str) -> List[tuple]:\n","    query_text = preprocess(source_doc)\n","    self.similar_document_tuple_of_indexes_and_similarity_scores = instance[query]  # A query is simply a \"look-up\" in the similarity class.\n","    return self.similar_document_tuple_of_indexes_and_similarity_scores\n","\n","class PrintingResultAndCalculateLoss:\n","  \"\"\"\n","  A class used to find the orientation of the robot with respect to a lane over time.\n","  \"\"\"\n","  def __init__(self, similar_document_tuple_of_indexes_and_similarity_scores: int, data_frame: pd.core.frame.DataFrame):\n","    \"\"\"\n","    Create a PreprocessData object\n","    Parameters:\n","      - \n","    \"\"\"\n","    self.similar_document_tuple_of_indexes_and_similarity_scores = similar_document_tuple_of_indexes_and_similarity_scores\n","    self.data_frame = data_frame\n","\n","  def calculate_accuracy_of_similarity_prediction_for_a_query_text_from_actual_(index_of_query_product, score_cutoff):\n","    \"\"\"\n","    Calculate accuracy for a given single similar products query. This query text must be the actual data from the table, so that it will have a \n","    label \"query\" field and \"relevance\" field.\n","\n","    if index_of_query_product \"relevance\" field < score_cutoff: #we dont want to use query items in which the similarity to the search query was low.\n","      return accuracy = None\n","    else: \n","      for every index in similar_document_tuple_of_indexes_and_similarity_scores:\n","        number_of_correct_similar_products += (index_of_query_product \"query\" field == similar_index \"query\" field)*(similar_index \"relevance\" field >= score_cutoff)\n","    accuracy = number_of_correct_similar_products/num_similar_queries\n","\n","    Parameters:\n","      - \n","    \"\"\"\n","  #in order to be classified correctly, the similar product has to be in same search query as target category\n","  #and has to have a relavance score >= score cutoff.\n","  #I know this is flawed because there might be a product in the wrong search query but is still similar to the target product.\n","  #but overall, im hoping this case to be less\n","\n","  #\n","    num_similar_queries = len(index_of_query_product)\n","\n","    search_query = df['query'][sims[i][0]]\n","    for i in range(num_similar_queries):\n","      index_of_\n","\n","def calculate_output_dataframe_accuracy_and_write_in_dataframe(output_dataframe, relevance_threshold):\n","  numRows = output_dataframe[\"similarDescription\"].shape[0]\n","  numMatchingCategory = 0\n","  numNonMatchingCategory = 0\n","  if output_dataframe[\"targetRelevanceToQuery\"][0] < relevance_threshold:\n","    accuracy, numMatchingCategory, numNonMatchingCategory = (-1,-1,-1)\n","    return accuracy, numMatchingCategory, numNonMatchingCategory\n","  for i in range(numRows):  \n","    if output_dataframe[\"targetSearchQuery\"][0] == output_dataframe[\"similarSearchQuery\"][i]:\n","      numMatchingCategory += 1\n","    else:\n","      numNonMatchingCategory +=1\n","  print(\"numMatchingCategory = \", numMatchingCategory)\n","  print(\"numNonMatchingCategory = \", numNonMatchingCategory)\n","  ##ipdb.set_trace()\n","  accuracy = numMatchingCategory/numRows\n","  output_dataframe[\"WMDAccuracy\"][0] = accuracy\n","  return accuracy, numMatchingCategory, numNonMatchingCategory\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ybq5Fed_Lim2","colab_type":"code","outputId":"9a81b080-b18e-42c2-e297-406e6ce26079","executionInfo":{"status":"ok","timestamp":1592191464157,"user_tz":240,"elapsed":64351,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["\n","############################\n","##download model and load model\n","##Inputs\n","url_to_word_embedding_zip = \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n","download_destination_folder_with_slash_at_end = \"data/\"\n","\n","download_success, desired_path_of_unzipped_word_embedding = download_and_unzip_pretrained_word_embedding(url_to_word_embedding_zip, download_destination_folder_with_slash_at_end)\n","word_embedding_file_path = desired_path_of_unzipped_word_embedding\n","model = load_word_embedding_into_KeyedVectors_model(word_embedding_file_path)\n","\n"],"execution_count":31,"outputs":[{"output_type":"stream","text":["datasetAlreadyExists\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Az_10UKtRVRs","colab_type":"code","outputId":"b25b884b-fafb-4627-a4bc-d0f3bcb09bb2","executionInfo":{"status":"ok","timestamp":1592201641042,"user_tz":240,"elapsed":2628,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["\n","#setup pretrained model\n","\n","# unitIdToGetIndexesOf = [711158459, 711158484]\n","\n","############################\n","#setup dataset\n","##Inputs\n","url_of_dataset = 'https://query.data.world/s/iccclhgzbyedtv54p5lresglnun3qz'\n","tuple_of_starting_and_ending_exclusive_index_of_sample = (0,500)\n","\n","dataset_frame = load_dataset_into_pandas(url_of_dataset, tuple_of_starting_and_ending_exclusive_index_of_sample)\n","list_of_queries_field = dataset_frame['query'].unique().tolist()\n","\n","indexesOfTargetItems = []\n","for i in range(len(list_of_queries_field)):\n","  # unitId = list_of_queries_field[i]\n","  # indexesOfTargetItems.append(pd.Index(dataset_frame['_unit_id']).get_loc(unitId))\n","  # print(i)\n","  # print(pd.Index(dataset_frame['query']).get_loc(list_of_queries_field[i]))\n","  if isinstance(pd.Index(dataset_frame['query']).get_loc(list_of_queries_field[i]), int):\n","    indexesOfTargetItems.append(pd.Index(dataset_frame['query']).get_loc(list_of_queries_field[i]))\n","  else:\n","    indexesOfTargetItems.append(pd.Index(dataset_frame['query']).get_loc(list_of_queries_field[i]).argmax())\n","print(list_of_queries_field)\n","# ipdb.set_trace()"],"execution_count":45,"outputs":[{"output_type":"stream","text":["['playstation 4', 'eye cream', 'routers', 'shaver panasonic', 'coffee grinder', 'high chairs', 'face cream', 'Brett Favre NY Titans jersey blue', 'aroma diffuser', 'Bluray Hobbit extended', 'white plain dinner set', 'double stroller', 'Vanilla Scented Perfumes', 'wireless mouse', 'spiderman', 'bedspreads', 'kitchen rugs', 'LED monitor', 'PS2 controller USB', 'extenal hardisk 500 gb', 'laptop lenovo']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tY8eF67X4aiQ","colab_type":"code","outputId":"2bb723c5-e63e-455b-f946-a25a9c7c12d3","executionInfo":{"status":"ok","timestamp":1592202277399,"user_tz":240,"elapsed":616421,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["\n","############################\n","for source_doc_index in indexesOfTargetItems:\n","  url_of_dataset = 'https://query.data.world/s/iccclhgzbyedtv54p5lresglnun3qz'\n","  tuple_of_starting_and_ending_exclusive_index_of_sample = (0,500)\n","\n","  dataset_frame = load_dataset_into_pandas(url_of_dataset, tuple_of_starting_and_ending_exclusive_index_of_sample)\n","\n","  ############################\n","  ##preprocess text\n","  ##create a new field that has\n","  ##Inputs\n","  # dataframe = dataset_frame\n","  # field_to_preprocess = 'product_description'\n","  # new_field_name = 'product_description_preprocessed'\n","\n","  # preprocess_data_object = PreprocessData()\n","  # #create_new_field_with_preprocessed_text_in_dataframe(self, dataframe: pd.core.frame.DataFrame, field_to_preprocess: str, new_field_name: str) -> pd.core.frame.DataFrame:\n","  # dataframe_modified = preprocess_data_object.create_new_field_with_preprocessed_text_in_dataframe(dataframe, field_to_preprocess, new_field_name)\n","\n","  ############################\n","  #alternate preprocess text\n","  ##Inputs\n","  dataframe = dataset_frame\n","  # fields_to_preprocess_in_order_of_appending = ['product_title', 'product_description']\n","  fields_to_preprocess_in_order_of_appending = ['product_title']\n","  new_field_name = \",\".join(fields_to_preprocess_in_order_of_appending)+'_preprocessed'\n","\n","  print(\"new_field_name = \", new_field_name)\n","\n","  preprocess_data_object = PreprocessData()\n","  #create_new_field_with_preprocessed_text_in_dataframe(self, dataframe: pd.core.frame.DataFrame, field_to_preprocess: str, new_field_name: str) -> pd.core.frame.DataFrame:\n","  dataframe_modified = preprocess_data_object.create_new_field_with_preprocessed_text_from_multiple_fields_in_dataframe(dataframe, fields_to_preprocess_in_order_of_appending, new_field_name)\n","\n","  ############################\n","  #Create Word movers distance similarity matrix\n","  #Inputs\n","  num_best_var = 20\n","  # source_doc_index = 0\n","  print(\"source_doc_index = \", source_doc_index)\n","\n","  source_doc_preprocessed = dataframe_modified[new_field_name][source_doc_index]\n","  print(source_doc_preprocessed)\n","\n","  from gensim.similarities import WmdSimilarity\n","  wmd_corpus = dataframe_modified[new_field_name].tolist()\n","  wmd_object_for_description = WmdSimilarity(wmd_corpus, model, num_best = num_best_var)\n","\n","  similar_item_indexes_and_similarity = wmd_object_for_description[source_doc_preprocessed]\n","\n","  similar_product_index = []\n","  similar_product_similarities =[]\n","\n","  for i in range(num_best_var):\n","    print(i)\n","    similar_product_index.append(similar_item_indexes_and_similarity[i][0])\n","    similar_product_similarities.append(similar_item_indexes_and_similarity[i][1])\n","\n","  target_index = ['']*num_best_var\n","  target_title_list = ['']*num_best_var\n","  target_search_query_list = ['']*num_best_var\n","  target_search_relevance_to_query = ['']*num_best_var\n","  target_description_list = ['']*num_best_var\n","  target_description_preprocesed_list = ['']*num_best_var\n","  wmd_accuracy = ['']*num_best_var\n","\n","  target_index[0] = source_doc_index\n","  target_title_list[0] = dataframe_modified['product_title'][source_doc_index]\n","  target_search_query_list[0] = dataframe_modified['query'][source_doc_index]\n","  target_search_relevance_to_query[0] = dataframe_modified['relevance'][source_doc_index]\n","  target_description_list[0] = dataframe_modified['product_description'][source_doc_index]\n","  target_description_preprocesed_list[0] = dataframe_modified[new_field_name][source_doc_index]\n","\n","  data = {'targetIndex': target_index, \n","          'targetTitle': target_title_list, \n","          'targetSearchQuery': target_search_query_list,\n","          'targetRelevanceToQuery': target_search_relevance_to_query, \n","          'targetDescription': target_description_list,\n","          'target'+new_field_name: target_description_preprocesed_list,\n","          'similarIndex': similar_product_index,\n","          'similarTitle': dataframe_modified['product_title'][similar_product_index].tolist(),\n","          'similarSearchQuery': dataframe_modified['query'][similar_product_index].tolist(),\n","          'similarRelevanceToQuery': dataframe_modified['relevance'][similar_product_index].tolist(),\n","          'similarDescription': dataframe_modified['product_description'][similar_product_index].tolist(),\n","          'similar'+new_field_name: dataframe_modified[new_field_name][similar_product_index].tolist(),\n","          'WMDRank': list(range(0, num_best_var)),\n","          'WMDSimilarity': similar_product_similarities, \n","          'WMDAccuracy': wmd_accuracy} \n","  output_dataframe = pd.DataFrame(data)\n","  relevance_threshold = 2\n","  accuracy, numMatchingCategory, numNonMatchingCategory = calculate_output_dataframe_accuracy_and_write_in_dataframe(output_dataframe, relevance_threshold)\n","  output_csv_name = 'csv_logs_only_title_06_14_20/searchQuery_'+dataframe_modified['query'][source_doc_index]+'_numMatches_'+str(num_best_var)\n","\n","  output_dataframe.to_csv(output_csv_name+'.csv')\n","\n","\n","\n","# wmd_corpus_for_description = create_corpus_from_preprocessed_text_field(self, dataframe: pd.core.frame.DataFrame, ) -> List[List[str]]\n","#   dataframe['preprocessed_description'].toList()\n","# WMD_object_for_description = create_WMD_object(wmd_corpus, model, num_best)\n","# similar_item_indexes_and_similarity = get_similar_item_indexes_and_similarity()\n","\n","# print_out_similar_item_details_to_dataframe\n","\n","\n","# #Find the similarities for one item in the search query, and display the fields of title, relevance, description, wmd ranking, and wmd rating\n","# download_and_unzip_pretrained_word_embedding"],"execution_count":46,"outputs":[{"output_type":"stream","text":["new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  0\n","['sony', 'playstation', 'latest', 'model', 'gb', 'jet', 'black', 'console']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  20\n","numNonMatchingCategory =  0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:299: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"],"name":"stderr"},{"output_type":"stream","text":["new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  25\n","['new', 'clinique', 'repairwear', 'laser', 'focus', 'wrinkle', 'correcting', 'eye', 'cream']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  17\n","numNonMatchingCategory =  3\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  50\n","['router']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  20\n","numNonMatchingCategory =  0\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  75\n","['panasonic', 'cordless', 'rechargeable', 'men', 'electric', 'shaver']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  20\n","numNonMatchingCategory =  0\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  100\n","['baratza', 'solis', 'maestro', 'conical', 'burr', 'coffee', 'bean', 'grinder', 'works', 'great', 'nice', 'cond']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  20\n","numNonMatchingCategory =  0\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  125\n","['brand', 'new', 'graco', 'tablefit', 'highchair', 'three', 'reclining', 'options', 'finley']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  5\n","numNonMatchingCategory =  15\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  150\n","['luminesce', 'advance', 'night', 'repair', 'cream', 'jeunesse', 'face', 'mosturizer', 'ml', 'wrinkle']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  12\n","numNonMatchingCategory =  8\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  175\n","['men', 'nwt', 'vintage', 'reebok', 'brett', 'favre', 'new', 'york', 'jets', 'titans', 'jersey', 'size', 'xl']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  20\n","numNonMatchingCategory =  0\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  200\n","['color', 'ultrasonic', 'home', 'aroma', 'humidifier', 'air', 'diffuser', 'purifier', 'lonizer', 'atomizer']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  20\n","numNonMatchingCategory =  0\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  225\n","['unexpected', 'journey', 'extended', 'edition', 'dvd']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  20\n","numNonMatchingCategory =  0\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  250\n","['corelle', 'plain', 'white', 'dinner', 'plate', 'set']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  20\n","numNonMatchingCategory =  0\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  275\n","['graco', 'classic', 'connect', 'lx', 'double', 'stroller', 'fiji']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  20\n","numNonMatchingCategory =  0\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  300\n","['vanilla', 'tahitian', 'bottle', 'perfume', 'body', 'oil', 'strong', 'scented', 'fragrance']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  20\n","numNonMatchingCategory =  0\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  324\n","['red', 'wireless', 'optical', 'mouse', 'mice', 'usb', 'receiver', 'pc', 'laptop', 'macbook']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  20\n","numNonMatchingCategory =  0\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  349\n","['hand', 'marvel', 'legends', 'infinite', 'series', 'hobgoblin', 'baf', 'spiderman']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  20\n","numNonMatchingCategory =  0\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  374\n","['english', 'roses', 'bedding', 'quilt', 'bedspread', 'coverlet', 'piece', 'reversible', 'king', 'set']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  19\n","numNonMatchingCategory =  1\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  399\n","['modern', 'indoor', 'cushion', 'kitchen', 'rug', 'floor', 'mat', 'actual', 'x']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  20\n","numNonMatchingCategory =  0\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  424\n","['new', 'samsung', 'series', 'white', 'high', 'glossy', 'toc', 'led', 'hdmi', 'monitor']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  20\n","numNonMatchingCategory =  0\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  449\n","['psx', 'pc', 'usb', 'dual', 'controller', 'adapter', 'converter']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  20\n","numNonMatchingCategory =  0\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  474\n","['glyph', 'hard', 'drive']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  20\n","numNonMatchingCategory =  0\n","new_field_name =  product_title_preprocessed\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  499\n","['ibm', 'thinkpad', 'lenovo', 'laptop', 'ghz', 'gb', 'win', 'wifi']\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","numMatchingCategory =  1\n","numNonMatchingCategory =  19\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Wi4QIDV2bsOZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":717},"outputId":"20f4d2c8-3883-4597-be8e-22d78a60f46a","executionInfo":{"status":"ok","timestamp":1592223647756,"user_tz":240,"elapsed":4785,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}}},"source":["import numpy as np\n","import pandas as pd\n","import glob\n","import ipdb\n","def calculate_average_accuracy_from_csv_logs(folder_path_of_csv_logs: str) ->  Tuple[float, pd.core.frame.DataFrame]:\n","  #get paths of all the csv files in the folder_path_of_csv_logs directory\n","  all_files = glob.glob(folder_path_of_csv_logs + \"*.csv\")\n","\n","  #initialize lists to feed into output dataframe later\n","  search_query_list = []\n","  accuracy_list = []\n","\n","  #for each file:\n","  for filename in all_files:\n","    #load the file into pandas\n","    df = pd.read_csv(filename)\n","     #read the accuracy value, and save it into a pandas dataframe with fields: search query, accuracy\n","    accuracy_list.append(df['WMDAccuracy'][0])\n","    search_query_list.append(df['targetSearchQuery'][0])\n","  \n","  #calculate average of all the accuracies\n","  average_accuracy = sum(accuracy_list)/len(accuracy_list)\n","   \n","  #return dictionary of dataframe, as well as average accuracy\n","  search_query_accuracy_dataframe = {\n","      'searchQuery': search_query_list, \n","      'accuracy': accuracy_list, \n","      } \n","  output_dataframe = pd.DataFrame(search_query_accuracy_dataframe)\n","\n","  return average_accuracy, output_dataframe\n","#####################\n","folder_path_of_csv_logs = 'csv_logs_only_title_06_14_20/'\n","# ipdb.set_trace()\n","average_accuracy, output_dataframe = calculate_average_accuracy_from_csv_logs(folder_path_of_csv_logs)\n","print(\"average accuracy of \", folder_path_of_csv_logs, \" = \", average_accuracy)\n","output_dataframe"],"execution_count":9,"outputs":[{"output_type":"stream","text":["average accuracy of  csv_logs_only_title_06_14_20/  =  0.8904761904761904\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>searchQuery</th>\n","      <th>accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>playstation 4</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>eye cream</td>\n","      <td>0.85</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>routers</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>shaver panasonic</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>coffee grinder</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>high chairs</td>\n","      <td>0.25</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>face cream</td>\n","      <td>0.60</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Brett Favre NY Titans jersey blue</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>aroma diffuser</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Bluray Hobbit extended</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>white plain dinner set</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>double stroller</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>Vanilla Scented Perfumes</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>wireless mouse</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>spiderman</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>bedspreads</td>\n","      <td>0.95</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>kitchen rugs</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>LED monitor</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>PS2 controller USB</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>extenal hardisk 500 gb</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>laptop lenovo</td>\n","      <td>0.05</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                          searchQuery  accuracy\n","0                       playstation 4      1.00\n","1                           eye cream      0.85\n","2                             routers      1.00\n","3                    shaver panasonic      1.00\n","4                      coffee grinder      1.00\n","5                         high chairs      0.25\n","6                          face cream      0.60\n","7   Brett Favre NY Titans jersey blue      1.00\n","8                      aroma diffuser      1.00\n","9              Bluray Hobbit extended      1.00\n","10             white plain dinner set      1.00\n","11                    double stroller      1.00\n","12           Vanilla Scented Perfumes      1.00\n","13                     wireless mouse      1.00\n","14                          spiderman      1.00\n","15                         bedspreads      0.95\n","16                       kitchen rugs      1.00\n","17                        LED monitor      1.00\n","18                 PS2 controller USB      1.00\n","19             extenal hardisk 500 gb      1.00\n","20                      laptop lenovo      0.05"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"5fkjKIDq2cal","colab_type":"code","outputId":"a7f64216-36df-4828-fc81-bb024864122e","executionInfo":{"status":"ok","timestamp":1591875337158,"user_tz":240,"elapsed":3390,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import unittest\n","class MyTest(unittest.TestCase):\n","  @unittest.skip(\"demonstrating skipping\")\n","  def test_download_and_unzip_pretrained_word_embedding(self):\n","    url_to_word_embedding_zip = \"http://dumps.wikimedia.org/other/articlefeedback/aa_combined-20110321.csv.gz\"\n","    download_destination_path = \"data/\"\n","    path_to_word_embedding  = download_destination_path + \"aa_combined-20110321.csv\"\n","    #ipdb.set_trace()\n","    !rm $path_to_word_embedding\n","    success, desired_path_of_unzipped_word_embedding = download_and_unzip_pretrained_word_embedding(url_to_word_embedding_zip, download_destination_path)\n","    self.assertEqual(success, 1)\n","\n","  @unittest.skip(\"demonstrating skipping\")\n","  def test_load_word_embedding_into_KeyedVectors_model(self):\n","    url_to_word_embedding_zip = \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n","    download_destination_folder_with_slash_at_end = \"data/\"\n","\n","    download_success, desired_path_of_unzipped_word_embedding = download_and_unzip_pretrained_word_embedding(url_to_word_embedding_zip, download_destination_folder_with_slash_at_end)\n","    if download_success == 0:\n","      success = 0\n","    else:\n","      word_embedding_file_path = desired_path_of_unzipped_word_embedding\n","      model = load_word_embedding_into_KeyedVectors_model(word_embedding_file_path)\n","      \n","      if isinstance(model, gensim.models.keyedvectors.Word2VecKeyedVectors):\n","        success =1\n","      else:\n","        success = 0\n","    \n","    self.assertEqual(success, 1)\n","\n","  @unittest.skip(\"demonstrating skipping\")\n","  def test_load_dataset_into_pandas(self):\n","    url_of_dataset = 'https://query.data.world/s/iccclhgzbyedtv54p5lresglnun3qz'\n","    tuple_of_starting_and_ending_exclusive_index_of_sample = (0,10)\n","    dataset_frame = load_dataset_into_pandas(url_of_dataset, tuple_of_starting_and_ending_exclusive_index_of_sample)\n","    \n","    #pd.set_option('display.max_columns', None)\n","    #print(dataset_frame)\n","    \n","    self.assertEqual(isinstance(dataset_frame, pd.core.frame.DataFrame),1) \n","    self.assertEqual(dataset_frame.shape[0] == (tuple_of_starting_and_ending_exclusive_index_of_sample[1]-tuple_of_starting_and_ending_exclusive_index_of_sample[0]), 1)\n","  \n","  def test_create_new_field_with_preprocessed_text_in_dataframe(self):\n","    url_of_dataset = 'https://query.data.world/s/iccclhgzbyedtv54p5lresglnun3qz'\n","    tuple_of_starting_and_ending_exclusive_index_of_sample = (0,10)\n","    dataset_frame = load_dataset_into_pandas(url_of_dataset, tuple_of_starting_and_ending_exclusive_index_of_sample)\n","    \n","    dataframe = dataset_frame\n","    field_to_preprocess = 'product_description'\n","    new_field_name = 'product_description_preprocessed'\n","    \n","    preprocess_data_object = PreprocessData()\n","    #create_new_field_with_preprocessed_text_in_dataframe(self, dataframe: pd.core.frame.DataFrame, field_to_preprocess: str, new_field_name: str) -> pd.core.frame.DataFrame:\n","    dataframe_modified = preprocess_data_object.create_new_field_with_preprocessed_text_in_dataframe(dataframe, field_to_preprocess, new_field_name)\n","    #end of function\n","    #self.assertEqual(isinstance(dataframe_modified, pd.core.frame.DataFrame),1) \n","    pd.set_option('display.max_columns', None)\n","    print(dataframe_modified)\n","    \n","\n","if __name__ == '__main__':\n","  unittest.main(argv=['first-arg-is-ignored'], exit=False)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":[".sss"],"name":"stderr"},{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","    _unit_id  relevance  relevance:variance  \\\n","0  711158459       3.67               0.471   \n","1  711158460       4.00               0.000   \n","2  711158461       4.00               0.000   \n","3  711158462       3.67               0.471   \n","4  711158463       3.33               0.471   \n","5  711158464       3.20               0.748   \n","6  711158465       4.00               0.000   \n","7  711158466       4.00               0.000   \n","8  711158467       3.75               0.433   \n","9  711158468       2.33               0.471   \n","\n","                                       product_image  \\\n","0  http://thumbs2.ebaystatic.com/d/l225/m/mzvzEUI...   \n","1  http://thumbs3.ebaystatic.com/d/l225/m/mJNDmSy...   \n","2  http://thumbs4.ebaystatic.com/d/l225/m/m10NZXA...   \n","3  http://thumbs2.ebaystatic.com/d/l225/m/mZZXTmA...   \n","4  http://thumbs3.ebaystatic.com/d/l225/m/mzvzEUI...   \n","5  http://thumbs4.ebaystatic.com/d/l225/m/mzvzEUI...   \n","6  http://thumbs4.ebaystatic.com/d/l225/m/m9TQTiW...   \n","7  http://thumbs4.ebaystatic.com/d/l225/m/mTZYG5N...   \n","8  http://thumbs2.ebaystatic.com/d/l225/m/mX5Qphr...   \n","9  http://thumbs2.ebaystatic.com/d/l225/m/mGjN4Ir...   \n","\n","                                        product_link  \\\n","0  http://www.ebay.com/itm/Sony-PlayStation-4-PS4...   \n","1  http://www.ebay.com/itm/Sony-PlayStation-4-Lat...   \n","2  http://www.ebay.com/itm/Sony-PlayStation-4-PS4...   \n","3  http://www.ebay.com/itm/Sony-PlayStation-4-500...   \n","4  http://www.ebay.com/itm/Sony-PlayStation-4-PS4...   \n","5  http://www.ebay.com/itm/Sony-PlayStation-4-PS4...   \n","6  http://www.ebay.com/itm/BRAND-NEW-Sony-PlaySta...   \n","7  http://www.ebay.com/itm/Sony-PlayStation-4-500...   \n","8  http://www.ebay.com/itm/Sony-PlayStation-4-Lat...   \n","9  http://www.ebay.com/itm/Sony-PlayStation-4-Lat...   \n","\n","                   product_price  \\\n","0                       $329.98    \n","1                       $324.84    \n","2                       $324.83    \n","3                       $350.00    \n","4  $308.00\\nTrending at\\n$319.99   \n","5                       $310.00    \n","6                       $910.00    \n","7                       $299.99    \n","8                       $350.00    \n","9                       $469.97    \n","\n","                                       product_title          query  rank  \\\n","0  Sony PlayStation 4 (PS4) (Latest Model)- 500 G...  playstation 4     1   \n","1  Sony PlayStation 4 (Latest Model)- 500 GB Jet ...  playstation 4     2   \n","2    Sony PlayStation 4 PS4 500 GB Jet Black Console  playstation 4     3   \n","3  Sony - PlayStation 4 500GB The Last of Us Rema...  playstation 4     4   \n","4  Sony PlayStation 4 (PS4) (Latest Model)- 500 G...  playstation 4     5   \n","5  Sony PlayStation 4 (PS4) (Latest Model)- 500 G...  playstation 4     6   \n","6          BRAND NEW Sony PlayStation 4 BUNDLE 500gb  playstation 4     7   \n","7  Sony PlayStation 4 500GB, Dualshock Wireless C...  playstation 4     8   \n","8  Sony PlayStation 4 (Latest Model)- 500 GB Jet ...  playstation 4     9   \n","9  Sony PlayStation 4 (Latest Model)- 500 GB Jet ...  playstation 4    10   \n","\n","  source                                                url  \\\n","0   eBay  http://www.ebay.com/sch/i.html?_from=R40&_trks...   \n","1   eBay  http://www.ebay.com/sch/i.html?_from=R40&_trks...   \n","2   eBay  http://www.ebay.com/sch/i.html?_from=R40&_trks...   \n","3   eBay  http://www.ebay.com/sch/i.html?_from=R40&_trks...   \n","4   eBay  http://www.ebay.com/sch/i.html?_from=R40&_trks...   \n","5   eBay  http://www.ebay.com/sch/i.html?_from=R40&_trks...   \n","6   eBay  http://www.ebay.com/sch/i.html?_from=R40&_trks...   \n","7   eBay  http://www.ebay.com/sch/i.html?_from=R40&_trks...   \n","8   eBay  http://www.ebay.com/sch/i.html?_from=R40&_trks...   \n","9   eBay  http://www.ebay.com/sch/i.html?_from=R40&_trks...   \n","\n","                                 product_description  \\\n","0  The PlayStation 4 system opens the door to an ...   \n","1  The PlayStation 4 system opens the door to an ...   \n","2  The PlayStation 4 system opens the door to an ...   \n","3                                                NaN   \n","4  The PlayStation 4 system opens the door to an ...   \n","5  The PlayStation 4 system opens the door to an ...   \n","6                                                NaN   \n","7  The PlayStation 4 system opens the door to an ...   \n","8                                                NaN   \n","9  The PlayStation 4 system opens the door to an ...   \n","\n","                    product_description_preprocessed  \n","0  [playstation, system, opens, door, incredible,...  \n","1  [playstation, system, opens, door, incredible,...  \n","2  [playstation, system, opens, door, incredible,...  \n","3                                                NaN  \n","4  [playstation, system, opens, door, incredible,...  \n","5  [playstation, system, opens, door, incredible,...  \n","6                                                NaN  \n","7  [playstation, system, opens, door, incredible,...  \n","8                                                NaN  \n","9  [playstation, system, opens, door, incredible,...  \n"],"name":"stdout"},{"output_type":"stream","text":["\n","----------------------------------------------------------------------\n","Ran 4 tests in 2.871s\n","\n","OK (skipped=3)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"hCabShfxUEVw","colab_type":"code","outputId":"647288b5-9721-41f3-db7f-09a8cbc1e3bc","executionInfo":{"status":"ok","timestamp":1591912183149,"user_tz":240,"elapsed":882,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["dataframe_modified['product_title'][0]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Sony PlayStation 4 (PS4) (Latest Model)- 500 GB Jet Black Console'"]},"metadata":{"tags":[]},"execution_count":143}]},{"cell_type":"code","metadata":{"id":"1y_JaquUYS7G","colab_type":"code","outputId":"97611e88-fb87-4091-d2ea-975f1e5bef55","executionInfo":{"status":"ok","timestamp":1591915667083,"user_tz":240,"elapsed":477,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print(output_dataframe[\"similarDescription\"].shape)\n","numMatchingCategory"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(20,)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["11"]},"metadata":{"tags":[]},"execution_count":154}]},{"cell_type":"code","metadata":{"id":"NCgEBYErYS4q","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4be0wAJnYS2B","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qTDbHNDxzWNR","colab_type":"code","colab":{}},"source":["#INPUTS\n","\n","#repoPath should start with the path \n","#'/content/gdrive/My Drive/' and go to the folder where the git repo is, and should end with the name of the repo:\n","repoPath = '/content/gdrive/My Drive/TestingGitCloneIntoGoogleDrive/textSimilarityUsingWordMoversDistance'\n","userEmail = 'sanmesh1@gmail.com'\n","userName = 'sanmesh1'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dT11jlEGye8a","colab_type":"code","outputId":"a1a9dd4d-e3bd-4372-9ca1-9ce0e9bcbd67","executionInfo":{"status":"ok","timestamp":1591729566062,"user_tz":240,"elapsed":20393,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BxNLdrvrzAFu","colab_type":"code","outputId":"6149af78-372b-42d7-a410-7f51ab871f68","executionInfo":{"status":"ok","timestamp":1592182815511,"user_tz":240,"elapsed":5045,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["! pip install ipdb -q\n","import ipdb \n","\n","#place the line of code below from whichever line you want to start debugging\n","#type n for step over and s for step into\n","# ipdb.set_trace()"],"execution_count":13,"outputs":[{"output_type":"stream","text":["  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WROFG7cizCP3","colab_type":"code","colab":{}},"source":["import os\n","os.chdir(repoPath)\n","#%cd gdrive/My Drive/project_folder/TextSimilarityUsingWord2Vec"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PC6_hu67zEs2","colab_type":"code","colab":{}},"source":["from time import time\n","start_nb = time()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bNWPXq1nz5k6","colab_type":"code","outputId":"6b9bdabd-9fbd-4db8-b0a6-eeb4ebe08bbb","executionInfo":{"status":"ok","timestamp":1591729579561,"user_tz":240,"elapsed":1992,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# Import and download stopwords from NLTK.\n","from nltk.corpus import stopwords\n","from nltk import download\n","download('stopwords')  # Download stopwords list.\n","\n","# Remove stopwords.\n","stop_words = stopwords.words('english')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qPDjGOSXz9MH","colab_type":"code","outputId":"17d66e75-4c1b-4a0d-9a1f-e40d808a84d3","executionInfo":{"status":"ok","timestamp":1591729665507,"user_tz":240,"elapsed":83500,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["start = time()\n","import os\n","from gensim.models import Word2Vec\n","\n","%cd data/\n","!ls\n","import os.path\n","from os import path\n","if path.exists(\"GoogleNews-vectors-negative300.bin\") == False:\n","  !wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n","  !gunzip GoogleNews-vectors-negative300.bin\n","else:\n","  print(\"datasetAlreadyExists\")\n","%cd ..\n","\n","from gensim.models.keyedvectors import KeyedVectors\n","model_path = './data/GoogleNews-vectors-negative300.bin'\n","model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n","\n","print('Cell took %.2f seconds to run.' % (time() - start))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/TestingGitCloneIntoGoogleDrive/textSimilarityUsingWordMoversDistance/data\n","GoogleNews-vectors-negative300.bin\n","datasetAlreadyExists\n","/content/gdrive/My Drive/TestingGitCloneIntoGoogleDrive/textSimilarityUsingWordMoversDistance\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"},{"output_type":"stream","text":["Cell took 83.21 seconds to run.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8p_LT8Sp0Lqo","colab_type":"code","outputId":"9aa06562-b2b2-4817-c1e0-5ce485c70a20","executionInfo":{"status":"ok","timestamp":1591729714208,"user_tz":240,"elapsed":1126,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# Pre-processing a document.\n","\n","from nltk import word_tokenize\n","download('punkt')  # Download data for tokenizer.\n","\n","def preprocess(doc):\n","    doc = doc.lower()  # Lower the text.\n","    doc = word_tokenize(doc)  # Split into words.\n","    doc = [w for w in doc if not w in stop_words]  # Remove stopwords.\n","    doc = [w for w in doc if w.isalpha()]  # Remove numbers and punctuation.\n","    return doc"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2ypQFYiAt0FG","colab_type":"code","colab":{}},"source":["#Create pandas dataframe on dataset\n","import pandas as pd\n","#https://data.world/crowdflower/ecommerce-search-relevance\n","df = pd.read_csv('https://query.data.world/s/iccclhgzbyedtv54p5lresglnun3qz', encoding = \"ISO-8859-1\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LTyM-vt6t0FI","colab_type":"code","colab":{}},"source":["#create source document and target document we will be comparing scores for\n","numDataPoints = 6000\n","\n","sourceTitle = df['product_title'][0]\n","source_doc = df['product_description'][0] #2584\n","\n","targetTitles = df['product_title'][:numDataPoints].tolist()\n","target_docs = df['product_description'][:numDataPoints].tolist()\n","targetQueries = df['query'][:numDataPoints].tolist()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7hZ0bzdUYpsG","colab_type":"code","colab":{}},"source":["#using WmdSimilarity\n","\n","wmd_corpus = []\n","documents = []\n","documentTitles = []\n","documentQueries = []\n","\n","for i in range(len(target_docs)):\n","    if target_docs[i] == target_docs[i]: #this is to check if there is an empty string\n","      # ipdb.set_trace()\n","      target_docs_preproc = preprocess(target_docs[i])\n","      if target_docs_preproc != []:\n","        wmd_corpus.append(target_docs_preproc)\n","      else:\n","        wmd_corpus.append([])\n","    else:\n","      wmd_corpus.append([])\n","    documentTitles.append(targetTitles[i])\n","    documents.append(target_docs[i])\n","    documentQueries.append(targetQueries[i])\n","# Initialize WmdSimilarity.\n","from gensim.similarities import WmdSimilarity\n","num_best = 10\n","instance = WmdSimilarity(wmd_corpus, model, num_best=10)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QnL_IkLlbP6f","colab_type":"code","outputId":"4b0e0cae-755c-4ed4-d24c-6ed7a69e8d5f","executionInfo":{"status":"ok","timestamp":1591731070599,"user_tz":240,"elapsed":416033,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["start = time()\n","\n","query = preprocess(source_doc)\n","\n","sims = instance[query]  # A query is simply a \"look-up\" in the similarity class.\n","print('Cell took %.2f seconds to run.' % (time() - start))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cell took 415.79 seconds to run.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hZkjyOXfboCV","colab_type":"code","outputId":"b3052e55-94ef-485f-8227-f73834ce2ba4","executionInfo":{"status":"ok","timestamp":1591731347432,"user_tz":240,"elapsed":266,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"colab":{"base_uri":"https://localhost:8080/","height":901}},"source":["# Print the query and the retrieved documents, together with their similarities.\n","print('Query:')\n","print(sourceTitle)\n","for i in range(num_best):\n","    print()\n","    print('sim in description = %.4f' % sims[i][1])\n","    print(documentTitles[sims[i][0]])\n","    simInTitles = model.wmdistance(sourceTitle, documentTitles[sims[i][0]])\n","    print(\"loss in title similarity = \", simInTitles)\n","    print(\"item query = \", documentQueries[sims[i][0]])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Query:\n","Sony PlayStation 4 (PS4) (Latest Model)- 500 GB Jet Black Console\n","\n","sim in description = 1.0000\n","Sony PlayStation 4 500GB, Dualshock Wireless Control, HDMI Gaming Console Refurb\n","loss in title similarity =  0.3408880498625308\n","item query =  playstation 4\n","\n","sim in description = 1.0000\n","Sony PlayStation 4 (Latest Model)- 500 GB Jet Black Console *NEW*\n","loss in title similarity =  0.10508894400103874\n","item query =  playstation 4\n","\n","sim in description = 1.0000\n","Sony PlayStation 4 500GB Console with 2 Controllers\n","loss in title similarity =  0.2689347376582511\n","item query =  playstation 4\n","\n","sim in description = 1.0000\n","Sony PlayStation 4 (Latest Model) 500 GB Jet Black Console\n","loss in title similarity =  0.06799642897847154\n","item query =  playstation 4\n","\n","sim in description = 1.0000\n","Sony PlayStation 4 (PS4) (Latest Model)- 500 GB Jet Black Console\n","loss in title similarity =  0.0\n","item query =  playstation 4\n","\n","sim in description = 1.0000\n","Sony PlayStation 4 PS4 500 GB Jet Black Console\n","loss in title similarity =  0.15031701531407654\n","item query =  playstation 4\n","\n","sim in description = 1.0000\n","Sony PlayStation 4 (Latest Model)- 500 GB Jet Black Console\n","loss in title similarity =  0.06799642897847154\n","item query =  playstation 4\n","\n","sim in description = 1.0000\n","Sony PlayStation 4 (PS4) (Latest Model)- 500 GB Jet Black Console\n","loss in title similarity =  0.0\n","item query =  playstation 4\n","\n","sim in description = 1.0000\n","Sony PlayStation 4 (PS4) (Latest Model)- 500 GB Jet Black Console\n","loss in title similarity =  0.0\n","item query =  playstation 4\n","\n","sim in description = 0.5621\n","PS4 - Playstation 4 Console\n","loss in title similarity =  0.3225856214945606\n","item query =  playstation 4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yXrFiyvuMIBu","colab_type":"code","colab":{}},"source":["def calculatePercentageOfSimilarItemsInSameQueryCategory(instance, df, num_best, numSamples):\n","  numRows = df.shape[0]\n","  numMatchingCategory = 0\n","  numNonMatchingCategory = 0\n","  # for i in range(numRows):  \n","  for i in range(numSamples):  \n","    print(i)\n","    source_doc_query = df['query'][i]\n","    if source_doc_query == source_doc_query:\n","      query = preprocess(source_doc_query)\n","      sims = instance[query]  # A query is simply a \"look-up\" in the similarity class.\n","      for i in range(num_best):\n","        if documentQueries[sims[i][0]] == source_doc_query:\n","          numMatchingCategory += 1\n","        else:\n","          numNonMatchingCategory\n","  print(\"numMatchingCategory = \", numMatchingCategory)\n","  print(\"numNonMatchingCategory = \", numNonMatchingCategory)\n","  return numMatchingCategory, numNonMatchingCategory\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VCp0wUaVAHaW","colab_type":"code","outputId":"1f0bac73-2154-4dd0-9d1c-751224681356","executionInfo":{"status":"ok","timestamp":1591571861172,"user_tz":240,"elapsed":82846,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"colab":{"base_uri":"https://localhost:8080/","height":221}},"source":["numMatchingCategory, numNonMatchingCategory =calculatePercentageOfSimilarItemsInSameQueryCategory(instance, df, num_best, 10)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","numMatchingCategory =  20\n","numNonMatchingCategory =  0\n"],"name":"stdout"}]}]}