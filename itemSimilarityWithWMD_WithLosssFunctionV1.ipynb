{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"itemSimilarityWithWMD_WithLosssFunctionV1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"cO7gZlDbXFdo","colab_type":"text"},"source":["Terminal"]},{"cell_type":"code","metadata":{"id":"V0rJqGFG3bUh","colab_type":"code","outputId":"64da0579-b12e-47eb-d1d1-676387fd733c","executionInfo":{"status":"ok","timestamp":1591816827046,"user_tz":240,"elapsed":1946,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# %cd data\n","# !pwd\n","# !gunzip GoogleNews-vectors-negative300.bin\n","# %cd ..\n","# url_to_word_embedding_zip = 'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz'\n","# import os.path\n","# from os import path\n","# url_with_extention_removed = os.path.splitext(url_to_word_embedding_zip)[0]\n","# unzipped_file_name = os.path.basename(url_with_extention_removed)\n","# desired_path_of_unzipped_word_embedding = \"data/\" + unzipped_file_name+'.gz'\n","# print(url_to_word_embedding_zip)\n","# !gunzip $url_to_word_embedding_zip $desired_path_of_unzipped_word_embedding\n","\n","# !pwd\n","# !ls \"data/\"\n","# !wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\" -P \"data/\"\n","# !ls\n","# !gunzip \"data/GoogleNews-vectors-negative300.bin\"\n","# !ls \"data/\"\n","\n","!ls \"data/\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["aa_combined-20110321.csv  GoogleNews-vectors-negative300.bin.gz\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"92XR35HoXIhl","colab_type":"text"},"source":["INPUTS"]},{"cell_type":"code","metadata":{"id":"yMIIJM8XXIXm","colab_type":"code","colab":{}},"source":["#repoPath should start with the path \n","#'/content/gdrive/My Drive/' and go to the folder where the git repo is, and should end with the name of the repo:\n","repoPath = '/content/gdrive/My Drive/TestingGitCloneIntoGoogleDrive/textSimilarityUsingWordMoversDistance'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Qnz6ppmYEv2","colab_type":"text"},"source":["Mount google drive"]},{"cell_type":"code","metadata":{"id":"8pCZgWJlXzbx","colab_type":"code","outputId":"41d7a8cf-c8df-4145-dfd5-f194510d6b9e","executionInfo":{"status":"ok","timestamp":1591902437187,"user_tz":240,"elapsed":495,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","import os\n","os.chdir(repoPath)"],"execution_count":83,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KLcpc588YGsi","colab_type":"text"},"source":["Pip Install packages, and import general packages"]},{"cell_type":"code","metadata":{"id":"ZdGKiaG8XHQ5","colab_type":"code","colab":{}},"source":["#pip installs\n","! pip install ipdb -q\n","import ipdb \n","#place the line of code below from whichever line you want to start debugging\n","#type n for step over and s for step into\n","# ipdb.set_trace()\n","\n","from time import time\n","import gensim\n","import pandas as pd\n","from typing import List, Tuple\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ybq5Fed_Lim2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":88},"outputId":"ed74148f-5d71-48c3-9718-619e67d5fcf4","executionInfo":{"status":"ok","timestamp":1591902530530,"user_tz":240,"elapsed":76123,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}}},"source":["\n","############################\n","##download model and load model\n","##Inputs\n","url_to_word_embedding_zip = \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n","download_destination_folder_with_slash_at_end = \"data/\"\n","\n","download_success, desired_path_of_unzipped_word_embedding = download_and_unzip_pretrained_word_embedding(url_to_word_embedding_zip, download_destination_folder_with_slash_at_end)\n","word_embedding_file_path = desired_path_of_unzipped_word_embedding\n","model = load_word_embedding_into_KeyedVectors_model(word_embedding_file_path)\n","\n"],"execution_count":85,"outputs":[{"output_type":"stream","text":["datasetAlreadyExists\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"tY8eF67X4aiQ","colab_type":"code","outputId":"adeb25fb-0bee-4022-a7dc-b4f737effed3","executionInfo":{"status":"ok","timestamp":1591907493758,"user_tz":240,"elapsed":35671,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"colab":{"base_uri":"https://localhost:8080/","height":394}},"source":["#setup pretrained model\n","\n","\n","############################\n","#setup dataset\n","##Inputs\n","url_of_dataset = 'https://query.data.world/s/iccclhgzbyedtv54p5lresglnun3qz'\n","tuple_of_starting_and_ending_exclusive_index_of_sample = (0,100)\n","\n","dataset_frame = load_dataset_into_pandas(url_of_dataset, tuple_of_starting_and_ending_exclusive_index_of_sample)\n","\n","\n","############################\n","#preprocess text\n","##create a new field that has\n","#Inputs\n","dataframe = dataset_frame\n","field_to_preprocess = 'product_description'\n","new_field_name = 'product_description_preprocessed'\n","\n","preprocess_data_object = PreprocessData()\n","#create_new_field_with_preprocessed_text_in_dataframe(self, dataframe: pd.core.frame.DataFrame, field_to_preprocess: str, new_field_name: str) -> pd.core.frame.DataFrame:\n","dataframe_modified = preprocess_data_object.create_new_field_with_preprocessed_text_in_dataframe(dataframe, field_to_preprocess, new_field_name)\n","#end of function\n","#assertEqual(isinstance(dataframe_modified, pd.core.frame.DataFrame),1)\n","\n","############################\n","#Create Word movers distance similarity matrix\n","#Inputs\n","num_best_var = 10\n","source_doc_index = 0\n","\n","print(\"source_doc_index = \", source_doc_index)\n","source_doc = dataframe_modified[field_to_preprocess][source_doc_index]\n","source_doc_preprocessed = preprocess_data_object.text_preprocessing_for_WMD(source_doc)\n","\n","from gensim.similarities import WmdSimilarity\n","wmd_corpus = dataframe_modified[new_field_name].tolist()\n","wmd_object_for_description = WmdSimilarity(wmd_corpus, model, num_best = num_best_var)\n","\n","similar_item_indexes_and_similarity = wmd_object_for_description[source_doc_preprocessed]\n","\n","similar_product_index = []\n","similar_product_similarities =[]\n","\n","for i in range(num_best_var):\n","  print(i)\n","  similar_product_index.append(similar_item_indexes_and_similarity[i][0])\n","  similar_product_similarities.append(similar_item_indexes_and_similarity[i][1])\n","\n","target_index = ['']*num_best_var\n","target_title_list = ['']*num_best_var\n","target_search_query_list = ['']*num_best_var\n","target_search_relevance_to_query = ['']*num_best_var\n","target_description_list = ['']*num_best_var\n","target_description_preprocesed_list = ['']*num_best_var\n","wmd_accuracy = ['']*num_best_var\n","\n","target_index[0] = source_doc_index\n","target_title_list[0] = dataframe_modified['product_title'][source_doc_index]\n","target_search_query_list[0] = dataframe_modified['query'][source_doc_index]\n","target_search_relevance_to_query[0] = dataframe_modified['relevance'][source_doc_index]\n","target_description_list[0] = dataframe_modified['product_description'][source_doc_index]\n","target_description_preprocesed_list[0] = dataframe_modified['product_description_preprocessed'][source_doc_index]\n","\n","data = {'targetIndex': target_index, \n","        'targetTitle': target_title_list, \n","        'targetSearchQuery': target_search_query_list,\n","        'targetRelevanceToQuery': target_search_relevance_to_query, \n","        'targetDescription': target_description_list,\n","        'targetDescriptionPreprocessed': target_description_preprocesed_list,\n","        'similarIndex': similar_product_index,\n","        'similarTitle': dataframe_modified['product_title'][similar_product_index].tolist(),\n","        'similarSearchQuery': dataframe_modified['query'][similar_product_index].tolist(),\n","        'similarRelevanceToQuery': dataframe_modified['relevance'][similar_product_index].tolist(),\n","        'similarDescription': dataframe_modified['product_description'][similar_product_index].tolist(),\n","        'similarDescriptionPreprocessed': dataframe_modified['product_description_preprocessed'][similar_product_index].tolist(),\n","        'WMDRank': list(range(0, num_best_var)),\n","        'WMDSimilarity': similar_product_similarities, \n","        'WMDAccuracy': wmd_accuracy} \n","output_dataframe = pd.DataFrame(data)\n","relevance_threshold = 2\n","accuracy, numMatchingCategory, numNonMatchingCategory = calculate_output_dataframe_accuracy_and_write_in_dataframe(output_dataframe, relevance_threshold)\n","output_dataframe.to_csv('output.csv')\n","\n","\n","\n","# wmd_corpus_for_description = create_corpus_from_preprocessed_text_field(self, dataframe: pd.core.frame.DataFrame, ) -> List[List[str]]\n","#   dataframe['preprocessed_description'].toList()\n","# WMD_object_for_description = create_WMD_object(wmd_corpus, model, num_best)\n","# similar_item_indexes_and_similarity = get_similar_item_indexes_and_similarity()\n","\n","# print_out_similar_item_details_to_dataframe\n","\n","\n","# #Find the similarities for one item in the search query, and display the fields of title, relevance, description, wmd ranking, and wmd rating\n","# download_and_unzip_pretrained_word_embedding"],"execution_count":111,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","source_doc_index =  0\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","numMatchingCategory =  9\n","numNonMatchingCategory =  1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:248: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"hCabShfxUEVw","colab_type":"code","outputId":"849b08ff-36e5-40b4-deeb-5f7f39f94315","executionInfo":{"status":"ok","timestamp":1591907363304,"user_tz":240,"elapsed":739,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["accuracy "],"execution_count":109,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9"]},"metadata":{"tags":[]},"execution_count":109}]},{"cell_type":"code","metadata":{"id":"5fkjKIDq2cal","colab_type":"code","outputId":"a7f64216-36df-4828-fc81-bb024864122e","executionInfo":{"status":"ok","timestamp":1591875337158,"user_tz":240,"elapsed":3390,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import unittest\n","class MyTest(unittest.TestCase):\n","  @unittest.skip(\"demonstrating skipping\")\n","  def test_download_and_unzip_pretrained_word_embedding(self):\n","    url_to_word_embedding_zip = \"http://dumps.wikimedia.org/other/articlefeedback/aa_combined-20110321.csv.gz\"\n","    download_destination_path = \"data/\"\n","    path_to_word_embedding  = download_destination_path + \"aa_combined-20110321.csv\"\n","    #ipdb.set_trace()\n","    !rm $path_to_word_embedding\n","    success, desired_path_of_unzipped_word_embedding = download_and_unzip_pretrained_word_embedding(url_to_word_embedding_zip, download_destination_path)\n","    self.assertEqual(success, 1)\n","\n","  @unittest.skip(\"demonstrating skipping\")\n","  def test_load_word_embedding_into_KeyedVectors_model(self):\n","    url_to_word_embedding_zip = \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n","    download_destination_folder_with_slash_at_end = \"data/\"\n","\n","    download_success, desired_path_of_unzipped_word_embedding = download_and_unzip_pretrained_word_embedding(url_to_word_embedding_zip, download_destination_folder_with_slash_at_end)\n","    if download_success == 0:\n","      success = 0\n","    else:\n","      word_embedding_file_path = desired_path_of_unzipped_word_embedding\n","      model = load_word_embedding_into_KeyedVectors_model(word_embedding_file_path)\n","      \n","      if isinstance(model, gensim.models.keyedvectors.Word2VecKeyedVectors):\n","        success =1\n","      else:\n","        success = 0\n","    \n","    self.assertEqual(success, 1)\n","\n","  @unittest.skip(\"demonstrating skipping\")\n","  def test_load_dataset_into_pandas(self):\n","    url_of_dataset = 'https://query.data.world/s/iccclhgzbyedtv54p5lresglnun3qz'\n","    tuple_of_starting_and_ending_exclusive_index_of_sample = (0,10)\n","    dataset_frame = load_dataset_into_pandas(url_of_dataset, tuple_of_starting_and_ending_exclusive_index_of_sample)\n","    \n","    #pd.set_option('display.max_columns', None)\n","    #print(dataset_frame)\n","    \n","    self.assertEqual(isinstance(dataset_frame, pd.core.frame.DataFrame),1) \n","    self.assertEqual(dataset_frame.shape[0] == (tuple_of_starting_and_ending_exclusive_index_of_sample[1]-tuple_of_starting_and_ending_exclusive_index_of_sample[0]), 1)\n","  \n","  def test_create_new_field_with_preprocessed_text_in_dataframe(self):\n","    url_of_dataset = 'https://query.data.world/s/iccclhgzbyedtv54p5lresglnun3qz'\n","    tuple_of_starting_and_ending_exclusive_index_of_sample = (0,10)\n","    dataset_frame = load_dataset_into_pandas(url_of_dataset, tuple_of_starting_and_ending_exclusive_index_of_sample)\n","    \n","    dataframe = dataset_frame\n","    field_to_preprocess = 'product_description'\n","    new_field_name = 'product_description_preprocessed'\n","    \n","    preprocess_data_object = PreprocessData()\n","    #create_new_field_with_preprocessed_text_in_dataframe(self, dataframe: pd.core.frame.DataFrame, field_to_preprocess: str, new_field_name: str) -> pd.core.frame.DataFrame:\n","    dataframe_modified = preprocess_data_object.create_new_field_with_preprocessed_text_in_dataframe(dataframe, field_to_preprocess, new_field_name)\n","    #end of function\n","    #self.assertEqual(isinstance(dataframe_modified, pd.core.frame.DataFrame),1) \n","    pd.set_option('display.max_columns', None)\n","    print(dataframe_modified)\n","    \n","\n","if __name__ == '__main__':\n","  unittest.main(argv=['first-arg-is-ignored'], exit=False)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":[".sss"],"name":"stderr"},{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","    _unit_id  relevance  relevance:variance  \\\n","0  711158459       3.67               0.471   \n","1  711158460       4.00               0.000   \n","2  711158461       4.00               0.000   \n","3  711158462       3.67               0.471   \n","4  711158463       3.33               0.471   \n","5  711158464       3.20               0.748   \n","6  711158465       4.00               0.000   \n","7  711158466       4.00               0.000   \n","8  711158467       3.75               0.433   \n","9  711158468       2.33               0.471   \n","\n","                                       product_image  \\\n","0  http://thumbs2.ebaystatic.com/d/l225/m/mzvzEUI...   \n","1  http://thumbs3.ebaystatic.com/d/l225/m/mJNDmSy...   \n","2  http://thumbs4.ebaystatic.com/d/l225/m/m10NZXA...   \n","3  http://thumbs2.ebaystatic.com/d/l225/m/mZZXTmA...   \n","4  http://thumbs3.ebaystatic.com/d/l225/m/mzvzEUI...   \n","5  http://thumbs4.ebaystatic.com/d/l225/m/mzvzEUI...   \n","6  http://thumbs4.ebaystatic.com/d/l225/m/m9TQTiW...   \n","7  http://thumbs4.ebaystatic.com/d/l225/m/mTZYG5N...   \n","8  http://thumbs2.ebaystatic.com/d/l225/m/mX5Qphr...   \n","9  http://thumbs2.ebaystatic.com/d/l225/m/mGjN4Ir...   \n","\n","                                        product_link  \\\n","0  http://www.ebay.com/itm/Sony-PlayStation-4-PS4...   \n","1  http://www.ebay.com/itm/Sony-PlayStation-4-Lat...   \n","2  http://www.ebay.com/itm/Sony-PlayStation-4-PS4...   \n","3  http://www.ebay.com/itm/Sony-PlayStation-4-500...   \n","4  http://www.ebay.com/itm/Sony-PlayStation-4-PS4...   \n","5  http://www.ebay.com/itm/Sony-PlayStation-4-PS4...   \n","6  http://www.ebay.com/itm/BRAND-NEW-Sony-PlaySta...   \n","7  http://www.ebay.com/itm/Sony-PlayStation-4-500...   \n","8  http://www.ebay.com/itm/Sony-PlayStation-4-Lat...   \n","9  http://www.ebay.com/itm/Sony-PlayStation-4-Lat...   \n","\n","                   product_price  \\\n","0                       $329.98    \n","1                       $324.84    \n","2                       $324.83    \n","3                       $350.00    \n","4  $308.00\\nTrending at\\n$319.99   \n","5                       $310.00    \n","6                       $910.00    \n","7                       $299.99    \n","8                       $350.00    \n","9                       $469.97    \n","\n","                                       product_title          query  rank  \\\n","0  Sony PlayStation 4 (PS4) (Latest Model)- 500 G...  playstation 4     1   \n","1  Sony PlayStation 4 (Latest Model)- 500 GB Jet ...  playstation 4     2   \n","2    Sony PlayStation 4 PS4 500 GB Jet Black Console  playstation 4     3   \n","3  Sony - PlayStation 4 500GB The Last of Us Rema...  playstation 4     4   \n","4  Sony PlayStation 4 (PS4) (Latest Model)- 500 G...  playstation 4     5   \n","5  Sony PlayStation 4 (PS4) (Latest Model)- 500 G...  playstation 4     6   \n","6          BRAND NEW Sony PlayStation 4 BUNDLE 500gb  playstation 4     7   \n","7  Sony PlayStation 4 500GB, Dualshock Wireless C...  playstation 4     8   \n","8  Sony PlayStation 4 (Latest Model)- 500 GB Jet ...  playstation 4     9   \n","9  Sony PlayStation 4 (Latest Model)- 500 GB Jet ...  playstation 4    10   \n","\n","  source                                                url  \\\n","0   eBay  http://www.ebay.com/sch/i.html?_from=R40&_trks...   \n","1   eBay  http://www.ebay.com/sch/i.html?_from=R40&_trks...   \n","2   eBay  http://www.ebay.com/sch/i.html?_from=R40&_trks...   \n","3   eBay  http://www.ebay.com/sch/i.html?_from=R40&_trks...   \n","4   eBay  http://www.ebay.com/sch/i.html?_from=R40&_trks...   \n","5   eBay  http://www.ebay.com/sch/i.html?_from=R40&_trks...   \n","6   eBay  http://www.ebay.com/sch/i.html?_from=R40&_trks...   \n","7   eBay  http://www.ebay.com/sch/i.html?_from=R40&_trks...   \n","8   eBay  http://www.ebay.com/sch/i.html?_from=R40&_trks...   \n","9   eBay  http://www.ebay.com/sch/i.html?_from=R40&_trks...   \n","\n","                                 product_description  \\\n","0  The PlayStation 4 system opens the door to an ...   \n","1  The PlayStation 4 system opens the door to an ...   \n","2  The PlayStation 4 system opens the door to an ...   \n","3                                                NaN   \n","4  The PlayStation 4 system opens the door to an ...   \n","5  The PlayStation 4 system opens the door to an ...   \n","6                                                NaN   \n","7  The PlayStation 4 system opens the door to an ...   \n","8                                                NaN   \n","9  The PlayStation 4 system opens the door to an ...   \n","\n","                    product_description_preprocessed  \n","0  [playstation, system, opens, door, incredible,...  \n","1  [playstation, system, opens, door, incredible,...  \n","2  [playstation, system, opens, door, incredible,...  \n","3                                                NaN  \n","4  [playstation, system, opens, door, incredible,...  \n","5  [playstation, system, opens, door, incredible,...  \n","6                                                NaN  \n","7  [playstation, system, opens, door, incredible,...  \n","8                                                NaN  \n","9  [playstation, system, opens, door, incredible,...  \n"],"name":"stdout"},{"output_type":"stream","text":["\n","----------------------------------------------------------------------\n","Ran 4 tests in 2.871s\n","\n","OK (skipped=3)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"FWii_7GVXHEn","colab_type":"code","colab":{}},"source":["#download pretrained word embedding model and load it\n","##download a file\n","##unzip a file\n","def download_and_unzip_pretrained_word_embedding(url_to_word_embedding_zip: str, download_destination_folder_with_slash_at_end: str) -> Tuple[bool, str]:\n","  #download if unzipped file exists already\n","  #if not, unzip and check if file exists after unzipping\n","  #if file exists, return 1 for success\n","  import os.path\n","  from os import path\n","  zipped_file_name = os.path.basename(url_to_word_embedding_zip)\n","  unzipped_file_name = os.path.splitext(zipped_file_name)[0]\n","\n","  path_of_zipped_word_embedding = download_destination_folder_with_slash_at_end + zipped_file_name\n","  desired_path_of_unzipped_word_embedding = download_destination_folder_with_slash_at_end + unzipped_file_name\n","  zip_file_extension = os.path.splitext(url_to_word_embedding_zip)[1][1:]\n","\n","  if path.exists(desired_path_of_unzipped_word_embedding) == False:\n","    !wget -c $url_to_word_embedding_zip -P $download_destination_folder_with_slash_at_end\n","    if zip_file_extension == 'gz':\n","      !gunzip $path_of_zipped_word_embedding\n","    elif zip_file_extension == 'zip':\n","      print(\"zip_file_extension\", \" not implemented yet\")\n","      return (False, None)\n","    else:\n","      print(\"This function doesnt handle zipped files with extension: \", zip_file_extension)\n","      return (False, None)\n","  else:\n","    print(\"datasetAlreadyExists\")\n","\n","  if path.exists(desired_path_of_unzipped_word_embedding) == True:\n","    successful_in_creating_word_embedding = True\n","  else:\n","    successful_in_creating_word_embedding = False\n","  return (successful_in_creating_word_embedding, desired_path_of_unzipped_word_embedding)\n","\n","##load word embedding\n","def load_word_embedding_into_KeyedVectors_model(word_embedding_file_path: str) -> gensim.models.keyedvectors.Word2VecKeyedVectors:\n","  from gensim.models.keyedvectors import KeyedVectors\n","  model = KeyedVectors.load_word2vec_format(word_embedding_file_path, binary=True)\n","  return model\n","\n","#load dataset\n","#extract desired features from dataset\n","def load_dataset_into_pandas(url_of_dataset: str, tuple_of_starting_and_ending_exclusive_index_of_sample: tuple) -> pd.core.frame.DataFrame:\n","  #Create pandas dataframe on dataset\n","  import pandas as pd\n","  #https://data.world/crowdflower/ecommerce-search-relevance\n","  df = pd.read_csv(url_of_dataset, encoding = \"ISO-8859-1\")\n","  start,end = tuple_of_starting_and_ending_exclusive_index_of_sample\n","  df = df.iloc[start:end]\n","  return df\n","\n","\n","#preprocess data\n","class PreprocessData:\n","  \"\"\"\n","  A class used to find the orientation of the robot with respect to a lane over time.\n","  \"\"\"\n","  def __init__(self):\n","    \"\"\"\n","    Create a PreprocessData object\n","    Parameters:\n","      - \n","    \"\"\"\n","    from nltk import download\n","    from nltk import word_tokenize\n","    # Import and download stopwords from NLTK.\n","    from nltk.corpus import stopwords\n","    \n","    download('punkt')  # Download data for tokenizer.\n","    download('stopwords')  # Download stopwords list.\n","\n","    # Remove stopwords.\n","    self.stop_words = stopwords.words('english')\n","\n","  def make_text_lower_case(self, text: str) -> str:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    return text.lower()\n","\n","  def tokenize_string_to_list_of_separate_words(self, text_string: str) -> List[str]:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    from nltk import word_tokenize\n","    return word_tokenize(text_string) \n","\n","  def remove_stopwords_from_text(self, text_list: List[str]) -> List[str]:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    return [w for w in text_list if not w in self.stop_words]\n","\n","  def remove_numbers_and_punctuation_from_text(self, text_list: List[str]) -> List[str]:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    return [w for w in text_list if w.isalpha()]\n","\n","  def text_preprocessing_for_WMD(self, text: str) -> List[str]:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    text = self.make_text_lower_case(text)\n","    text_list = self.tokenize_string_to_list_of_separate_words(text)\n","    text_list = self.remove_stopwords_from_text(text_list)\n","    text_list = self.remove_numbers_and_punctuation_from_text(text_list)\n","    return text_list\n","\n","  def create_new_field_with_preprocessed_text_in_dataframe(self, dataframe: pd.core.frame.DataFrame, field_to_preprocess: str, new_field_name: str) -> pd.core.frame.DataFrame:\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    num_rows_dataframe = len(dataframe.index)\n","    preprocessed_text_list = [np.nan] * num_rows_dataframe\n","    empty_text = []\n","    for ind in dataframe.index:\n","      field_text = dataframe[field_to_preprocess][ind]\n","      #check if field is empty or nan\n","      if field_text == field_text:\n","        preprocessed_text = self.text_preprocessing_for_WMD(field_text)\n","        if preprocessed_text != []:\n","          preprocessed_text_list[ind] = preprocessed_text\n","        else:\n","          preprocessed_text_list[ind] = empty_text\n","      else:\n","        preprocessed_text_list[ind] = empty_text\n","\n","    dataframe.insert(len(dataframe.columns), new_field_name, preprocessed_text_list, True)\n","    return dataframe\n","    \n","     \n","\n","#find WMD similarity\n","class WordMoversDistanceSimilarity:\n","  \"\"\"\n","  A class used to find the orientation of the robot with respect to a lane over time.\n","  \"\"\"\n","  def __init__(self, keyed_vector_model: gensim.models.keyedvectors.Word2VecKeyedVectors, list_of_document_lists: List[List[str]], num_similar_queries: int):\n","    \"\"\"\n","    Create a PreprocessData object\n","    Parameters:\n","      - \n","    \"\"\"\n","    self.model = keyed_vector_model\n","    self.wmd_corpus = list_of_document_lists\n","    self.num_similar_queries = num_similar_queries\n","    self.preprocess_data_object = PreprocessData()\n","    self.similarityMatrix = 0\n","    self.similar_document_tuple_of_indexes_and_similarity_scores = 0\n","\n","  def create_similarity_matrix(self):\n","    \"\"\"\n","    make text lower case. Returns None if text is empty\n","    Params:\n","      - text: string of the input text we want to make lowercase\n","    Returns:\n","      - lower_case_text: the input text made lowercase\n","    \"\"\"\n","    from gensim.similarities import WmdSimilarity\n","    self.similarityMatrix = WmdSimilarity(self.wmd_corpus, self.model, self.num_similar_queries)\n","\n","  def find_similar_document_index_and_similarity(self, query_text: str) -> List[tuple]:\n","    query_text = preprocess(source_doc)\n","    self.similar_document_tuple_of_indexes_and_similarity_scores = instance[query]  # A query is simply a \"look-up\" in the similarity class.\n","    return self.similar_document_tuple_of_indexes_and_similarity_scores\n","\n","class PrintingResultAndCalculateLoss:\n","  \"\"\"\n","  A class used to find the orientation of the robot with respect to a lane over time.\n","  \"\"\"\n","  def __init__(self, similar_document_tuple_of_indexes_and_similarity_scores: int, data_frame: pd.core.frame.DataFrame):\n","    \"\"\"\n","    Create a PreprocessData object\n","    Parameters:\n","      - \n","    \"\"\"\n","    self.similar_document_tuple_of_indexes_and_similarity_scores = similar_document_tuple_of_indexes_and_similarity_scores\n","    self.data_frame = data_frame\n","\n","  def calculate_accuracy_of_similarity_prediction_for_a_query_text_from_actual_(index_of_query_product, score_cutoff):\n","    \"\"\"\n","    Calculate accuracy for a given single similar products query. This query text must be the actual data from the table, so that it will have a \n","    label \"query\" field and \"relevance\" field.\n","\n","    if index_of_query_product \"relevance\" field < score_cutoff: #we dont want to use query items in which the similarity to the search query was low.\n","      return accuracy = None\n","    else: \n","      for every index in similar_document_tuple_of_indexes_and_similarity_scores:\n","        number_of_correct_similar_products += (index_of_query_product \"query\" field == similar_index \"query\" field)*(similar_index \"relevance\" field >= score_cutoff)\n","    accuracy = number_of_correct_similar_products/num_similar_queries\n","\n","    Parameters:\n","      - \n","    \"\"\"\n","  #in order to be classified correctly, the similar product has to be in same search query as target category\n","  #and has to have a relavance score >= score cutoff.\n","  #I know this is flawed because there might be a product in the wrong search query but is still similar to the target product.\n","  #but overall, im hoping this case to be less\n","\n","  #\n","    num_similar_queries = len(index_of_query_product)\n","\n","    search_query = df['query'][sims[i][0]]\n","    for i in range(num_similar_queries):\n","      index_of_\n","\n","def calculate_output_dataframe_accuracy_and_write_in_dataframe(output_dataframe, relevance_threshold):\n","  numRows = output_dataframe[\"similarDescription\"].shape[0]\n","  numMatchingCategory = 0\n","  numNonMatchingCategory = 0\n","  if output_dataframe[\"targetRelevanceToQuery\"][0] < relevance_threshold:\n","    accuracy, numMatchingCategory, numNonMatchingCategory = (-1,-1,-1)\n","    return accuracy, numMatchingCategory, numNonMatchingCategory\n","  for i in range(numRows):  \n","    if output_dataframe[\"targetSearchQuery\"][0] == output_dataframe[\"similarSearchQuery\"][i]:\n","      numMatchingCategory += 1\n","    else:\n","      numNonMatchingCategory +=1\n","  print(\"numMatchingCategory = \", numMatchingCategory)\n","  print(\"numNonMatchingCategory = \", numNonMatchingCategory)\n","  accuracy = numMatchingCategory/num_best\n","  output_dataframe[\"WMDAccuracy\"][0] = accuracy\n","  return accuracy, numMatchingCategory, numNonMatchingCategory\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1y_JaquUYS7G","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"15d9c37e-b314-4c56-9d27-ae5f6b7747e5","executionInfo":{"status":"ok","timestamp":1591905705901,"user_tz":240,"elapsed":512,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}}},"source":["output_dataframe[\"similarDescription\"].shape[0]"],"execution_count":97,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10"]},"metadata":{"tags":[]},"execution_count":97}]},{"cell_type":"code","metadata":{"id":"NCgEBYErYS4q","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4be0wAJnYS2B","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qTDbHNDxzWNR","colab_type":"code","colab":{}},"source":["#INPUTS\n","\n","#repoPath should start with the path \n","#'/content/gdrive/My Drive/' and go to the folder where the git repo is, and should end with the name of the repo:\n","repoPath = '/content/gdrive/My Drive/TestingGitCloneIntoGoogleDrive/textSimilarityUsingWordMoversDistance'\n","userEmail = 'sanmesh1@gmail.com'\n","userName = 'sanmesh1'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dT11jlEGye8a","colab_type":"code","outputId":"a1a9dd4d-e3bd-4372-9ca1-9ce0e9bcbd67","executionInfo":{"status":"ok","timestamp":1591729566062,"user_tz":240,"elapsed":20393,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BxNLdrvrzAFu","colab_type":"code","outputId":"09077dc3-5207-45f5-f69b-a68209254320","executionInfo":{"status":"ok","timestamp":1591729573038,"user_tz":240,"elapsed":5587,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["! pip install ipdb -q\n","import ipdb \n","\n","#place the line of code below from whichever line you want to start debugging\n","#type n for step over and s for step into\n","# ipdb.set_trace()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WROFG7cizCP3","colab_type":"code","colab":{}},"source":["import os\n","os.chdir(repoPath)\n","#%cd gdrive/My Drive/project_folder/TextSimilarityUsingWord2Vec"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PC6_hu67zEs2","colab_type":"code","colab":{}},"source":["from time import time\n","start_nb = time()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bNWPXq1nz5k6","colab_type":"code","outputId":"6b9bdabd-9fbd-4db8-b0a6-eeb4ebe08bbb","executionInfo":{"status":"ok","timestamp":1591729579561,"user_tz":240,"elapsed":1992,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# Import and download stopwords from NLTK.\n","from nltk.corpus import stopwords\n","from nltk import download\n","download('stopwords')  # Download stopwords list.\n","\n","# Remove stopwords.\n","stop_words = stopwords.words('english')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qPDjGOSXz9MH","colab_type":"code","outputId":"17d66e75-4c1b-4a0d-9a1f-e40d808a84d3","executionInfo":{"status":"ok","timestamp":1591729665507,"user_tz":240,"elapsed":83500,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["start = time()\n","import os\n","from gensim.models import Word2Vec\n","\n","%cd data/\n","!ls\n","import os.path\n","from os import path\n","if path.exists(\"GoogleNews-vectors-negative300.bin\") == False:\n","  !wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n","  !gunzip GoogleNews-vectors-negative300.bin\n","else:\n","  print(\"datasetAlreadyExists\")\n","%cd ..\n","\n","from gensim.models.keyedvectors import KeyedVectors\n","model_path = './data/GoogleNews-vectors-negative300.bin'\n","model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n","\n","print('Cell took %.2f seconds to run.' % (time() - start))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/TestingGitCloneIntoGoogleDrive/textSimilarityUsingWordMoversDistance/data\n","GoogleNews-vectors-negative300.bin\n","datasetAlreadyExists\n","/content/gdrive/My Drive/TestingGitCloneIntoGoogleDrive/textSimilarityUsingWordMoversDistance\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"},{"output_type":"stream","text":["Cell took 83.21 seconds to run.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8p_LT8Sp0Lqo","colab_type":"code","outputId":"9aa06562-b2b2-4817-c1e0-5ce485c70a20","executionInfo":{"status":"ok","timestamp":1591729714208,"user_tz":240,"elapsed":1126,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# Pre-processing a document.\n","\n","from nltk import word_tokenize\n","download('punkt')  # Download data for tokenizer.\n","\n","def preprocess(doc):\n","    doc = doc.lower()  # Lower the text.\n","    doc = word_tokenize(doc)  # Split into words.\n","    doc = [w for w in doc if not w in stop_words]  # Remove stopwords.\n","    doc = [w for w in doc if w.isalpha()]  # Remove numbers and punctuation.\n","    return doc"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2ypQFYiAt0FG","colab_type":"code","colab":{}},"source":["#Create pandas dataframe on dataset\n","import pandas as pd\n","#https://data.world/crowdflower/ecommerce-search-relevance\n","df = pd.read_csv('https://query.data.world/s/iccclhgzbyedtv54p5lresglnun3qz', encoding = \"ISO-8859-1\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LTyM-vt6t0FI","colab_type":"code","colab":{}},"source":["#create source document and target document we will be comparing scores for\n","numDataPoints = 6000\n","\n","sourceTitle = df['product_title'][0]\n","source_doc = df['product_description'][0] #2584\n","\n","targetTitles = df['product_title'][:numDataPoints].tolist()\n","target_docs = df['product_description'][:numDataPoints].tolist()\n","targetQueries = df['query'][:numDataPoints].tolist()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7hZ0bzdUYpsG","colab_type":"code","colab":{}},"source":["#using WmdSimilarity\n","\n","wmd_corpus = []\n","documents = []\n","documentTitles = []\n","documentQueries = []\n","\n","for i in range(len(target_docs)):\n","    if target_docs[i] == target_docs[i]: #this is to check if there is an empty string\n","      # ipdb.set_trace()\n","      target_docs_preproc = preprocess(target_docs[i])\n","      if target_docs_preproc != []:\n","        wmd_corpus.append(target_docs_preproc)\n","      else:\n","        wmd_corpus.append([])\n","    else:\n","      wmd_corpus.append([])\n","    documentTitles.append(targetTitles[i])\n","    documents.append(target_docs[i])\n","    documentQueries.append(targetQueries[i])\n","# Initialize WmdSimilarity.\n","from gensim.similarities import WmdSimilarity\n","num_best = 10\n","instance = WmdSimilarity(wmd_corpus, model, num_best=10)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QnL_IkLlbP6f","colab_type":"code","outputId":"4b0e0cae-755c-4ed4-d24c-6ed7a69e8d5f","executionInfo":{"status":"ok","timestamp":1591731070599,"user_tz":240,"elapsed":416033,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["start = time()\n","\n","query = preprocess(source_doc)\n","\n","sims = instance[query]  # A query is simply a \"look-up\" in the similarity class.\n","print('Cell took %.2f seconds to run.' % (time() - start))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cell took 415.79 seconds to run.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hZkjyOXfboCV","colab_type":"code","outputId":"b3052e55-94ef-485f-8227-f73834ce2ba4","executionInfo":{"status":"ok","timestamp":1591731347432,"user_tz":240,"elapsed":266,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"colab":{"base_uri":"https://localhost:8080/","height":901}},"source":["# Print the query and the retrieved documents, together with their similarities.\n","print('Query:')\n","print(sourceTitle)\n","for i in range(num_best):\n","    print()\n","    print('sim in description = %.4f' % sims[i][1])\n","    print(documentTitles[sims[i][0]])\n","    simInTitles = model.wmdistance(sourceTitle, documentTitles[sims[i][0]])\n","    print(\"loss in title similarity = \", simInTitles)\n","    print(\"item query = \", documentQueries[sims[i][0]])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Query:\n","Sony PlayStation 4 (PS4) (Latest Model)- 500 GB Jet Black Console\n","\n","sim in description = 1.0000\n","Sony PlayStation 4 500GB, Dualshock Wireless Control, HDMI Gaming Console Refurb\n","loss in title similarity =  0.3408880498625308\n","item query =  playstation 4\n","\n","sim in description = 1.0000\n","Sony PlayStation 4 (Latest Model)- 500 GB Jet Black Console *NEW*\n","loss in title similarity =  0.10508894400103874\n","item query =  playstation 4\n","\n","sim in description = 1.0000\n","Sony PlayStation 4 500GB Console with 2 Controllers\n","loss in title similarity =  0.2689347376582511\n","item query =  playstation 4\n","\n","sim in description = 1.0000\n","Sony PlayStation 4 (Latest Model) 500 GB Jet Black Console\n","loss in title similarity =  0.06799642897847154\n","item query =  playstation 4\n","\n","sim in description = 1.0000\n","Sony PlayStation 4 (PS4) (Latest Model)- 500 GB Jet Black Console\n","loss in title similarity =  0.0\n","item query =  playstation 4\n","\n","sim in description = 1.0000\n","Sony PlayStation 4 PS4 500 GB Jet Black Console\n","loss in title similarity =  0.15031701531407654\n","item query =  playstation 4\n","\n","sim in description = 1.0000\n","Sony PlayStation 4 (Latest Model)- 500 GB Jet Black Console\n","loss in title similarity =  0.06799642897847154\n","item query =  playstation 4\n","\n","sim in description = 1.0000\n","Sony PlayStation 4 (PS4) (Latest Model)- 500 GB Jet Black Console\n","loss in title similarity =  0.0\n","item query =  playstation 4\n","\n","sim in description = 1.0000\n","Sony PlayStation 4 (PS4) (Latest Model)- 500 GB Jet Black Console\n","loss in title similarity =  0.0\n","item query =  playstation 4\n","\n","sim in description = 0.5621\n","PS4 - Playstation 4 Console\n","loss in title similarity =  0.3225856214945606\n","item query =  playstation 4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yXrFiyvuMIBu","colab_type":"code","colab":{}},"source":["def calculatePercentageOfSimilarItemsInSameQueryCategory(instance, df, num_best, numSamples):\n","  numRows = df.shape[0]\n","  numMatchingCategory = 0\n","  numNonMatchingCategory = 0\n","  # for i in range(numRows):  \n","  for i in range(numSamples):  \n","    print(i)\n","    source_doc_query = df['query'][i]\n","    if source_doc_query == source_doc_query:\n","      query = preprocess(source_doc_query)\n","      sims = instance[query]  # A query is simply a \"look-up\" in the similarity class.\n","      for i in range(num_best):\n","        if documentQueries[sims[i][0]] == source_doc_query:\n","          numMatchingCategory += 1\n","        else:\n","          numNonMatchingCategory\n","  print(\"numMatchingCategory = \", numMatchingCategory)\n","  print(\"numNonMatchingCategory = \", numNonMatchingCategory)\n","  return numMatchingCategory, numNonMatchingCategory\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VCp0wUaVAHaW","colab_type":"code","outputId":"1f0bac73-2154-4dd0-9d1c-751224681356","executionInfo":{"status":"ok","timestamp":1591571861172,"user_tz":240,"elapsed":82846,"user":{"displayName":"Sanmesh Udhayakumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhH2CQfLcbaO6L1jQbMzumlmE3uClv-dbmfy-V_=s64","userId":"15276226272130959420"}},"colab":{"base_uri":"https://localhost:8080/","height":221}},"source":["numMatchingCategory, numNonMatchingCategory =calculatePercentageOfSimilarItemsInSameQueryCategory(instance, df, num_best, 10)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","numMatchingCategory =  20\n","numNonMatchingCategory =  0\n"],"name":"stdout"}]}]}